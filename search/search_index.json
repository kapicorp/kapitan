{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kapitan: Generic templated configuration management for Kubernetes, Terraform and other things Kapitan is a tool to manage complex deployments using jsonnet, kadet (alpha) and jinja2. Use Kapitan to manage your Kubernetes manifests, your documentation, your Terraform configuration or even simplify your scripts. Community Main Blog, articles and tutorials : Kapitan Blog Slack #kapitan Website https://kapitan.dev London Meetup Group London Kapitan Meetup How is it different from Helm ? Please look at our FAQ ! Main Features Use the Inventory as the single source of truth to tie together deployments, resources and documentation. based on reclass Use Jsonnet or Kadet (alpha) to create json/yaml based configurations (e.g. Kubernetes, Terraform); Use Jinja2 to create text based templates for scripts and documentation; Manage secrets with GPG, AWS KMS or gCloud KMS and define who can access them, without compromising collaboration with other users. Create dynamically generated documentation about a single deployment (i.e. ad-hoc instructions) or all deployments at once (i.e. global state of deployments) Quickstart Docker (recommended) docker run -t --rm -v $(pwd):/src:delegated deepmind/kapitan -h On Linux you can add -u $(id -u) to docker run to preserve file permissions. For CI/CD usage, check out CI.md Pip Kapitan needs Python 3.6. Install Python 3.6: Linux: sudo apt-get update && sudo apt-get install -y python3.6-dev python3-pip python3-yaml Mac: brew install python3 libyaml Install Kapitan: User ( $HOME/.local/lib/python3.6/bin on Linux or $HOME/Library/Python/3.6/bin on macOS): pip3 install --user --upgrade kapitan System-wide (not recommended): sudo pip3 install --upgrade kapitan Standalone binary From v0.24.0, kapitan is also available as a standalone binary which you can download from the releases page . The platform currently supported is Linux amd64. Example The example below compiles 2 targets inside the examples/kubernetes folder. Each target represents a different namespace in a minikube cluster. These targets generate the following resources: Kubernetes Namespace for the targets Kubernetes StatefulSet for ElasticSearch Master node Kubernetes StatefulSet for ElasticSearch Client node Kubernetes StatefulSet for ElasticSearch Data node Kubernetes Service to expose ElasticSearch discovery port Kubernetes Service to expose ElasticSearch service port Kubernetes StatefulSet for MySQL Kubernetes Service to expose MySQL service port Kubernetes Secret for MySQL credentials Scripts to configure kubectl context to control the targets and helpers to apply/delete objects. Documentation $ cd examples/kubernetes $ kapitan compile Compiled minikube-mysql Compiled minikube-es Documentation Getting Started Kapitan Overview Understanding inventory Compile operation Kapitan features References (formerly secrets) Manifest validation External dependencies management Miscellaneous Usage Continuous Integration Set up kapitan on older Python systems Examples Kubernetes Credits Jsonnet Jinja2 reclass FAQ Why do we prefer Kapitan to Helm ? Before developing Kapitan, we turned to Helm in an attempt to improve our old Jinja based templating system. We quickly discovered that Helm did not fit well with our workflow, for the following reasons (which were true at the time of the evaluation): Helm uses Go templates to define Kubernetes (yaml) manifests. We were already unsatisfied by using Jinja and we did not see a huge improvement from our previous system, the main reason being: YAML files are not suitable to be managed by text templating frameworks. Helm does not have a solution for sharing values across charts, if not through subcharts. We wanted to be able to have one single place to define all values for all our templates. Sharing data between charts felt awkward and complicated. Helm is component/chart based. We wanted to have something that would treat all our deployments as a whole. We did not fancy the dependency on the tiller. In short, we feel Helm is trying to be apt-get for Kubernetes charts, while we are trying to take you further than that. Why do I need Kapitan? With Kapitan, we worked to de-compose several problems that most of the other solutions are treating as one. 1) Kubernetes manifests : We like the jsonnet approach of using json as the working language. Jsonnet allows us to use inheritance and composition, and hide complexity at higher levels. 2) Configuration files : Most solutions will assume this problem is solved somewhere else. We feel Jinja (or your template engine of choice) have the upper hand here. 3) Hierarchical inventory : This is the feature that sets us apart from other solutions. We use the inventory (based on reclass ) to define variables and properties that can be reused across different projects/deployments. This allows us to limit repetition, but also to define a nicer interface with developers (or CI tools) which will only need to understand YAML to operate changes. 4) Secrets : We manage most of our secrets with kapitan using the GPG, Google Cloud KMS and AWS KMS integrations. Keys can be setup per class, per target or shared so you can easily and flexibly manage access per environment. They can also be dynamically generated on compilation, if you don't feel like generating random passwords or RSA private keys, and they can be referenced in the inventory like any other variables. We have plans to support other providers such as Vault, in addition to GPG, Google Cloud KMS and AWS KMS. 5) Canned scripts : We treat scripts as text templates, so that we can craft pre-canned scripts for the specific target we are working on. This can be used for instance to define scripts that setup clusters, contexts or allow running kubectl with all the correct settings. Most other solutions require you to define contexts and call kubectl with the correct settings. We take care of that for you. Less ambiguity, fewer mistakes. 6) Documentation : We also use templates to create documentation for the targets we deploy. Documentation lived alongside everything else and it is treated as a first class citizen. We feel most other solutions are pushing the limits of their capacity in order to provide for the above problems. Helm treats everything as a text template, while jsonnet tries to do everything as json. We believe that these approaches can be blended in a powerful new way, glued together by the inventory. Related projects sublime-jsonnet-syntax - Jsonnet syntax highlighting for Sublime Text language-jsonnet - Jsonnet syntax highlighting for Atom vim-jsonnet - Jsonnet plugin for Vim (requires a vim plugin manager)","title":"Home"},{"location":"#kapitan-generic-templated-configuration-management-for-kubernetes-terraform-and-other-things","text":"Kapitan is a tool to manage complex deployments using jsonnet, kadet (alpha) and jinja2. Use Kapitan to manage your Kubernetes manifests, your documentation, your Terraform configuration or even simplify your scripts.","title":"Kapitan: Generic templated configuration management for Kubernetes, Terraform and other things"},{"location":"#community","text":"Main Blog, articles and tutorials : Kapitan Blog Slack #kapitan Website https://kapitan.dev London Meetup Group London Kapitan Meetup How is it different from Helm ? Please look at our FAQ !","title":"Community"},{"location":"#main-features","text":"Use the Inventory as the single source of truth to tie together deployments, resources and documentation. based on reclass Use Jsonnet or Kadet (alpha) to create json/yaml based configurations (e.g. Kubernetes, Terraform); Use Jinja2 to create text based templates for scripts and documentation; Manage secrets with GPG, AWS KMS or gCloud KMS and define who can access them, without compromising collaboration with other users. Create dynamically generated documentation about a single deployment (i.e. ad-hoc instructions) or all deployments at once (i.e. global state of deployments)","title":"Main Features"},{"location":"#quickstart","text":"","title":"Quickstart"},{"location":"#docker-recommended","text":"docker run -t --rm -v $(pwd):/src:delegated deepmind/kapitan -h On Linux you can add -u $(id -u) to docker run to preserve file permissions. For CI/CD usage, check out CI.md","title":"Docker (recommended)"},{"location":"#pip","text":"Kapitan needs Python 3.6. Install Python 3.6: Linux: sudo apt-get update && sudo apt-get install -y python3.6-dev python3-pip python3-yaml Mac: brew install python3 libyaml Install Kapitan: User ( $HOME/.local/lib/python3.6/bin on Linux or $HOME/Library/Python/3.6/bin on macOS): pip3 install --user --upgrade kapitan System-wide (not recommended): sudo pip3 install --upgrade kapitan","title":"Pip"},{"location":"#standalone-binary","text":"From v0.24.0, kapitan is also available as a standalone binary which you can download from the releases page . The platform currently supported is Linux amd64.","title":"Standalone binary"},{"location":"#example","text":"The example below compiles 2 targets inside the examples/kubernetes folder. Each target represents a different namespace in a minikube cluster. These targets generate the following resources: Kubernetes Namespace for the targets Kubernetes StatefulSet for ElasticSearch Master node Kubernetes StatefulSet for ElasticSearch Client node Kubernetes StatefulSet for ElasticSearch Data node Kubernetes Service to expose ElasticSearch discovery port Kubernetes Service to expose ElasticSearch service port Kubernetes StatefulSet for MySQL Kubernetes Service to expose MySQL service port Kubernetes Secret for MySQL credentials Scripts to configure kubectl context to control the targets and helpers to apply/delete objects. Documentation $ cd examples/kubernetes $ kapitan compile Compiled minikube-mysql Compiled minikube-es","title":"Example"},{"location":"#documentation","text":"","title":"Documentation"},{"location":"#getting-started","text":"Kapitan Overview Understanding inventory Compile operation","title":"Getting Started"},{"location":"#kapitan-features","text":"References (formerly secrets) Manifest validation External dependencies management","title":"Kapitan features"},{"location":"#miscellaneous","text":"Usage Continuous Integration Set up kapitan on older Python systems","title":"Miscellaneous"},{"location":"#examples","text":"Kubernetes","title":"Examples"},{"location":"#credits","text":"Jsonnet Jinja2 reclass","title":"Credits"},{"location":"#faq","text":"","title":"FAQ"},{"location":"#why-do-we-prefer-kapitan-to-helm","text":"Before developing Kapitan, we turned to Helm in an attempt to improve our old Jinja based templating system. We quickly discovered that Helm did not fit well with our workflow, for the following reasons (which were true at the time of the evaluation): Helm uses Go templates to define Kubernetes (yaml) manifests. We were already unsatisfied by using Jinja and we did not see a huge improvement from our previous system, the main reason being: YAML files are not suitable to be managed by text templating frameworks. Helm does not have a solution for sharing values across charts, if not through subcharts. We wanted to be able to have one single place to define all values for all our templates. Sharing data between charts felt awkward and complicated. Helm is component/chart based. We wanted to have something that would treat all our deployments as a whole. We did not fancy the dependency on the tiller. In short, we feel Helm is trying to be apt-get for Kubernetes charts, while we are trying to take you further than that.","title":"Why do we prefer Kapitan to Helm?"},{"location":"#why-do-i-need-kapitan","text":"With Kapitan, we worked to de-compose several problems that most of the other solutions are treating as one. 1) Kubernetes manifests : We like the jsonnet approach of using json as the working language. Jsonnet allows us to use inheritance and composition, and hide complexity at higher levels. 2) Configuration files : Most solutions will assume this problem is solved somewhere else. We feel Jinja (or your template engine of choice) have the upper hand here. 3) Hierarchical inventory : This is the feature that sets us apart from other solutions. We use the inventory (based on reclass ) to define variables and properties that can be reused across different projects/deployments. This allows us to limit repetition, but also to define a nicer interface with developers (or CI tools) which will only need to understand YAML to operate changes. 4) Secrets : We manage most of our secrets with kapitan using the GPG, Google Cloud KMS and AWS KMS integrations. Keys can be setup per class, per target or shared so you can easily and flexibly manage access per environment. They can also be dynamically generated on compilation, if you don't feel like generating random passwords or RSA private keys, and they can be referenced in the inventory like any other variables. We have plans to support other providers such as Vault, in addition to GPG, Google Cloud KMS and AWS KMS. 5) Canned scripts : We treat scripts as text templates, so that we can craft pre-canned scripts for the specific target we are working on. This can be used for instance to define scripts that setup clusters, contexts or allow running kubectl with all the correct settings. Most other solutions require you to define contexts and call kubectl with the correct settings. We take care of that for you. Less ambiguity, fewer mistakes. 6) Documentation : We also use templates to create documentation for the targets we deploy. Documentation lived alongside everything else and it is treated as a first class citizen. We feel most other solutions are pushing the limits of their capacity in order to provide for the above problems. Helm treats everything as a text template, while jsonnet tries to do everything as json. We believe that these approaches can be blended in a powerful new way, glued together by the inventory.","title":"Why do I need Kapitan?"},{"location":"#related-projects","text":"sublime-jsonnet-syntax - Jsonnet syntax highlighting for Sublime Text language-jsonnet - Jsonnet syntax highlighting for Atom vim-jsonnet - Jsonnet plugin for Vim (requires a vim plugin manager)","title":"Related projects"},{"location":"CI/","text":"Kapitan: CI/CD usage The Docker image ( deepmind/kapitan:ci ) ( Dockerfile ) comes pre-packaged with gcloud , gsutil , bq , kubectl , terraform , promtool and kapitan . Example workflow - Deploy to GKE The following commands are run using the deepmind/kapitan:ci Docker image. Compile: kapitan compile Compiled app ( 2 .23s ) Setup gcloud and GKE credentials: echo \" $GCP_SA_KEY_FROM_CI_SECRETS \" > service_account_key.json gcloud auth activate-service-account --key-file service_account_key.json gcloud container clusters get-credentials CLUSTER --zone ZONE --project GCP_PROJECT_ID Setup kubectl: kubectl config set-context CLUSTER_CONTEXT --cluster CLUSTER --user USER --namespace NAMESPACE kubectl config use-context CLUSTER_CONTEXT Deploy: kubectl apply -f compiled/app/manifests/","title":"Continuous Integration"},{"location":"CI/#kapitan-cicd-usage","text":"The Docker image ( deepmind/kapitan:ci ) ( Dockerfile ) comes pre-packaged with gcloud , gsutil , bq , kubectl , terraform , promtool and kapitan .","title":"Kapitan: CI/CD usage"},{"location":"CI/#example-workflow-deploy-to-gke","text":"The following commands are run using the deepmind/kapitan:ci Docker image. Compile: kapitan compile Compiled app ( 2 .23s ) Setup gcloud and GKE credentials: echo \" $GCP_SA_KEY_FROM_CI_SECRETS \" > service_account_key.json gcloud auth activate-service-account --key-file service_account_key.json gcloud container clusters get-credentials CLUSTER --zone ZONE --project GCP_PROJECT_ID Setup kubectl: kubectl config set-context CLUSTER_CONTEXT --cluster CLUSTER --user USER --namespace NAMESPACE kubectl config use-context CLUSTER_CONTEXT Deploy: kubectl apply -f compiled/app/manifests/","title":"Example workflow - Deploy to GKE"},{"location":"compile/","text":"Kapitan compile Note: make sure to read up on inventory before moving on. Specifying inputs and outputs Input types can be specified in the inventory under kapitan.compile in the following format: parameters : kapitan : compile : - output_path : <output_path_in_target_dir> input_type : jinja2 | jsonnet | kadet | helm | copy prune : <boolean> (Default : global --prune) input_paths : - path/to/input/dir/or/file - globbed/path/*/main.jsonnet output_type : <output_type_specific_to_input_type> Supported input types Kapitan supports the following input template types: jinja2 jsonnet kadet (alpha) helm (alpha) copy jinja2 This renders jinja2 templates, typically stored in templates/ directory, such as README, scripts and config files. Refer to jinja2 docs to understand how the template engine works. For Jinja2, input_paths can be either a file or a directory: in case of a directory, all the templates in the directory will be rendered and outputted to output_path . Supported output types : N/A (no need to specify output_type ) Using the inventory in jinja2 Jinja2 types will pass the \"inventory\" and whatever target vars as context keys in your template. This snippet renders the same java_opts for the elasticsearch data role: java_opts for elasticsearch data role are: {{ inventory.parameters.elasticsearch.roles.data.java_opts }} Jinja2 custom filters We support the following custom filters for use in Jinja2 templates: sha256 - SHA256 hashing of text e.g. {{ text | sha256 }} yaml - Dump text as YAML e.g. {{ text | yaml }} toml - Dump text as TOML e.g. {{ text | toml }} b64encode - base64 encode text e.g. {{ text | b64encode }} b64decode - base64 decode text e.g. {{ text | b64decode }} fileglob - return list of matched regular files for glob e.g. {{ ./path/file* | fileglob }} bool - return the bool for value e.g. {{ yes | bool }} to_datetime - return datetime object for string e.g. {{ \"2019-03-07 13:37:00\" | to_datetime }} strftime - return current date string for format e.g. {{ \"%a, %d %b %Y %H:%M\" | strftime }} regex_replace - perform a re.sub returning a string e.g. {{ hello world | regex_replace(pattern=\"world\", replacement=\"kapitan\") }} regex_escape - escape all regular expressions special characters from string e.g. {{ \"+s[a-z].*\" | regex_escape }} regex_search - perform re.search and return the list of matches or a backref e.g. {{ hello world | regex_search(\"world.*\") }} regex_findall - perform re.findall and return the list of matches as array e.g. {{ hello world | regex_findall(\"world.*\") }} ternary - value ? true_val : false_val e.g. {{ condition | ternary(\"yes\", \"no\") }} shuffle - randomly shuffle elements of a list {{ [1, 2, 3, 4, 5] | shuffle }} reveal_maybe - reveal ref/secret tag only if `compile --reveal` flag is set e.g. {{ \"?{ref:my_ref}\" | reveal_maybe }} You can also provide path to your custom filter modules in CLI. By default, you can put your filters in lib/jinja2_filters.py and they will automatically get loaded. jsonnet Jsonnet is a superset of json format that includes features such as conditionals, variables and imports. Refer to jsonnet docs to understand how it works. Note: unlike jinja2 templates, one jsonnet template can output multiple files (one per object declared in the file). Supported output types: yaml (default) json Using the inventory in jsonnet Typical jsonnet files would start as follows: local kap = import \"lib/kapitan.libjsonnet\"; local inventory = kap.inventory(); The first line is required to access the kapitan inventory values. On the second line, inventory() callback is used to initialise a local variable through which inventory values for this target can be referenced. For example, the script below local kap = import \"lib/kapitan.libjsonnet\"; local inventory = kap.inventory(); { \"data_java_opts\": inventory.parameters.elasticsearch.roles.data.java_opts, } imports the inventory for the target you're compiling and returns the java_opts for the elasticsearch data role. Note: The dictionary keys of the jsonnet object are used as filenames for the generated output files. If your jsonnet is not a dictionary, but is a valid json(net) object, then the output filename will be the same as the input filename. E.g. 'my_string' is inside templates/input_file.jsonnet so the generated output file will be named input_file.json for example and will contain \"my_string\" . Callback functions In addition, importing kapitan.libjsonnet makes available the following native_callback functions gluing reclass to jsonnet (amongst others): yaml_load - returns a json string of the specified yaml file yaml_load_stream - returns a list of json strings of the specified yaml file yaml_dump - returns a string yaml from a json string yaml_dump_stream - returns a string yaml stream from a json string file_read - reads the file specified file_exists - returns informative object if a file exists dir_files_list - returns a list of file in a dir dir_files_read - returns an object with keys - file_name and values - file contents jinja2_template - renders the jinja2 file with context specified sha256_string - returns sha256 of string gzip_b64 - returns base64 encoded gzip of obj inventory - returns a dictionary with the inventory for target jsonschema - validates obj with schema, returns object with 'valid' and 'reason' keys Jsonschema validation from jsonnet Given the follow example inventory: mysql: storage: 10G storage_class: standard image: mysql:latest The yaml inventory structure can be validated with the new jsonschema() function: local schema = { type: \"object\", properties: { storage: { type: \"string\", pattern: \"^[0-9]+[MGT]{1}$\"}, image: { type: \"string\" }, } }; // run jsonschema validation local validation = kap.jsonschema(inv.parameters.mysql, schema); // assert valid, otherwise error with validation.reason assert validation.valid: validation.reason; If validation.valid is not true, it will then fail compilation and display validation.reason . For example, if defining the storage value with an invalid pattern ( 10Z ), compile fails: Jsonnet error: failed to compile /code/components/mysql/main.jsonnet: RUNTIME ERROR: '10Z' does not match '^[0-9]+[MGT]{1}$' Failed validating 'pattern' in schema['properties']['storage']: {'pattern': '^[0-9]+[MGT]{1}$', 'type': 'string'} On instance['storage']: '10Z' /code/mysql/main.jsonnet:(19:1)-(43:2) Compile error: failed to compile target: minikube-mysql Jinja2 jsonnet templating The following jsonnet snippet renders the jinja2 template in templates/got.j2 : local kap = import \"lib/kapitan.libjsonnet\"; { \"jon_snow\": kap.jinja2_template(\"templates/got.j2\", { is_dead: false }), } It's up to you to decide what the output is. kadet Kadet is an extensible input type that enables you to generate templates using python. The key benefit being the ability to utilize familiar programing principles while having access to kapitan's powerful inventory system. A library that defines resources as classes using the Base Object class is required. These can then be utilized within components to render output. The following functions are provided by the class BaseObj() . Method definitions: new() : Provides parameter checking capabilities body() : Enables in-depth parameter configuration Method functions: root() : Defines values that will be compiled into the output need() : Ability to check & define input parameters update_root() : Updates the template file associated with the class A class can be a resource such as a kubernetes Deployment as shown here: class Deployment(BaseObj): def new(self): self.need(\"name\", \"name string needed\") self.need(\"labels\", \"labels dict needed\") self.need(\"containers\", \"containers dict needed\") self.update_root(\"lib/kubelib/deployment.yml\") def body(self): self.root.metadata.name = self.kwargs.name self.root.metadata.namespace = inv.parameters.target_name self.root.spec.template.metadata.labels = self.kwargs.labels self.root.spec.template.spec.containers = self.kwargs.containers The deployment is an BaseObj() which has two main functions. New can be used to perform parameter validation & template compilation. Body is utilized to set those parameters to be rendered. self.root.metadata.name is a direct reference to a key in the corresponding yaml. We have established that you may define a library which holds information on classes that represent resource objects. The library is then utilized by defined components to generate the required output. Here we import kubelib using load_from_search_paths() . We then use kubelib to access the defined object classes. In this instance the Deployment & Service resource class. ... kubelib = kadet.load_from_search_paths(\"kubelib\") ... name = \"nginx\" labels = kadet.BaseObj.from_dict({\"app\": name}) nginx_container = kubelib.Container( name=name, image=inv.parameters.nginx.image, ports=[{\"containerPort\": 80}] ) ... def main(): output = kadet.BaseObj() output.root.nginx_deployment = kubelib.Deployment(name=name, labels=labels, containers=[nginx_container]) output.root.nginx_service = kubelib.Service( name=name, labels=labels, ports=[svc_port], selector=svc_selector ) return output Kadet uses a library called addict to organise the parameters inline with the yaml templates. As shown above we create a BaseObject() named output. We update the root of this output with the data structure returned from kubelib. This output is what is then returned to kapitan to be compiled into the desired output type. For a deeper understanding of this input type please review the proposal document at kadet & the examples located at examples/kubernetes/components/nginx . Supported output types: yaml (default) json helm This is a Python binding to helm template command for users with helm charts. This does not require the helm executable, and the templates are rendered without the Tiller server. Unlike other input types, Helm input types support the following additional parameters under kapitan.compile : parameters : kapitan : compile : - output_path : <output_path> input_type : helm input_paths : - <chart_path> helm_values : <object_with_values_to_override> helm_values_files : - <values_file_path> helm_params : namespace : <substitutes_.Release.Namespace> name_template : <namespace_template> release_name : <chart_release_name> helm_values is an object containing values specified that will override the default values in the input chart. This has exactly the same effect as specifying --values custom_values.yml for helm template command where custom_values.yml structure mirrors that of helm_values . helm_values_files is an array containing the paths to helm values files used as input for the chart. This has exactly the same effect as specifying --file my_custom_values.yml for the helm template command where my_custom_values.yml is a helm values file. If the same keys exist in helm_values and in multiple specified helm_values_files , the last indexed file in the helm_values_files will take precedence followed by the preceding helm_values_files and at the bottom the helm_values defined in teh compile block. There is an example in the tests. The monitoring-dev (kapitan/tests/test_resources/inventory/targets/monitoring-dev.yml) and monitoring-prd (kapitan/tests/test_resources/inventory/targets/monitoring-prd.yml) targets both use the monitoring (tests/test_resources/inventory/classes/component/monitoring.yml) component. This component has helm chart input and takes a common.yml helm_values file which is \"shared\" by any target that uses the component and it also takes a dynamically defined file based on a kapitan variable defined in the target. helm_params correspond to the options for helm template as follows: namespace: equivalent of --namespace option: note that due to the restriction on helm template command, specifying the namespace does not automatically add metadata.namespace property to the resources. Therefore, users are encourage to explicitly specify in all resources: metadata : namespace : {{ .Release.Namespace }} # or any other custom values name_template: equivalent of --name-template option release_name: equivalent of --name option See the helm doc for further detail. Example Let's use nginx-ingress helm chart as the input. Using kapitan dependency manager , this chart can be fetched via a URL as listed in https://helm.nginx.com/stable/index.yaml . On a side note, https://helm.nginx.com/stable/ is the chart repository URL which you would helm repo add , and this repository should contain index.yaml that lists out all the available charts and their URLs. By locating this index.yaml file, you can locate all the charts available in the repository. We can use version 0.3.3 found at https://helm.nginx.com/stable/nginx-ingress-0.3.3.tgz . We can create a simple target file as inventory/targets/nginx-from-chart.yml whose content is as follows: parameters : kapitan : vars : target : nginx-from-chart dependencies : - type : https source : https://helm.nginx.com/stable/nginx-ingress-0.3.3.tgz unpack : True output_path : components/charts compile : - output_path : . input_type : helm input_paths : - components/charts/nginx-ingress helm_values : controller : name : my-controller image : repository : custom_repo helm_params : release_name : my-first-release-name namespace : my-first-namespace To compile this target, run: $ kapitan compile --fetch Dependency https://helm.nginx.com/stable/nginx-ingress-0.3.3.tgz : fetching now Dependency https://helm.nginx.com/stable/nginx-ingress-0.3.3.tgz : successfully fetched Dependency https://helm.nginx.com/stable/nginx-ingress-0.3.3.tgz : extracted to components/charts Compiled nginx-from-chart ( 0 .07s ) The chart is fetched before compile, which creates components/charts/nginx-ingress folder that is used as the input_paths for the helm input type. To confirm if the helm_values actually has overridden the default values, we can try: $ grep \"my-controller\" compiled/nginx-from-chart/nginx-ingress/templates/controller-deployment.yaml name: my-controller app: my-controller app: my-controller Building the binding from source Run cd kapitan/inputs/helm ./build.sh This requires Go 1.14. Helm subcharts There is an external dependency manager of type helm which enables you to specify helm charts to download, including subcharts. Copy This input type simply copies the input templates to the output directory without any rendering/processing. For Copy, input_paths can be either a file or a directory: in case of a directory, all the templates in the directory will be copied and outputted to output_path . Supported output types : N/A (no need to specify output_type )","title":"Compiling components/templates"},{"location":"compile/#kapitan-compile","text":"Note: make sure to read up on inventory before moving on.","title":"Kapitan compile"},{"location":"compile/#specifying-inputs-and-outputs","text":"Input types can be specified in the inventory under kapitan.compile in the following format: parameters : kapitan : compile : - output_path : <output_path_in_target_dir> input_type : jinja2 | jsonnet | kadet | helm | copy prune : <boolean> (Default : global --prune) input_paths : - path/to/input/dir/or/file - globbed/path/*/main.jsonnet output_type : <output_type_specific_to_input_type>","title":"Specifying inputs and outputs"},{"location":"compile/#supported-input-types","text":"Kapitan supports the following input template types: jinja2 jsonnet kadet (alpha) helm (alpha) copy","title":"Supported input types"},{"location":"compile/#jinja2","text":"This renders jinja2 templates, typically stored in templates/ directory, such as README, scripts and config files. Refer to jinja2 docs to understand how the template engine works. For Jinja2, input_paths can be either a file or a directory: in case of a directory, all the templates in the directory will be rendered and outputted to output_path . Supported output types : N/A (no need to specify output_type )","title":"jinja2"},{"location":"compile/#using-the-inventory-in-jinja2","text":"Jinja2 types will pass the \"inventory\" and whatever target vars as context keys in your template. This snippet renders the same java_opts for the elasticsearch data role: java_opts for elasticsearch data role are: {{ inventory.parameters.elasticsearch.roles.data.java_opts }}","title":"Using the inventory in jinja2"},{"location":"compile/#jinja2-custom-filters","text":"We support the following custom filters for use in Jinja2 templates: sha256 - SHA256 hashing of text e.g. {{ text | sha256 }} yaml - Dump text as YAML e.g. {{ text | yaml }} toml - Dump text as TOML e.g. {{ text | toml }} b64encode - base64 encode text e.g. {{ text | b64encode }} b64decode - base64 decode text e.g. {{ text | b64decode }} fileglob - return list of matched regular files for glob e.g. {{ ./path/file* | fileglob }} bool - return the bool for value e.g. {{ yes | bool }} to_datetime - return datetime object for string e.g. {{ \"2019-03-07 13:37:00\" | to_datetime }} strftime - return current date string for format e.g. {{ \"%a, %d %b %Y %H:%M\" | strftime }} regex_replace - perform a re.sub returning a string e.g. {{ hello world | regex_replace(pattern=\"world\", replacement=\"kapitan\") }} regex_escape - escape all regular expressions special characters from string e.g. {{ \"+s[a-z].*\" | regex_escape }} regex_search - perform re.search and return the list of matches or a backref e.g. {{ hello world | regex_search(\"world.*\") }} regex_findall - perform re.findall and return the list of matches as array e.g. {{ hello world | regex_findall(\"world.*\") }} ternary - value ? true_val : false_val e.g. {{ condition | ternary(\"yes\", \"no\") }} shuffle - randomly shuffle elements of a list {{ [1, 2, 3, 4, 5] | shuffle }} reveal_maybe - reveal ref/secret tag only if `compile --reveal` flag is set e.g. {{ \"?{ref:my_ref}\" | reveal_maybe }} You can also provide path to your custom filter modules in CLI. By default, you can put your filters in lib/jinja2_filters.py and they will automatically get loaded.","title":"Jinja2 custom filters"},{"location":"compile/#jsonnet","text":"Jsonnet is a superset of json format that includes features such as conditionals, variables and imports. Refer to jsonnet docs to understand how it works. Note: unlike jinja2 templates, one jsonnet template can output multiple files (one per object declared in the file). Supported output types: yaml (default) json","title":"jsonnet"},{"location":"compile/#using-the-inventory-in-jsonnet","text":"Typical jsonnet files would start as follows: local kap = import \"lib/kapitan.libjsonnet\"; local inventory = kap.inventory(); The first line is required to access the kapitan inventory values. On the second line, inventory() callback is used to initialise a local variable through which inventory values for this target can be referenced. For example, the script below local kap = import \"lib/kapitan.libjsonnet\"; local inventory = kap.inventory(); { \"data_java_opts\": inventory.parameters.elasticsearch.roles.data.java_opts, } imports the inventory for the target you're compiling and returns the java_opts for the elasticsearch data role. Note: The dictionary keys of the jsonnet object are used as filenames for the generated output files. If your jsonnet is not a dictionary, but is a valid json(net) object, then the output filename will be the same as the input filename. E.g. 'my_string' is inside templates/input_file.jsonnet so the generated output file will be named input_file.json for example and will contain \"my_string\" .","title":"Using the inventory in jsonnet"},{"location":"compile/#callback-functions","text":"In addition, importing kapitan.libjsonnet makes available the following native_callback functions gluing reclass to jsonnet (amongst others): yaml_load - returns a json string of the specified yaml file yaml_load_stream - returns a list of json strings of the specified yaml file yaml_dump - returns a string yaml from a json string yaml_dump_stream - returns a string yaml stream from a json string file_read - reads the file specified file_exists - returns informative object if a file exists dir_files_list - returns a list of file in a dir dir_files_read - returns an object with keys - file_name and values - file contents jinja2_template - renders the jinja2 file with context specified sha256_string - returns sha256 of string gzip_b64 - returns base64 encoded gzip of obj inventory - returns a dictionary with the inventory for target jsonschema - validates obj with schema, returns object with 'valid' and 'reason' keys","title":"Callback functions"},{"location":"compile/#jsonschema-validation-from-jsonnet","text":"Given the follow example inventory: mysql: storage: 10G storage_class: standard image: mysql:latest The yaml inventory structure can be validated with the new jsonschema() function: local schema = { type: \"object\", properties: { storage: { type: \"string\", pattern: \"^[0-9]+[MGT]{1}$\"}, image: { type: \"string\" }, } }; // run jsonschema validation local validation = kap.jsonschema(inv.parameters.mysql, schema); // assert valid, otherwise error with validation.reason assert validation.valid: validation.reason; If validation.valid is not true, it will then fail compilation and display validation.reason . For example, if defining the storage value with an invalid pattern ( 10Z ), compile fails: Jsonnet error: failed to compile /code/components/mysql/main.jsonnet: RUNTIME ERROR: '10Z' does not match '^[0-9]+[MGT]{1}$' Failed validating 'pattern' in schema['properties']['storage']: {'pattern': '^[0-9]+[MGT]{1}$', 'type': 'string'} On instance['storage']: '10Z' /code/mysql/main.jsonnet:(19:1)-(43:2) Compile error: failed to compile target: minikube-mysql","title":"Jsonschema validation from jsonnet"},{"location":"compile/#jinja2-jsonnet-templating","text":"The following jsonnet snippet renders the jinja2 template in templates/got.j2 : local kap = import \"lib/kapitan.libjsonnet\"; { \"jon_snow\": kap.jinja2_template(\"templates/got.j2\", { is_dead: false }), } It's up to you to decide what the output is.","title":"Jinja2 jsonnet templating"},{"location":"compile/#kadet","text":"Kadet is an extensible input type that enables you to generate templates using python. The key benefit being the ability to utilize familiar programing principles while having access to kapitan's powerful inventory system. A library that defines resources as classes using the Base Object class is required. These can then be utilized within components to render output. The following functions are provided by the class BaseObj() . Method definitions: new() : Provides parameter checking capabilities body() : Enables in-depth parameter configuration Method functions: root() : Defines values that will be compiled into the output need() : Ability to check & define input parameters update_root() : Updates the template file associated with the class A class can be a resource such as a kubernetes Deployment as shown here: class Deployment(BaseObj): def new(self): self.need(\"name\", \"name string needed\") self.need(\"labels\", \"labels dict needed\") self.need(\"containers\", \"containers dict needed\") self.update_root(\"lib/kubelib/deployment.yml\") def body(self): self.root.metadata.name = self.kwargs.name self.root.metadata.namespace = inv.parameters.target_name self.root.spec.template.metadata.labels = self.kwargs.labels self.root.spec.template.spec.containers = self.kwargs.containers The deployment is an BaseObj() which has two main functions. New can be used to perform parameter validation & template compilation. Body is utilized to set those parameters to be rendered. self.root.metadata.name is a direct reference to a key in the corresponding yaml. We have established that you may define a library which holds information on classes that represent resource objects. The library is then utilized by defined components to generate the required output. Here we import kubelib using load_from_search_paths() . We then use kubelib to access the defined object classes. In this instance the Deployment & Service resource class. ... kubelib = kadet.load_from_search_paths(\"kubelib\") ... name = \"nginx\" labels = kadet.BaseObj.from_dict({\"app\": name}) nginx_container = kubelib.Container( name=name, image=inv.parameters.nginx.image, ports=[{\"containerPort\": 80}] ) ... def main(): output = kadet.BaseObj() output.root.nginx_deployment = kubelib.Deployment(name=name, labels=labels, containers=[nginx_container]) output.root.nginx_service = kubelib.Service( name=name, labels=labels, ports=[svc_port], selector=svc_selector ) return output Kadet uses a library called addict to organise the parameters inline with the yaml templates. As shown above we create a BaseObject() named output. We update the root of this output with the data structure returned from kubelib. This output is what is then returned to kapitan to be compiled into the desired output type. For a deeper understanding of this input type please review the proposal document at kadet & the examples located at examples/kubernetes/components/nginx . Supported output types: yaml (default) json","title":"kadet"},{"location":"compile/#helm","text":"This is a Python binding to helm template command for users with helm charts. This does not require the helm executable, and the templates are rendered without the Tiller server. Unlike other input types, Helm input types support the following additional parameters under kapitan.compile : parameters : kapitan : compile : - output_path : <output_path> input_type : helm input_paths : - <chart_path> helm_values : <object_with_values_to_override> helm_values_files : - <values_file_path> helm_params : namespace : <substitutes_.Release.Namespace> name_template : <namespace_template> release_name : <chart_release_name> helm_values is an object containing values specified that will override the default values in the input chart. This has exactly the same effect as specifying --values custom_values.yml for helm template command where custom_values.yml structure mirrors that of helm_values . helm_values_files is an array containing the paths to helm values files used as input for the chart. This has exactly the same effect as specifying --file my_custom_values.yml for the helm template command where my_custom_values.yml is a helm values file. If the same keys exist in helm_values and in multiple specified helm_values_files , the last indexed file in the helm_values_files will take precedence followed by the preceding helm_values_files and at the bottom the helm_values defined in teh compile block. There is an example in the tests. The monitoring-dev (kapitan/tests/test_resources/inventory/targets/monitoring-dev.yml) and monitoring-prd (kapitan/tests/test_resources/inventory/targets/monitoring-prd.yml) targets both use the monitoring (tests/test_resources/inventory/classes/component/monitoring.yml) component. This component has helm chart input and takes a common.yml helm_values file which is \"shared\" by any target that uses the component and it also takes a dynamically defined file based on a kapitan variable defined in the target. helm_params correspond to the options for helm template as follows: namespace: equivalent of --namespace option: note that due to the restriction on helm template command, specifying the namespace does not automatically add metadata.namespace property to the resources. Therefore, users are encourage to explicitly specify in all resources: metadata : namespace : {{ .Release.Namespace }} # or any other custom values name_template: equivalent of --name-template option release_name: equivalent of --name option See the helm doc for further detail.","title":"helm"},{"location":"compile/#example","text":"Let's use nginx-ingress helm chart as the input. Using kapitan dependency manager , this chart can be fetched via a URL as listed in https://helm.nginx.com/stable/index.yaml . On a side note, https://helm.nginx.com/stable/ is the chart repository URL which you would helm repo add , and this repository should contain index.yaml that lists out all the available charts and their URLs. By locating this index.yaml file, you can locate all the charts available in the repository. We can use version 0.3.3 found at https://helm.nginx.com/stable/nginx-ingress-0.3.3.tgz . We can create a simple target file as inventory/targets/nginx-from-chart.yml whose content is as follows: parameters : kapitan : vars : target : nginx-from-chart dependencies : - type : https source : https://helm.nginx.com/stable/nginx-ingress-0.3.3.tgz unpack : True output_path : components/charts compile : - output_path : . input_type : helm input_paths : - components/charts/nginx-ingress helm_values : controller : name : my-controller image : repository : custom_repo helm_params : release_name : my-first-release-name namespace : my-first-namespace To compile this target, run: $ kapitan compile --fetch Dependency https://helm.nginx.com/stable/nginx-ingress-0.3.3.tgz : fetching now Dependency https://helm.nginx.com/stable/nginx-ingress-0.3.3.tgz : successfully fetched Dependency https://helm.nginx.com/stable/nginx-ingress-0.3.3.tgz : extracted to components/charts Compiled nginx-from-chart ( 0 .07s ) The chart is fetched before compile, which creates components/charts/nginx-ingress folder that is used as the input_paths for the helm input type. To confirm if the helm_values actually has overridden the default values, we can try: $ grep \"my-controller\" compiled/nginx-from-chart/nginx-ingress/templates/controller-deployment.yaml name: my-controller app: my-controller app: my-controller","title":"Example"},{"location":"compile/#building-the-binding-from-source","text":"Run cd kapitan/inputs/helm ./build.sh This requires Go 1.14.","title":"Building the binding from source"},{"location":"compile/#helm-subcharts","text":"There is an external dependency manager of type helm which enables you to specify helm charts to download, including subcharts.","title":"Helm subcharts"},{"location":"compile/#copy","text":"This input type simply copies the input templates to the output directory without any rendering/processing. For Copy, input_paths can be either a file or a directory: in case of a directory, all the templates in the directory will be copied and outputted to output_path . Supported output types : N/A (no need to specify output_type )","title":"Copy"},{"location":"contributing/","text":"How to Contribute We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow. Submitting changes We would like ask you to fork Kapitan project and create a Pull Request targeting master branch. All submissions, including submissions by project members, require review. Setting up environment We highly recommend that you create a dedicated Python environment for Kapitan. There are multiple solutions: pyenv virtualenv venv Once you've done it, please install all Kapitan's dependencies: pip3 install -r requirements.txt Because we are using a pinned version of reclass which is added as a submodule into Kapitan's repository, you need to pull it separately by executing the command below: git submodule update --init Testing Run make test to run all tests. If you modify anything in the examples/ folder make sure you replicate the compiled result of that in tests/test_kubernetes_compiled . If you add new features, run make test_coverage && make test_formatting to make sure the test coverage remains at current or better levels and that code formatting is applied. If you would like to evaluate your changes by running your version of Kapitan, you can do that by running bin/kapitan from this repository or even setting an alias to it. Code Style Run make format_codestyle before submitting to make sure you adhere to the Style Guide for Python (PEP8) . Python Black is used to apply the formatting so make sure you have it installed with pip3 install black . Releasing Create a branch named release-v<NUMBER> . Use v0.*.*-rc.* if you want pre-release versions to be uploaded. Update CHANGELOG.md with the release changes. Once reviewed and merged, Travis will auto-release. The merge has to happen with a merge commit not with squash/rebase so that the commit message still mentions deepmind/release-v* inside. Updating gh-pages docs We use mkdocs to generate our gh-pages from .md files under docs/ folder. Updating our gh-pages is therefore a two-step process. 1. Update the .md files Submit a PR for our master branch that updates the .md file(s). Test how the changes would look like when deployed to gh-pages by serving it on localhost: # from project root pip install mkdocs mkdocs-material pymdown-extensions markdown-include mkdocs build mkdocs serve # open http://127.0.0.1:8000 2. Submit a PR for gh-pages branch to deploy the update Once the above PR has been merged, use mkdocs gh-deploy command to push the commit that updates the site content to your own gh-pages branch. Make sure that you already have this gh-pages branch in your fork that is up-to-date with our gh-pages branch such that the two branches share the commit history (otherwise Github would not allow PRs to be created). # locally, on master branch (which has your updated docs) mkdocs gh-deploy -m \"Commit message\" -f ./mkdocs.yml -b gh-pages After it's pushed, create a PR that targets our gh-pages branch from your gh-pages branch. Packaging extra resources in python package or binary To package any extra resources/files in the pip package or the kapitan binary, make sure you modify both MANIFEST.in and scripts/pyinstaller.sh . Contributor License Agreement Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution, this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributing"},{"location":"contributing/#how-to-contribute","text":"We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.","title":"How to Contribute"},{"location":"contributing/#submitting-changes","text":"We would like ask you to fork Kapitan project and create a Pull Request targeting master branch. All submissions, including submissions by project members, require review.","title":"Submitting changes"},{"location":"contributing/#setting-up-environment","text":"We highly recommend that you create a dedicated Python environment for Kapitan. There are multiple solutions: pyenv virtualenv venv Once you've done it, please install all Kapitan's dependencies: pip3 install -r requirements.txt Because we are using a pinned version of reclass which is added as a submodule into Kapitan's repository, you need to pull it separately by executing the command below: git submodule update --init","title":"Setting up environment"},{"location":"contributing/#testing","text":"Run make test to run all tests. If you modify anything in the examples/ folder make sure you replicate the compiled result of that in tests/test_kubernetes_compiled . If you add new features, run make test_coverage && make test_formatting to make sure the test coverage remains at current or better levels and that code formatting is applied. If you would like to evaluate your changes by running your version of Kapitan, you can do that by running bin/kapitan from this repository or even setting an alias to it.","title":"Testing"},{"location":"contributing/#code-style","text":"Run make format_codestyle before submitting to make sure you adhere to the Style Guide for Python (PEP8) . Python Black is used to apply the formatting so make sure you have it installed with pip3 install black .","title":"Code Style"},{"location":"contributing/#releasing","text":"Create a branch named release-v<NUMBER> . Use v0.*.*-rc.* if you want pre-release versions to be uploaded. Update CHANGELOG.md with the release changes. Once reviewed and merged, Travis will auto-release. The merge has to happen with a merge commit not with squash/rebase so that the commit message still mentions deepmind/release-v* inside.","title":"Releasing"},{"location":"contributing/#updating-gh-pages-docs","text":"We use mkdocs to generate our gh-pages from .md files under docs/ folder. Updating our gh-pages is therefore a two-step process.","title":"Updating gh-pages docs"},{"location":"contributing/#1-update-the-md-files","text":"Submit a PR for our master branch that updates the .md file(s). Test how the changes would look like when deployed to gh-pages by serving it on localhost: # from project root pip install mkdocs mkdocs-material pymdown-extensions markdown-include mkdocs build mkdocs serve # open http://127.0.0.1:8000","title":"1. Update the .md files"},{"location":"contributing/#2-submit-a-pr-for-gh-pages-branch-to-deploy-the-update","text":"Once the above PR has been merged, use mkdocs gh-deploy command to push the commit that updates the site content to your own gh-pages branch. Make sure that you already have this gh-pages branch in your fork that is up-to-date with our gh-pages branch such that the two branches share the commit history (otherwise Github would not allow PRs to be created). # locally, on master branch (which has your updated docs) mkdocs gh-deploy -m \"Commit message\" -f ./mkdocs.yml -b gh-pages After it's pushed, create a PR that targets our gh-pages branch from your gh-pages branch.","title":"2. Submit a PR for gh-pages branch to deploy the update"},{"location":"contributing/#packaging-extra-resources-in-python-package-or-binary","text":"To package any extra resources/files in the pip package or the kapitan binary, make sure you modify both MANIFEST.in and scripts/pyinstaller.sh .","title":"Packaging extra resources in python package or binary"},{"location":"contributing/#contributor-license-agreement","text":"Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution, this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributor License Agreement"},{"location":"example-kubernetes/","text":"Kubernetes example Here, we walk through how kapitan could be used to help create kubernetes manifests, whose values are customized for each target according to the inventory structure. The example folder can be found in our repository on Github at https://github.com/deepmind/kapitan/tree/master/examples/kubernetes . Directory structure The following tree shows what this directory looks like (only showing tree level 1): \u251c\u2500\u2500 components \u251c\u2500\u2500 docs \u251c\u2500\u2500 inventory \u251c\u2500\u2500 lib \u251c\u2500\u2500 scripts \u251c\u2500\u2500 refs \u2514\u2500\u2500 templates We will describe the role of each folder in the following sections. inventory This folder contains the inventory values used to render the templates for each target. The structure of this folder is as follows: . \u251c\u2500\u2500 classes \u2502 \u251c\u2500\u2500 cluster \u2502 \u2502 \u251c\u2500\u2500 common.yml \u2502 \u2502 \u2514\u2500\u2500 minikube.yml \u2502 \u251c\u2500\u2500 common.yml \u2502 \u2514\u2500\u2500 component \u2502 \u251c\u2500\u2500 elasticsearch.yml \u2502 \u251c\u2500\u2500 mysql.yml \u2502 \u251c\u2500\u2500 namespace.yml \u2502 \u2514\u2500\u2500 nginx.yml \u2514\u2500\u2500 targets \u251c\u2500\u2500 minikube-es.yml \u251c\u2500\u2500 minikube-mysql.yml \u2514\u2500\u2500 minikube-nginx.yml The required sub-folder is targets : during compile, kapitan searches for the yaml files under targets in order to identify the targets. In this example, there are three targets: minikube-es minikube-mysql minikube-nginx Therefore, when you run kapitan compile , under the compiled folder that kapitan generates, you will see three folders named after these targets. classes is a folder that contains yaml files used as the \"base class\" in the hierarchical inventory database. The values defined here are inherited by the target files. For more explanation on how this works, look at the inventory documentation . Notice how the classes are nicely divided up into components and clusters, such as nginx and mysql, in order to clearly define what components each target should contain and to make the classes reusable. For example, take a look at targets/nginx.yml : classes : - common - cluster.minikube - component.namespace - component.nginx parameters : target_name : minikube-nginx namespace : ${target_name} This target inherits values from four files under classes folder: common.yml cluster/minikube.yml component/namespace.yml component/nginx.yml Note: that some of these classes themselves may inherit from other classes. And the way classes are defined makes it easy to identify what components and clusters this target should contain and belong to! Let's take a close look now at component/namespace.yml : parameters : namespace : ${target_name} kapitan : compile : - output_path : pre-deploy input_type : jsonnet output_type : yaml input_paths : - components/namespace/main.jsonnet As we see, this file declares a kapitan.compile item whose input path (i.e. the template file) is components/namespace/main.jsonnet which, when rendered, will generate yaml file(s) under compiled/minikube-nginx/pre-deploy . Don't confuse the components folder with inventory/classes/components folder: the former contains the actual templates, while the latter contains inventory classes. components This folder contains the template files as discussed above, typically jsonnet and kadet files. The tree of this directory looks as follows: . \u251c\u2500\u2500 elasticsearch \u2502 \u251c\u2500\u2500 elasticsearch.container.jsonnet \u2502 \u251c\u2500\u2500 elasticsearch.statefulset.jsonnet \u2502 \u2514\u2500\u2500 main.jsonnet \u251c\u2500\u2500 mysql \u2502 \u251c\u2500\u2500 main.jsonnet \u2502 \u251c\u2500\u2500 secret.jsonnet \u2502 \u251c\u2500\u2500 service.jsonnet \u2502 \u2514\u2500\u2500 statefulset.jsonnet \u251c\u2500\u2500 namespace \u2502 \u2514\u2500\u2500 main.jsonnet \u2514\u2500\u2500 nginx \u2514\u2500\u2500 __init__.py Notice how the directory structure corresponds to that of inventory/classes/components in order to make it easy to identify which templates are used for which components. As mentioned above, we know that the target minikube-nginx inherits from component.namespace . Let's take a look at components/namespace/main.jsonnet : local kube = import \"lib/kube.libjsonnet\"; local kap = import \"lib/kapitan.libjsonnet\"; local inventory = kap.inventory(); local p = inventory.parameters; { \"00_namespace\": kube.Namespace(p.namespace), \"10_serviceaccount\": kube.ServiceAccount(\"default\") } The first two lines import libjsonnet files under lib folder: this is the folder that contains helper files used inside templates. For example, kapitan.libjsonnet allows you to access inventory values inside jsonnet templates, and kube.libjsonnet defines functions to generate popular kubernetes manifests. The actual object defined in components/namespace/main.jsonnet looks like this: { \"00_namespace\": kube.Namespace(p.namespace), \"10_serviceaccount\": kube.ServiceAccount(\"default\") } We have \"00_namespace\" and \"10_serviceaccount\" as the keys. These will become files under compiled/minikube-nginx/pre-deploy , since pre-deploy is the input_paths declared in the inventory. For instance, 00_namespace.yml would look like this: apiVersion : v1 kind : Namespace metadata : annotations : {} labels : name : minikube-nginx name : minikube-nginx namespace : minikube-nginx spec : {} templates, docs, scripts These folders contain jinja2 template files. For example, component.elasticsearch contains: kapitan : compile : # other items abbreviated for clarity - output_path : scripts input_type : jinja2 input_paths : - scripts - output_path : . input_type : jinja2 input_paths : - docs/elasticsearch/README.md Since component.elasticsearch is inherited by the target minikube-es , this generates files under compiled/minikube-es/scripts and compiled/minikube-es/README.md . secrets This folder contains secrets created manually by the user, or automatically by kapitan. Refer to secrets management for how it works. In this example, the configuration, such as the recipients, is declared in inventory/classes/common.yml : parameters : kapitan : vars : target : ${target_name} namespace : ${target_name} secrets : gpg : recipients : - name : example@kapitan.dev fingerprint : D9234C61F58BEB3ED8552A57E28DC07A3CBFAE7C The references to the secrets are declared in inventory/classes/component/mysql , which is inherited by the target minikube-mysql . After running kapitan compile , some of the generated manifests contain the references to secrets. For example, have a look at compiled/minikube-mysql/manifests/mysql_secret.yml : apiVersion : v1 data : MYSQL_ROOT_PASSWORD : ?{gpg:targets/minikube-mysql/mysql/password:ec3d54de} MYSQL_ROOT_PASSWORD_SHA256 : ?{gpg:targets/minikube-mysql/mysql/password_sha256:122d2732} kind : Secret metadata : annotations : {} labels : name : example-mysql name : example-mysql namespace : minikube-mysql type : Opaque MYSQL_ROOT_PASSWORD refers to the secret stored in refs/targets/minikube-mysql/mysql/password and so on. You may reveal the secrets by running kapitan refs --reveal -f mysql_secret.yml and use the manifest by piping the output to kubectl!","title":"Kubernetes"},{"location":"example-kubernetes/#kubernetes-example","text":"Here, we walk through how kapitan could be used to help create kubernetes manifests, whose values are customized for each target according to the inventory structure. The example folder can be found in our repository on Github at https://github.com/deepmind/kapitan/tree/master/examples/kubernetes .","title":"Kubernetes example"},{"location":"example-kubernetes/#directory-structure","text":"The following tree shows what this directory looks like (only showing tree level 1): \u251c\u2500\u2500 components \u251c\u2500\u2500 docs \u251c\u2500\u2500 inventory \u251c\u2500\u2500 lib \u251c\u2500\u2500 scripts \u251c\u2500\u2500 refs \u2514\u2500\u2500 templates We will describe the role of each folder in the following sections.","title":"Directory structure"},{"location":"example-kubernetes/#inventory","text":"This folder contains the inventory values used to render the templates for each target. The structure of this folder is as follows: . \u251c\u2500\u2500 classes \u2502 \u251c\u2500\u2500 cluster \u2502 \u2502 \u251c\u2500\u2500 common.yml \u2502 \u2502 \u2514\u2500\u2500 minikube.yml \u2502 \u251c\u2500\u2500 common.yml \u2502 \u2514\u2500\u2500 component \u2502 \u251c\u2500\u2500 elasticsearch.yml \u2502 \u251c\u2500\u2500 mysql.yml \u2502 \u251c\u2500\u2500 namespace.yml \u2502 \u2514\u2500\u2500 nginx.yml \u2514\u2500\u2500 targets \u251c\u2500\u2500 minikube-es.yml \u251c\u2500\u2500 minikube-mysql.yml \u2514\u2500\u2500 minikube-nginx.yml The required sub-folder is targets : during compile, kapitan searches for the yaml files under targets in order to identify the targets. In this example, there are three targets: minikube-es minikube-mysql minikube-nginx Therefore, when you run kapitan compile , under the compiled folder that kapitan generates, you will see three folders named after these targets. classes is a folder that contains yaml files used as the \"base class\" in the hierarchical inventory database. The values defined here are inherited by the target files. For more explanation on how this works, look at the inventory documentation . Notice how the classes are nicely divided up into components and clusters, such as nginx and mysql, in order to clearly define what components each target should contain and to make the classes reusable. For example, take a look at targets/nginx.yml : classes : - common - cluster.minikube - component.namespace - component.nginx parameters : target_name : minikube-nginx namespace : ${target_name} This target inherits values from four files under classes folder: common.yml cluster/minikube.yml component/namespace.yml component/nginx.yml Note: that some of these classes themselves may inherit from other classes. And the way classes are defined makes it easy to identify what components and clusters this target should contain and belong to! Let's take a close look now at component/namespace.yml : parameters : namespace : ${target_name} kapitan : compile : - output_path : pre-deploy input_type : jsonnet output_type : yaml input_paths : - components/namespace/main.jsonnet As we see, this file declares a kapitan.compile item whose input path (i.e. the template file) is components/namespace/main.jsonnet which, when rendered, will generate yaml file(s) under compiled/minikube-nginx/pre-deploy . Don't confuse the components folder with inventory/classes/components folder: the former contains the actual templates, while the latter contains inventory classes.","title":"inventory"},{"location":"example-kubernetes/#components","text":"This folder contains the template files as discussed above, typically jsonnet and kadet files. The tree of this directory looks as follows: . \u251c\u2500\u2500 elasticsearch \u2502 \u251c\u2500\u2500 elasticsearch.container.jsonnet \u2502 \u251c\u2500\u2500 elasticsearch.statefulset.jsonnet \u2502 \u2514\u2500\u2500 main.jsonnet \u251c\u2500\u2500 mysql \u2502 \u251c\u2500\u2500 main.jsonnet \u2502 \u251c\u2500\u2500 secret.jsonnet \u2502 \u251c\u2500\u2500 service.jsonnet \u2502 \u2514\u2500\u2500 statefulset.jsonnet \u251c\u2500\u2500 namespace \u2502 \u2514\u2500\u2500 main.jsonnet \u2514\u2500\u2500 nginx \u2514\u2500\u2500 __init__.py Notice how the directory structure corresponds to that of inventory/classes/components in order to make it easy to identify which templates are used for which components. As mentioned above, we know that the target minikube-nginx inherits from component.namespace . Let's take a look at components/namespace/main.jsonnet : local kube = import \"lib/kube.libjsonnet\"; local kap = import \"lib/kapitan.libjsonnet\"; local inventory = kap.inventory(); local p = inventory.parameters; { \"00_namespace\": kube.Namespace(p.namespace), \"10_serviceaccount\": kube.ServiceAccount(\"default\") } The first two lines import libjsonnet files under lib folder: this is the folder that contains helper files used inside templates. For example, kapitan.libjsonnet allows you to access inventory values inside jsonnet templates, and kube.libjsonnet defines functions to generate popular kubernetes manifests. The actual object defined in components/namespace/main.jsonnet looks like this: { \"00_namespace\": kube.Namespace(p.namespace), \"10_serviceaccount\": kube.ServiceAccount(\"default\") } We have \"00_namespace\" and \"10_serviceaccount\" as the keys. These will become files under compiled/minikube-nginx/pre-deploy , since pre-deploy is the input_paths declared in the inventory. For instance, 00_namespace.yml would look like this: apiVersion : v1 kind : Namespace metadata : annotations : {} labels : name : minikube-nginx name : minikube-nginx namespace : minikube-nginx spec : {}","title":"components"},{"location":"example-kubernetes/#templates-docs-scripts","text":"These folders contain jinja2 template files. For example, component.elasticsearch contains: kapitan : compile : # other items abbreviated for clarity - output_path : scripts input_type : jinja2 input_paths : - scripts - output_path : . input_type : jinja2 input_paths : - docs/elasticsearch/README.md Since component.elasticsearch is inherited by the target minikube-es , this generates files under compiled/minikube-es/scripts and compiled/minikube-es/README.md .","title":"templates, docs, scripts"},{"location":"example-kubernetes/#secrets","text":"This folder contains secrets created manually by the user, or automatically by kapitan. Refer to secrets management for how it works. In this example, the configuration, such as the recipients, is declared in inventory/classes/common.yml : parameters : kapitan : vars : target : ${target_name} namespace : ${target_name} secrets : gpg : recipients : - name : example@kapitan.dev fingerprint : D9234C61F58BEB3ED8552A57E28DC07A3CBFAE7C The references to the secrets are declared in inventory/classes/component/mysql , which is inherited by the target minikube-mysql . After running kapitan compile , some of the generated manifests contain the references to secrets. For example, have a look at compiled/minikube-mysql/manifests/mysql_secret.yml : apiVersion : v1 data : MYSQL_ROOT_PASSWORD : ?{gpg:targets/minikube-mysql/mysql/password:ec3d54de} MYSQL_ROOT_PASSWORD_SHA256 : ?{gpg:targets/minikube-mysql/mysql/password_sha256:122d2732} kind : Secret metadata : annotations : {} labels : name : example-mysql name : example-mysql namespace : minikube-mysql type : Opaque MYSQL_ROOT_PASSWORD refers to the secret stored in refs/targets/minikube-mysql/mysql/password and so on. You may reveal the secrets by running kapitan refs --reveal -f mysql_secret.yml and use the manifest by piping the output to kubectl!","title":"secrets"},{"location":"example-terraform/","text":"Terraform example We will be looking at how to use Kapitan to compile terraform files with Jsonnet as the input type. It's possible to use other input types, however, Jsonnet is recommended. For example, we could use the Kadet input to generate terraform files but this would require templates to be written in YAML then rendered into JSON. It is possible to allow Kadet to consume JSON as an input. This enables you to integrate your organizations pre-existing terraform JSON file's as templates. Jsonnet is the most straightforward input type as you will see due to its functional nature. The only appropriate output type is JSON since this is the format that Terraform consumes. Directory structure There are several examples available in examples/terraform . This will be our working directory for this documentation. The directory structure is as follows: \u251c\u2500\u2500 inventory \u2514\u2500\u2500 templates It is possible to further extend this locally to include a lib directory where a terraform.libjsonnet file can be stored for use. This is generally dependent on the project scope and organizational patterns. We will describe in more detail the role of each of these folders in the following sections. inventory This folder contains the inventory files used to render the templates for each target. The structure of this folder is as follows: . \u251c\u2500\u2500 classes \u2502 \u251c\u2500\u2500 env \u2502 \u2502 \u251c\u2500\u2500 develop.yml \u2502 \u2502 \u251c\u2500\u2500 prod.yml \u2502 \u2502 \u2514\u2500\u2500 sandbox.yml \u2502 \u251c\u2500\u2500 provider \u2502 \u2502 \u2514\u2500\u2500 gcp.yml \u2502 \u2514\u2500\u2500 type \u2502 \u2514\u2500\u2500 terraform.yml \u251c\u2500\u2500 reclass-config.yml \u2514\u2500\u2500 targets \u251c\u2500\u2500 develop \u2502 \u2514\u2500\u2500 project1.yml \u251c\u2500\u2500 prod \u2502 \u2514\u2500\u2500 project2.yml \u2514\u2500\u2500 sandbox \u2514\u2500\u2500 project3.yml The targets directory enables us to define various projects. We can specify each project as an environment such as dev , staging and production with each having unique parameters. The following is an example targets file. type.terraform is what defines the entry point into the main Jsonnet template file. The parameters in the file inventory/targets/develop/project1.yml will then be utilized to set the environmental specific provider/resource configuration. We define the default region and zone for terraform's provider configuration. The default DNS TTL for the DNS resource is also configured for the development environment. classes : - type.terraform parameters : name : project1 region : europe-west2 zone : europe-west2-a dns_default_ttl : 300 In the following example, we use a reclass configuration file to specify further parameters that we would like to merge into our project files. Thus we define nodes, which are stored in targets and environmental mandatory parameters stored in classes/env/ . The reclass config is shown below: storage_type : yaml_fs pretty_print : true output : yaml inventory_base_uri : . nodes_uri : targets classes_uri : classes compose_node_name : false class_mappings : - develop/* env.develop - prod/* env.prod - sandbox/* env.sandbox The following class provider.gcp will be found in all files in this path since it is a common configuration for the cloud authentication module. classes : - provider.gcp Further classes that group parameters together can be included. To assist in further refining the configuration. components We tend to use components as a method to organize Jsonnet files. This is not mandatory since it is possible to configure Kapitan to look for input files wherever you would like. You can have these in any path just ensure you define that path in inventory/classes/type/terraform.yml . The templates folder is where the Jsonnet is located in this instance as shown below: . \u251c\u2500\u2500 cloudbuild.jsonnet \u251c\u2500\u2500 dns.jsonnet \u251c\u2500\u2500 iam.jsonnet \u251c\u2500\u2500 iam_service_account.jsonnet \u251c\u2500\u2500 kms.jsonnet \u251c\u2500\u2500 kubernetes.jsonnet \u251c\u2500\u2500 logging.jsonnet \u251c\u2500\u2500 main.jsonnet \u251c\u2500\u2500 monitoring.jsonnet \u251c\u2500\u2500 org_iam.jsonnet \u251c\u2500\u2500 output.jsonnet \u251c\u2500\u2500 provider.jsonnet \u251c\u2500\u2500 pubsub.jsonnet \u251c\u2500\u2500 README.md.j2 \u2514\u2500\u2500 storage.jsonnet The main thing to understand about terraform components is that they are strictly handled by Jsonnet for simplicity. The rendering logic is as follows: { \"output.tf\": output, \"provider.tf\": provider, [if name_in_resoures(\"cloudbuild\") then \"cloudbuild.tf\"]: cloudbuild, [if name_in_resoures(\"container\") then \"kubernetes.tf\"]: kubernetes, [if name_in_resoures(\"dns\") then \"dns.tf\"]: dns, [if name_in_resoures(\"iam\") && \"serviceaccounts\" in p.resources.iam then \"iam_service_account.tf\"]: iam_service_account, ... [if name_in_resoures(\"pubsub\") then \"pubsub.tf\"]: pubsub, [if name_in_resoures(\"storage\") then \"storage.tf\"]: storage, } Each Jsonnet file defines a resource and then it is imported. Jsonnet then filters through all the inventory parameters to find specific keys that have been defined. Let's take for example the cloud build resource: local cloudbuild = import \"cloudbuild.jsonnet\"; ... { \"output.tf\": output, \"provider.tf\": provider, [if name_in_resoures(\"cloudbuild\") then \"cloudbuild.tf\"]: cloudbuild, ... } Assuming that one of the configuration files for a specific environment has the parameter key cloudbuild set. These parameters will then be interpreted by the cloudbuild.jsonnet template. A file named cloudbuild.tf.json will then be compiled using the parameters associated with the cloudbuild parameter key. It is important to understand that once you have a deeper understanding of Kapitan's capabilities, you can organize these files to a style and logic suitable for your organization. templates, docs, scripts Jinja2 is used to generate documentation that sources information from kapitan's inventory. This enables the ability to have dynamic documentation based on your infrastructure configuration changes. In templates/terraform you will find README.md.j2 . This is used to generate a README.md template to be utilized by terraform's output module. The following is what generates the documentation: data: { template_file: { readme: { template: kap.jinja2_template(\"templates/terraform/README.md.j2\", inv), }, }, }, output: { \"README.md\": { value: \"${data.template_file.readme.rendered}\", sensitive: true, }, }, The function kap.jinja2_template() (imported from kapitan.libjsonnet ) is used to convert and interpreter the README.md.j2 file into a raw string using the inventory for evaluation logic. Based on the various parameters jinja2 decides which sections of the readme should be included. When terraform runs it will use the output module to generate your desired README.md using information from terraform's state. Scripts are located in the scripts directory. They are compiled using jinja2 as the templating language. An example is as follows: export TF_DATA_DIR = $( realpath -m ${ DIR } /../../../.TF_DATA_DIR/ {{ inventory.parameters.name }} ) # Folder for TF initialization (preferable outside of compiled) export OUTPUT_DIR = $( realpath -m ${ DIR } /../../../output/ {{ inventory.parameters.name }} ) # Folder for storing output files (preferable outside of compiled) It is good practice to utilize this method to improve integration with various CLI based tools. Scripts help to ensure terraform and kapitan can function with your CI/CD systems. It generally depends on your organizational workflows. secrets Although there are no particular secrets in this instance. It is possible to utilize Kapitan secrets as defined in secrets management . Collaboration In some situations you may find teams that are used to writing terraform in HCL. In such situations it may be difficult to adopt Kapitan into the companies workflows. We can however use terraform modules to simplify the integration process. This means teams which are used to writing in HCL will not need to completely adopt Jsonnet. Modules can be imported into projects by defining them under the modules parameter key as shown in inventory/targets/sandbox . This means teams will only have to worry about coordinating parameter inputs for different projects. Jsonnet provides the ability to specify conventions and validation of input parameters. This provides peace of mind to infrastructure administrators around the tools usage.","title":"Terraform"},{"location":"example-terraform/#terraform-example","text":"We will be looking at how to use Kapitan to compile terraform files with Jsonnet as the input type. It's possible to use other input types, however, Jsonnet is recommended. For example, we could use the Kadet input to generate terraform files but this would require templates to be written in YAML then rendered into JSON. It is possible to allow Kadet to consume JSON as an input. This enables you to integrate your organizations pre-existing terraform JSON file's as templates. Jsonnet is the most straightforward input type as you will see due to its functional nature. The only appropriate output type is JSON since this is the format that Terraform consumes.","title":"Terraform example"},{"location":"example-terraform/#directory-structure","text":"There are several examples available in examples/terraform . This will be our working directory for this documentation. The directory structure is as follows: \u251c\u2500\u2500 inventory \u2514\u2500\u2500 templates It is possible to further extend this locally to include a lib directory where a terraform.libjsonnet file can be stored for use. This is generally dependent on the project scope and organizational patterns. We will describe in more detail the role of each of these folders in the following sections.","title":"Directory structure"},{"location":"example-terraform/#inventory","text":"This folder contains the inventory files used to render the templates for each target. The structure of this folder is as follows: . \u251c\u2500\u2500 classes \u2502 \u251c\u2500\u2500 env \u2502 \u2502 \u251c\u2500\u2500 develop.yml \u2502 \u2502 \u251c\u2500\u2500 prod.yml \u2502 \u2502 \u2514\u2500\u2500 sandbox.yml \u2502 \u251c\u2500\u2500 provider \u2502 \u2502 \u2514\u2500\u2500 gcp.yml \u2502 \u2514\u2500\u2500 type \u2502 \u2514\u2500\u2500 terraform.yml \u251c\u2500\u2500 reclass-config.yml \u2514\u2500\u2500 targets \u251c\u2500\u2500 develop \u2502 \u2514\u2500\u2500 project1.yml \u251c\u2500\u2500 prod \u2502 \u2514\u2500\u2500 project2.yml \u2514\u2500\u2500 sandbox \u2514\u2500\u2500 project3.yml The targets directory enables us to define various projects. We can specify each project as an environment such as dev , staging and production with each having unique parameters. The following is an example targets file. type.terraform is what defines the entry point into the main Jsonnet template file. The parameters in the file inventory/targets/develop/project1.yml will then be utilized to set the environmental specific provider/resource configuration. We define the default region and zone for terraform's provider configuration. The default DNS TTL for the DNS resource is also configured for the development environment. classes : - type.terraform parameters : name : project1 region : europe-west2 zone : europe-west2-a dns_default_ttl : 300 In the following example, we use a reclass configuration file to specify further parameters that we would like to merge into our project files. Thus we define nodes, which are stored in targets and environmental mandatory parameters stored in classes/env/ . The reclass config is shown below: storage_type : yaml_fs pretty_print : true output : yaml inventory_base_uri : . nodes_uri : targets classes_uri : classes compose_node_name : false class_mappings : - develop/* env.develop - prod/* env.prod - sandbox/* env.sandbox The following class provider.gcp will be found in all files in this path since it is a common configuration for the cloud authentication module. classes : - provider.gcp Further classes that group parameters together can be included. To assist in further refining the configuration.","title":"inventory"},{"location":"example-terraform/#components","text":"We tend to use components as a method to organize Jsonnet files. This is not mandatory since it is possible to configure Kapitan to look for input files wherever you would like. You can have these in any path just ensure you define that path in inventory/classes/type/terraform.yml . The templates folder is where the Jsonnet is located in this instance as shown below: . \u251c\u2500\u2500 cloudbuild.jsonnet \u251c\u2500\u2500 dns.jsonnet \u251c\u2500\u2500 iam.jsonnet \u251c\u2500\u2500 iam_service_account.jsonnet \u251c\u2500\u2500 kms.jsonnet \u251c\u2500\u2500 kubernetes.jsonnet \u251c\u2500\u2500 logging.jsonnet \u251c\u2500\u2500 main.jsonnet \u251c\u2500\u2500 monitoring.jsonnet \u251c\u2500\u2500 org_iam.jsonnet \u251c\u2500\u2500 output.jsonnet \u251c\u2500\u2500 provider.jsonnet \u251c\u2500\u2500 pubsub.jsonnet \u251c\u2500\u2500 README.md.j2 \u2514\u2500\u2500 storage.jsonnet The main thing to understand about terraform components is that they are strictly handled by Jsonnet for simplicity. The rendering logic is as follows: { \"output.tf\": output, \"provider.tf\": provider, [if name_in_resoures(\"cloudbuild\") then \"cloudbuild.tf\"]: cloudbuild, [if name_in_resoures(\"container\") then \"kubernetes.tf\"]: kubernetes, [if name_in_resoures(\"dns\") then \"dns.tf\"]: dns, [if name_in_resoures(\"iam\") && \"serviceaccounts\" in p.resources.iam then \"iam_service_account.tf\"]: iam_service_account, ... [if name_in_resoures(\"pubsub\") then \"pubsub.tf\"]: pubsub, [if name_in_resoures(\"storage\") then \"storage.tf\"]: storage, } Each Jsonnet file defines a resource and then it is imported. Jsonnet then filters through all the inventory parameters to find specific keys that have been defined. Let's take for example the cloud build resource: local cloudbuild = import \"cloudbuild.jsonnet\"; ... { \"output.tf\": output, \"provider.tf\": provider, [if name_in_resoures(\"cloudbuild\") then \"cloudbuild.tf\"]: cloudbuild, ... } Assuming that one of the configuration files for a specific environment has the parameter key cloudbuild set. These parameters will then be interpreted by the cloudbuild.jsonnet template. A file named cloudbuild.tf.json will then be compiled using the parameters associated with the cloudbuild parameter key. It is important to understand that once you have a deeper understanding of Kapitan's capabilities, you can organize these files to a style and logic suitable for your organization.","title":"components"},{"location":"example-terraform/#templates-docs-scripts","text":"Jinja2 is used to generate documentation that sources information from kapitan's inventory. This enables the ability to have dynamic documentation based on your infrastructure configuration changes. In templates/terraform you will find README.md.j2 . This is used to generate a README.md template to be utilized by terraform's output module. The following is what generates the documentation: data: { template_file: { readme: { template: kap.jinja2_template(\"templates/terraform/README.md.j2\", inv), }, }, }, output: { \"README.md\": { value: \"${data.template_file.readme.rendered}\", sensitive: true, }, }, The function kap.jinja2_template() (imported from kapitan.libjsonnet ) is used to convert and interpreter the README.md.j2 file into a raw string using the inventory for evaluation logic. Based on the various parameters jinja2 decides which sections of the readme should be included. When terraform runs it will use the output module to generate your desired README.md using information from terraform's state. Scripts are located in the scripts directory. They are compiled using jinja2 as the templating language. An example is as follows: export TF_DATA_DIR = $( realpath -m ${ DIR } /../../../.TF_DATA_DIR/ {{ inventory.parameters.name }} ) # Folder for TF initialization (preferable outside of compiled) export OUTPUT_DIR = $( realpath -m ${ DIR } /../../../output/ {{ inventory.parameters.name }} ) # Folder for storing output files (preferable outside of compiled) It is good practice to utilize this method to improve integration with various CLI based tools. Scripts help to ensure terraform and kapitan can function with your CI/CD systems. It generally depends on your organizational workflows.","title":"templates, docs, scripts"},{"location":"example-terraform/#secrets","text":"Although there are no particular secrets in this instance. It is possible to utilize Kapitan secrets as defined in secrets management .","title":"secrets"},{"location":"example-terraform/#collaboration","text":"In some situations you may find teams that are used to writing terraform in HCL. In such situations it may be difficult to adopt Kapitan into the companies workflows. We can however use terraform modules to simplify the integration process. This means teams which are used to writing in HCL will not need to completely adopt Jsonnet. Modules can be imported into projects by defining them under the modules parameter key as shown in inventory/targets/sandbox . This means teams will only have to worry about coordinating parameter inputs for different projects. Jsonnet provides the ability to specify conventions and validation of input parameters. This provides peace of mind to infrastructure administrators around the tools usage.","title":"Collaboration"},{"location":"external_dependencies/","text":"Fetching external dependencies Kapitan is capable of fetching components stored in remote locations. This feature can be used by specifying those dependencies in the inventory under parameters.kapitan.dependencies . Supported types are: git type http type helm type Some use cases of this feature may include: using templates/jsonnet libraries hosted remotely using values in remote files via file_read jsonnet callback Usage parameters : kapitan : dependencies : - type : <dependency_type> output_path : path/to/file/or/dir source : <source_of_dependency> # other type-specific parameters, if any Use --fetch option to fetch the dependencies: $ kapitan compile --fetch This will download the dependencies and store them at their respective output_path . Git type Git types can fetch external dependencies available via HTTP/HTTPS or SSH URLs. This is useful for fetching repositories or their sub-directories, as well as accessing them in specific commits and branches (refs). Note : git types require git binary on your system. Usage parameters : kapitan : dependencies : - type : git output_path : path/to/dir source : git_url subdir : relative/path/from/repo/root (optional) ref : tag, commit, branch etc. (optional) Example Say we want to fetch the source code from our kapitan repository, specifically, deepmind/kapitan/kapitan/version.py . Let's create a very simple target file inventory/targets/kapitan-example.yml . parameters : kapitan : vars : target : kapitan-example dependencies : - type : git output_path : source/kapitan source : git@github.com:deepmind/kapitan.git subdir : kapitan ref : master compile : - input_paths : - source/kapitan/version.py input_type : jinja2 # just to copy the file over to target output_path : . Then run: $ kapitan compile --fetch -t kapitan-example Dependency git@github.com:deepmind/kapitan.git : fetching now Dependency git@github.com:deepmind/kapitan.git : successfully fetched Dependency git@github.com:deepmind/kapitan.git : saved to source/kapitan Compiled kapitan-example ( 0 .02s ) $ ls source kapitan This will download the kapitan repository (deepmind/kapitan), copy the sub-directory kapitan and save it to source/kapitan . Therefore, deepmind/kapitan/kapitan corresponds to source/kapitan locally. Note that even if you are not using subdir parameter, you can and should specify the repository name in the output_path parameter. If you only specify source as the output_path , then all the kapitan files will be under source and not source/kapitan . HTTP type http[s] types can fetch external dependencies available at http:// or https:// URL. Usage parameters : kapitan : dependencies : - type : http | https output_path : path/to/file source : http[s]://<url> unpack : True | False output_path must fully specify the file name. For example: parameters : kapitan : dependencies : - type : https output_path : foo.txt source : https://example.com/foo.txt Example Say we want to download kapitan README.md file. Since it's on Github, we can access it as https://raw.githubusercontent.com/deepmind/kapitan/master/README.md . Using the following inventory, we can copy this to our target folder: parameters : kapitan : vars : target : kapitan-example dependencies : - type : https output_path : README.md source : https://raw.githubusercontent.com/deepmind/kapitan/master/README.md compile : - input_paths : - README.md input_type : jinja2 output_path : . Then run: $ kapitan compile --fetch -t kapitan-example Dependency https://raw.githubusercontent.com/deepmind/kapitan/master/README.md : fetching now Dependency https://raw.githubusercontent.com/deepmind/kapitan/master/README.md : successfully fetched Dependency https://raw.githubusercontent.com/deepmind/kapitan/master/README.md : saved to README.md Compiled kapitan-example ( 0 .02s ) $ ls compiled inventory README.md This fetches the README.md file from the URL and save it locally. Another use case for http types is when we want to download an archive file, such as helm packages, and extract its content. Setting unpack: True will unpack zip or tar files onto the output_path . In such cases, set output_path to a folder where you extract the content, and not the file name. You can refer to here for the example. Helm type Fetches helm charts and any specific subcharts in the requirements.yaml file. Currently only works on linux with the helm_fetch_binding . Usage parameters : kapitan : dependencies : - type : helm output_path : path/to/chart source : http[s]://<helm_chart_repository_url> version : <specific chart version> chart_name : <name of chart> Example If we want to download the prometheus helm chart we simply add the dependency to the monitoring target. We want a specific version 11.3.0 so we put that in. parameters : kapitan : vars : target : monitoring dependencies : - type : helm output_path : charts/prometheus source : https://kubernetes-charts.storage.googleapis.com version : 11.3.0 chart_name : prometheus compile : - input_type : helm output_path : . input_paths : - charts/prometheus helm_values : alertmanager : enabled : false helm_params : namespace : monitoring name_template : prometheus release_name : prometheus Then run: $ kapitan compile --fetch -t monitoring Dependency helm chart prometheus and version 11 .3.0: fetching now Dependency helm chart prometheus and version 11 .3.0: successfully fetched Dependency helm chart prometheus and version 11 .3.0: saved to charts/prometheus Compiled monitoring ( 1 .48s ) $ tree -L 3 \u251c\u2500\u2500 charts \u2502 \u2514\u2500\u2500 prometheus \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 requirements.lock \u2502 \u251c\u2500\u2500 requirements.yaml \u2502 \u251c\u2500\u2500 templates \u2502 \u2514\u2500\u2500 values.yaml \u251c\u2500\u2500 compiled \u2502 \u251c\u2500\u2500 monitoring \u251c\u2500\u2500 inventory \u251c\u2500\u2500 classes \u251c\u2500\u2500 common.yml \u251c\u2500\u2500 component If you simply want the latest chart available, either don't include the version key or specify an empty string. parameters : kapitan : vars : target : monitoring dependencies : - type : helm output_path : charts/prometheus source : https://kubernetes-charts.storage.googleapis.com version : \"\" chart_name : prometheus compile : - input_type : helm output_path : . input_paths : - charts/prometheus helm_values : alertmanager : enabled : false helm_params : namespace : monitoring name_template : prometheus release_name : prometheus Then run: $ kapitan compile --fetch -t monitoring Dependency helm chart prometheus being fetch with using latest version available Dependency helm chart prometheus and version : fetching now Dependency helm chart prometheus and version : successfully fetched Dependency helm chart prometheus and version : saved to charts/prometheus Compiled monitoring ( 1 .58s ) $ tree -L 3 \u251c\u2500\u2500 charts \u2502 \u2514\u2500\u2500 prometheus \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 requirements.lock \u2502 \u251c\u2500\u2500 requirements.yaml \u2502 \u251c\u2500\u2500 templates \u2502 \u2514\u2500\u2500 values.yaml \u251c\u2500\u2500 compiled \u2502 \u251c\u2500\u2500 monitoring \u251c\u2500\u2500 inventory \u251c\u2500\u2500 classes \u251c\u2500\u2500 common.yml \u251c\u2500\u2500 component","title":"External dependency management"},{"location":"external_dependencies/#fetching-external-dependencies","text":"Kapitan is capable of fetching components stored in remote locations. This feature can be used by specifying those dependencies in the inventory under parameters.kapitan.dependencies . Supported types are: git type http type helm type Some use cases of this feature may include: using templates/jsonnet libraries hosted remotely using values in remote files via file_read jsonnet callback","title":"Fetching external dependencies"},{"location":"external_dependencies/#usage","text":"parameters : kapitan : dependencies : - type : <dependency_type> output_path : path/to/file/or/dir source : <source_of_dependency> # other type-specific parameters, if any Use --fetch option to fetch the dependencies: $ kapitan compile --fetch This will download the dependencies and store them at their respective output_path .","title":"Usage"},{"location":"external_dependencies/#git-type","text":"Git types can fetch external dependencies available via HTTP/HTTPS or SSH URLs. This is useful for fetching repositories or their sub-directories, as well as accessing them in specific commits and branches (refs). Note : git types require git binary on your system.","title":"Git type"},{"location":"external_dependencies/#usage_1","text":"parameters : kapitan : dependencies : - type : git output_path : path/to/dir source : git_url subdir : relative/path/from/repo/root (optional) ref : tag, commit, branch etc. (optional)","title":"Usage"},{"location":"external_dependencies/#example","text":"Say we want to fetch the source code from our kapitan repository, specifically, deepmind/kapitan/kapitan/version.py . Let's create a very simple target file inventory/targets/kapitan-example.yml . parameters : kapitan : vars : target : kapitan-example dependencies : - type : git output_path : source/kapitan source : git@github.com:deepmind/kapitan.git subdir : kapitan ref : master compile : - input_paths : - source/kapitan/version.py input_type : jinja2 # just to copy the file over to target output_path : . Then run: $ kapitan compile --fetch -t kapitan-example Dependency git@github.com:deepmind/kapitan.git : fetching now Dependency git@github.com:deepmind/kapitan.git : successfully fetched Dependency git@github.com:deepmind/kapitan.git : saved to source/kapitan Compiled kapitan-example ( 0 .02s ) $ ls source kapitan This will download the kapitan repository (deepmind/kapitan), copy the sub-directory kapitan and save it to source/kapitan . Therefore, deepmind/kapitan/kapitan corresponds to source/kapitan locally. Note that even if you are not using subdir parameter, you can and should specify the repository name in the output_path parameter. If you only specify source as the output_path , then all the kapitan files will be under source and not source/kapitan .","title":"Example"},{"location":"external_dependencies/#http-type","text":"http[s] types can fetch external dependencies available at http:// or https:// URL.","title":"HTTP type"},{"location":"external_dependencies/#usage_2","text":"parameters : kapitan : dependencies : - type : http | https output_path : path/to/file source : http[s]://<url> unpack : True | False output_path must fully specify the file name. For example: parameters : kapitan : dependencies : - type : https output_path : foo.txt source : https://example.com/foo.txt","title":"Usage"},{"location":"external_dependencies/#example_1","text":"Say we want to download kapitan README.md file. Since it's on Github, we can access it as https://raw.githubusercontent.com/deepmind/kapitan/master/README.md . Using the following inventory, we can copy this to our target folder: parameters : kapitan : vars : target : kapitan-example dependencies : - type : https output_path : README.md source : https://raw.githubusercontent.com/deepmind/kapitan/master/README.md compile : - input_paths : - README.md input_type : jinja2 output_path : . Then run: $ kapitan compile --fetch -t kapitan-example Dependency https://raw.githubusercontent.com/deepmind/kapitan/master/README.md : fetching now Dependency https://raw.githubusercontent.com/deepmind/kapitan/master/README.md : successfully fetched Dependency https://raw.githubusercontent.com/deepmind/kapitan/master/README.md : saved to README.md Compiled kapitan-example ( 0 .02s ) $ ls compiled inventory README.md This fetches the README.md file from the URL and save it locally. Another use case for http types is when we want to download an archive file, such as helm packages, and extract its content. Setting unpack: True will unpack zip or tar files onto the output_path . In such cases, set output_path to a folder where you extract the content, and not the file name. You can refer to here for the example.","title":"Example"},{"location":"external_dependencies/#helm-type","text":"Fetches helm charts and any specific subcharts in the requirements.yaml file. Currently only works on linux with the helm_fetch_binding .","title":"Helm type"},{"location":"external_dependencies/#usage_3","text":"parameters : kapitan : dependencies : - type : helm output_path : path/to/chart source : http[s]://<helm_chart_repository_url> version : <specific chart version> chart_name : <name of chart>","title":"Usage"},{"location":"external_dependencies/#example_2","text":"If we want to download the prometheus helm chart we simply add the dependency to the monitoring target. We want a specific version 11.3.0 so we put that in. parameters : kapitan : vars : target : monitoring dependencies : - type : helm output_path : charts/prometheus source : https://kubernetes-charts.storage.googleapis.com version : 11.3.0 chart_name : prometheus compile : - input_type : helm output_path : . input_paths : - charts/prometheus helm_values : alertmanager : enabled : false helm_params : namespace : monitoring name_template : prometheus release_name : prometheus Then run: $ kapitan compile --fetch -t monitoring Dependency helm chart prometheus and version 11 .3.0: fetching now Dependency helm chart prometheus and version 11 .3.0: successfully fetched Dependency helm chart prometheus and version 11 .3.0: saved to charts/prometheus Compiled monitoring ( 1 .48s ) $ tree -L 3 \u251c\u2500\u2500 charts \u2502 \u2514\u2500\u2500 prometheus \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 requirements.lock \u2502 \u251c\u2500\u2500 requirements.yaml \u2502 \u251c\u2500\u2500 templates \u2502 \u2514\u2500\u2500 values.yaml \u251c\u2500\u2500 compiled \u2502 \u251c\u2500\u2500 monitoring \u251c\u2500\u2500 inventory \u251c\u2500\u2500 classes \u251c\u2500\u2500 common.yml \u251c\u2500\u2500 component If you simply want the latest chart available, either don't include the version key or specify an empty string. parameters : kapitan : vars : target : monitoring dependencies : - type : helm output_path : charts/prometheus source : https://kubernetes-charts.storage.googleapis.com version : \"\" chart_name : prometheus compile : - input_type : helm output_path : . input_paths : - charts/prometheus helm_values : alertmanager : enabled : false helm_params : namespace : monitoring name_template : prometheus release_name : prometheus Then run: $ kapitan compile --fetch -t monitoring Dependency helm chart prometheus being fetch with using latest version available Dependency helm chart prometheus and version : fetching now Dependency helm chart prometheus and version : successfully fetched Dependency helm chart prometheus and version : saved to charts/prometheus Compiled monitoring ( 1 .58s ) $ tree -L 3 \u251c\u2500\u2500 charts \u2502 \u2514\u2500\u2500 prometheus \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 requirements.lock \u2502 \u251c\u2500\u2500 requirements.yaml \u2502 \u251c\u2500\u2500 templates \u2502 \u2514\u2500\u2500 values.yaml \u251c\u2500\u2500 compiled \u2502 \u251c\u2500\u2500 monitoring \u251c\u2500\u2500 inventory \u251c\u2500\u2500 classes \u251c\u2500\u2500 common.yml \u251c\u2500\u2500 component","title":"Example"},{"location":"inventory/","text":"Inventory Overview Inventory is a hierarchical database of variables that are passed to the targets during compilation. By default, Kapitan will look for an inventory/ directory to render the inventory from. There are 2 types of objects inside the inventory; inventory classes and inventory targets . Inventory Classes Classes define variables that are shared across many targets. You can have, for example, a component.elasticsearch class with all the default values for targets using elasticsearch. Or a production or dev class to enable / disable certain features based on the type of target. You can always override values further up the tree (i.e. in the inventory target file or in a class that inherits another class) Classifying almost anything will help you avoid repetition (DRY) and will force you to organise parameters hierarchically. Example: elasticsearch For example, the snippet below, taken from the example elasticsearch class, declares what parameters are needed for the elasticsearch component: $ cat inventory/classes/component/elasticsearch.yml parameters: elasticsearch: image: \"quay.io/pires/docker-elasticsearch-kubernetes:5.5.0\" java_opts: \"-Xms512m -Xmx512m\" replicas: 1 masters: 1 roles: master: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} data: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} client: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} ingest: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} As shown above, within the inventory, you can refer to the values with the syntax ${obj_name:key_name} (no need to specify the parameters key). Example: mysql Or in the mysql class example, we declare the generic variables that will be shared by all targets that import the component and what to compile. We include a secret that is referencing a GPG encrypted value in secrets/targets/minikube-mysql/mysql/password , or if the file doesn't exist, it will dynamically generate a random b64-encoded password, encrypt it and save it into the file. $ cat inventory/classes/component/mysql.yml parameters: mysql: storage: 10G storage_class: standard image: mysql:latest users: root: # If 'refs/targets/${target_name}/mysql/password' doesn't exist, it will gen a random b64-encoded password password: ?{gpg:targets/${target_name}/mysql/password||randomstr|base64} # password: ?{gkms:targets/${target_name}/mysql/password||randomstr|base64} # password: ?{awskms:targets/${target_name}/mysql/password||randomstr|base64} # Generates the sha256 checksum of the previously declared B64'ed password # It's base64'ed again so that it can be used in kubernetes secrets password_sha256: ?{gpg:targets/${target_name}/mysql/password_sha256||reveal:targets/${target_name}/mysql/password|sha256|base64} kapitan: compile: - output_path: manifests input_type: jsonnet input_paths: - components/mysql/main.jsonnet output_type: yaml - output_path: scripts input_type: jinja2 input_paths: - scripts - output_path: . output_type: yaml input_type: jinja2 input_paths: - docs/mysql/README.md Inventory Targets A target usually represents a single namespace in a kubernetes cluster and defines all components, scripts and documentation that will be generated for that target. Kapitan will recognise files in inventory/targets as targets. When you run kapitan compile , kapitan will generate compiled directory whose sub-directories are named after the targets, each of which contains all the compiled output defined under parameters.kapitan.compile for a target. Inside the inventory target files you can include classes and define new values or override any values inherited from the included classes. For example: $ cat inventory/targets/minikube-es.yml classes: - common - cluster.minikube - component.elasticsearch parameters: target_name: minikube-es elasticsearch: replicas: 2 Targets can also be defined inside the inventory . Note : Each target must contain the property parameters.kapitan.vars.target whose value equals to the name of the target file. For example, for the target inventory/targets/minikube-es.yml , the rendered inventory must contain: parameters : kapitan : vars : target : minikube-es kapitan-specific inventory values: parameters.kapitan Values under parameters.kapitan , such as parameters.kapitan.vars as mentioned above, are special values that kapitan parses and processes. These include: kapitan.compile items which indicate which files to compile kapitan.secrets which contains secret encryption/decryption information kapitan.validate items which indicate which compiled output to validate kapitan.vars which are also passed down to jsonnet and jinja2 templates as contexts Useful commands kapitan inventory Renders the resulting inventory values for a specific target. For example, rendering the inventory for the minikube-es target: $ kapitan inventory -t minikube-es ... classes: - component.namespace - cluster.common - common - cluster.minikube - component.elasticsearch environment: base exports: {} parameters: _reclass_: environment: base name: full: minikube-es short: minikube-es cluster: id: minikube name: minikube type: minikube user: minikube elasticsearch: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 roles: client: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 data: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 ingest: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 master: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 kapitan: compile: - input_paths: - components/namespace/main.jsonnet input_type: jsonnet output_path: pre-deploy output_type: yaml - input_paths: - components/elasticsearch/main.jsonnet input_type: jsonnet output_path: manifests output_type: yaml - input_paths: - scripts input_type: jinja2 output_path: scripts - input_paths: - docs/elasticsearch/README.md input_type: jinja2 output_path: . secrets: gpg: recipients: - fingerprint: D9234C61F58BEB3ED8552A57E28DC07A3CBFAE7C name: example@kapitan.dev vars: namespace: minikube-es target: minikube-es kubectl: insecure_skip_tls_verify: false minikube: cpus: 4 memory: 4096 version: v0.25.0 mysql: hostname: localhost namespace: minikube-es target_name: minikube-es vault: address: https://localhost:8200 Use kapitan lint to checkup on your inventory or refs. kapitan searchvar Shows all inventory files where a variable is declared: $ kapitan searchvar parameters.elasticsearch.replicas ./inventory/targets/minikube-es.yml 2 ./inventory/classes/component/elasticsearch.yml 1","title":"Inventory"},{"location":"inventory/#inventory","text":"","title":"Inventory"},{"location":"inventory/#overview","text":"Inventory is a hierarchical database of variables that are passed to the targets during compilation. By default, Kapitan will look for an inventory/ directory to render the inventory from. There are 2 types of objects inside the inventory; inventory classes and inventory targets .","title":"Overview"},{"location":"inventory/#inventory-classes","text":"Classes define variables that are shared across many targets. You can have, for example, a component.elasticsearch class with all the default values for targets using elasticsearch. Or a production or dev class to enable / disable certain features based on the type of target. You can always override values further up the tree (i.e. in the inventory target file or in a class that inherits another class) Classifying almost anything will help you avoid repetition (DRY) and will force you to organise parameters hierarchically.","title":"Inventory Classes"},{"location":"inventory/#example-elasticsearch","text":"For example, the snippet below, taken from the example elasticsearch class, declares what parameters are needed for the elasticsearch component: $ cat inventory/classes/component/elasticsearch.yml parameters: elasticsearch: image: \"quay.io/pires/docker-elasticsearch-kubernetes:5.5.0\" java_opts: \"-Xms512m -Xmx512m\" replicas: 1 masters: 1 roles: master: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} data: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} client: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} ingest: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} As shown above, within the inventory, you can refer to the values with the syntax ${obj_name:key_name} (no need to specify the parameters key).","title":"Example: elasticsearch"},{"location":"inventory/#example-mysql","text":"Or in the mysql class example, we declare the generic variables that will be shared by all targets that import the component and what to compile. We include a secret that is referencing a GPG encrypted value in secrets/targets/minikube-mysql/mysql/password , or if the file doesn't exist, it will dynamically generate a random b64-encoded password, encrypt it and save it into the file. $ cat inventory/classes/component/mysql.yml parameters: mysql: storage: 10G storage_class: standard image: mysql:latest users: root: # If 'refs/targets/${target_name}/mysql/password' doesn't exist, it will gen a random b64-encoded password password: ?{gpg:targets/${target_name}/mysql/password||randomstr|base64} # password: ?{gkms:targets/${target_name}/mysql/password||randomstr|base64} # password: ?{awskms:targets/${target_name}/mysql/password||randomstr|base64} # Generates the sha256 checksum of the previously declared B64'ed password # It's base64'ed again so that it can be used in kubernetes secrets password_sha256: ?{gpg:targets/${target_name}/mysql/password_sha256||reveal:targets/${target_name}/mysql/password|sha256|base64} kapitan: compile: - output_path: manifests input_type: jsonnet input_paths: - components/mysql/main.jsonnet output_type: yaml - output_path: scripts input_type: jinja2 input_paths: - scripts - output_path: . output_type: yaml input_type: jinja2 input_paths: - docs/mysql/README.md","title":"Example: mysql"},{"location":"inventory/#inventory-targets","text":"A target usually represents a single namespace in a kubernetes cluster and defines all components, scripts and documentation that will be generated for that target. Kapitan will recognise files in inventory/targets as targets. When you run kapitan compile , kapitan will generate compiled directory whose sub-directories are named after the targets, each of which contains all the compiled output defined under parameters.kapitan.compile for a target. Inside the inventory target files you can include classes and define new values or override any values inherited from the included classes. For example: $ cat inventory/targets/minikube-es.yml classes: - common - cluster.minikube - component.elasticsearch parameters: target_name: minikube-es elasticsearch: replicas: 2 Targets can also be defined inside the inventory . Note : Each target must contain the property parameters.kapitan.vars.target whose value equals to the name of the target file. For example, for the target inventory/targets/minikube-es.yml , the rendered inventory must contain: parameters : kapitan : vars : target : minikube-es","title":"Inventory Targets"},{"location":"inventory/#kapitan-specific-inventory-values-parameterskapitan","text":"Values under parameters.kapitan , such as parameters.kapitan.vars as mentioned above, are special values that kapitan parses and processes. These include: kapitan.compile items which indicate which files to compile kapitan.secrets which contains secret encryption/decryption information kapitan.validate items which indicate which compiled output to validate kapitan.vars which are also passed down to jsonnet and jinja2 templates as contexts","title":"kapitan-specific inventory values: parameters.kapitan"},{"location":"inventory/#useful-commands","text":"","title":"Useful commands"},{"location":"inventory/#kapitan-inventory","text":"Renders the resulting inventory values for a specific target. For example, rendering the inventory for the minikube-es target: $ kapitan inventory -t minikube-es ... classes: - component.namespace - cluster.common - common - cluster.minikube - component.elasticsearch environment: base exports: {} parameters: _reclass_: environment: base name: full: minikube-es short: minikube-es cluster: id: minikube name: minikube type: minikube user: minikube elasticsearch: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 roles: client: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 data: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 ingest: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 master: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 kapitan: compile: - input_paths: - components/namespace/main.jsonnet input_type: jsonnet output_path: pre-deploy output_type: yaml - input_paths: - components/elasticsearch/main.jsonnet input_type: jsonnet output_path: manifests output_type: yaml - input_paths: - scripts input_type: jinja2 output_path: scripts - input_paths: - docs/elasticsearch/README.md input_type: jinja2 output_path: . secrets: gpg: recipients: - fingerprint: D9234C61F58BEB3ED8552A57E28DC07A3CBFAE7C name: example@kapitan.dev vars: namespace: minikube-es target: minikube-es kubectl: insecure_skip_tls_verify: false minikube: cpus: 4 memory: 4096 version: v0.25.0 mysql: hostname: localhost namespace: minikube-es target_name: minikube-es vault: address: https://localhost:8200 Use kapitan lint to checkup on your inventory or refs.","title":"kapitan inventory"},{"location":"inventory/#kapitan-searchvar","text":"Shows all inventory files where a variable is declared: $ kapitan searchvar parameters.elasticsearch.replicas ./inventory/targets/minikube-es.yml 2 ./inventory/classes/component/elasticsearch.yml 1","title":"kapitan searchvar"},{"location":"kapitan_overview/","text":"Main concepts Inventory Inventory is a hierarchical database of variables, defined in yaml files, that are passed to the targets during compilation. This will be explained in detail in the inventory section of the documentation. Components (templates) Components will receive the inventory values for each individual target and gets rendered and saved into the compiled directory. The types of available templates and how to use them is discussed in the compile operation section of the documentation. Typical Folder Structure kapitan init To start off a kapitan project, you can run kapitan init --directory <directory> to populate a new directory with the recommended kapitan folder structure. The bare minimum structure that makes use of kapitan features may look as follows: . \u251c\u2500\u2500 components \u2502 \u251c\u2500\u2500 mycomponent.jsonnet \u251c\u2500\u2500 templates \u251c\u2500\u2500 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 inventory \u2502 \u251c\u2500\u2500 classes \u2502 \u2502 \u251c\u2500\u2500 common.yml \u2502 \u2514\u2500\u2500 targets \u2502 \u251c\u2500\u2500 dev.yml \u2502 \u251c\u2500\u2500 staging.yml \u2502 \u2514\u2500\u2500 prod.yml \u251c\u2500\u2500 refs \u2502 \u251c\u2500\u2500 targets \u2502 \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2502 \u2514\u2500\u2500 password \u2514\u2500\u2500\u2500\u251c\u2500\u2500 common \u2514\u2500\u2500 example-com-tls.key components : stores jsonnet files each of which corresponds to an application (for example) templates : stores Jinja2 and Kadet templates inventory/targets : stores target files inventory/classes : stores inventory values to be inherited by targets refs : stores secrets referenced inside the inventory Example: kubernetes deployment Refer to the structure below for more production-like uses of kapitan for kubernetes deployment: . \u251c\u2500\u2500 components \u2502 \u251c\u2500\u2500 elasticsearch \u2502 \u2502 \u251c\u2500\u2500 configmap.jsonnet \u2502 \u2502 \u251c\u2500\u2500 deployment.jsonnet \u2502 \u2502 \u251c\u2500\u2500 main.jsonnet \u2502 \u2502 \u2514\u2500\u2500 service.jsonnet \u2502 \u2514\u2500\u2500 nginx \u2502 \u251c\u2500\u2500 configmap.jsonnet \u2502 \u251c\u2500\u2500 deployment.jsonnet \u2502 \u251c\u2500\u2500 main.jsonnet \u2502 \u251c\u2500\u2500 nginx.conf.j2 \u2502 \u2514\u2500\u2500 service.jsonnet \u251c\u2500\u2500 inventory \u2502 \u251c\u2500\u2500 classes \u2502 \u2502 \u251c\u2500\u2500 cluster \u2502 \u2502 \u2502 \u251c\u2500\u2500 cluster1.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 cluster2.yml \u2502 \u2502 \u251c\u2500\u2500 component \u2502 \u2502 \u2502 \u251c\u2500\u2500 elasticsearch.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 nginx.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 zookeeper.yml \u2502 \u2502 \u2514\u2500\u2500 environment \u2502 \u2502 \u251c\u2500\u2500 dev.yml \u2502 \u2502 \u2514\u2500\u2500 prod.yml \u2502 \u2514\u2500\u2500 targets \u2502 \u251c\u2500\u2500 dev-cluster1-elasticsearch.yml \u2502 \u251c\u2500\u2500 prod-cluster1-elasticsearch.yml \u2502 \u2514\u2500\u2500 prod-cluster2-frontend.yml \u251c\u2500\u2500 refs \u2502 \u251c\u2500\u2500 targets \u2502 \u2502 \u251c\u2500\u2500 prod-cluster1-elasticsearch \u2502 \u2502 \u2502 \u2514\u2500\u2500 password \u2502 \u251c\u2500\u2500 common \u2502 \u2502 \u2514\u2500\u2500 example-com-tls.key \u251c\u2500\u2500 lib \u251c\u2500\u2500 kapitan.libjsonnet \u2514\u2500\u2500 kube.libjsonnet The use of each file in this folder will become clear in the the subsequent documentations.","title":"Kapitan Overview"},{"location":"kapitan_overview/#main-concepts","text":"","title":"Main concepts"},{"location":"kapitan_overview/#inventory","text":"Inventory is a hierarchical database of variables, defined in yaml files, that are passed to the targets during compilation. This will be explained in detail in the inventory section of the documentation.","title":"Inventory"},{"location":"kapitan_overview/#components-templates","text":"Components will receive the inventory values for each individual target and gets rendered and saved into the compiled directory. The types of available templates and how to use them is discussed in the compile operation section of the documentation.","title":"Components (templates)"},{"location":"kapitan_overview/#typical-folder-structure","text":"","title":"Typical Folder Structure"},{"location":"kapitan_overview/#kapitan-init","text":"To start off a kapitan project, you can run kapitan init --directory <directory> to populate a new directory with the recommended kapitan folder structure. The bare minimum structure that makes use of kapitan features may look as follows: . \u251c\u2500\u2500 components \u2502 \u251c\u2500\u2500 mycomponent.jsonnet \u251c\u2500\u2500 templates \u251c\u2500\u2500 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 inventory \u2502 \u251c\u2500\u2500 classes \u2502 \u2502 \u251c\u2500\u2500 common.yml \u2502 \u2514\u2500\u2500 targets \u2502 \u251c\u2500\u2500 dev.yml \u2502 \u251c\u2500\u2500 staging.yml \u2502 \u2514\u2500\u2500 prod.yml \u251c\u2500\u2500 refs \u2502 \u251c\u2500\u2500 targets \u2502 \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2502 \u2514\u2500\u2500 password \u2514\u2500\u2500\u2500\u251c\u2500\u2500 common \u2514\u2500\u2500 example-com-tls.key components : stores jsonnet files each of which corresponds to an application (for example) templates : stores Jinja2 and Kadet templates inventory/targets : stores target files inventory/classes : stores inventory values to be inherited by targets refs : stores secrets referenced inside the inventory","title":"kapitan init"},{"location":"kapitan_overview/#example-kubernetes-deployment","text":"Refer to the structure below for more production-like uses of kapitan for kubernetes deployment: . \u251c\u2500\u2500 components \u2502 \u251c\u2500\u2500 elasticsearch \u2502 \u2502 \u251c\u2500\u2500 configmap.jsonnet \u2502 \u2502 \u251c\u2500\u2500 deployment.jsonnet \u2502 \u2502 \u251c\u2500\u2500 main.jsonnet \u2502 \u2502 \u2514\u2500\u2500 service.jsonnet \u2502 \u2514\u2500\u2500 nginx \u2502 \u251c\u2500\u2500 configmap.jsonnet \u2502 \u251c\u2500\u2500 deployment.jsonnet \u2502 \u251c\u2500\u2500 main.jsonnet \u2502 \u251c\u2500\u2500 nginx.conf.j2 \u2502 \u2514\u2500\u2500 service.jsonnet \u251c\u2500\u2500 inventory \u2502 \u251c\u2500\u2500 classes \u2502 \u2502 \u251c\u2500\u2500 cluster \u2502 \u2502 \u2502 \u251c\u2500\u2500 cluster1.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 cluster2.yml \u2502 \u2502 \u251c\u2500\u2500 component \u2502 \u2502 \u2502 \u251c\u2500\u2500 elasticsearch.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 nginx.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 zookeeper.yml \u2502 \u2502 \u2514\u2500\u2500 environment \u2502 \u2502 \u251c\u2500\u2500 dev.yml \u2502 \u2502 \u2514\u2500\u2500 prod.yml \u2502 \u2514\u2500\u2500 targets \u2502 \u251c\u2500\u2500 dev-cluster1-elasticsearch.yml \u2502 \u251c\u2500\u2500 prod-cluster1-elasticsearch.yml \u2502 \u2514\u2500\u2500 prod-cluster2-frontend.yml \u251c\u2500\u2500 refs \u2502 \u251c\u2500\u2500 targets \u2502 \u2502 \u251c\u2500\u2500 prod-cluster1-elasticsearch \u2502 \u2502 \u2502 \u2514\u2500\u2500 password \u2502 \u251c\u2500\u2500 common \u2502 \u2502 \u2514\u2500\u2500 example-com-tls.key \u251c\u2500\u2500 lib \u251c\u2500\u2500 kapitan.libjsonnet \u2514\u2500\u2500 kube.libjsonnet The use of each file in this folder will become clear in the the subsequent documentations.","title":"Example: kubernetes deployment"},{"location":"proposals/","text":"Kapitan proposals Introduction Proposals can be submitted for review by performing a pull request against this repository. If approved the proposal will be published here for further review by the Kapitan community. Proposals tend to be improvements or design consideration for new features. Existing proposals Kadet input type External dependency management Helm charts input type Kubernetes scheme validation Portable standalone Kapitan executable Ref types redesign Hashicorp vault secrets","title":"Proposals"},{"location":"proposals/#kapitan-proposals","text":"","title":"Kapitan proposals"},{"location":"proposals/#introduction","text":"Proposals can be submitted for review by performing a pull request against this repository. If approved the proposal will be published here for further review by the Kapitan community. Proposals tend to be improvements or design consideration for new features.","title":"Introduction"},{"location":"proposals/#existing-proposals","text":"Kadet input type External dependency management Helm charts input type Kubernetes scheme validation Portable standalone Kapitan executable Ref types redesign Hashicorp vault secrets","title":"Existing proposals"},{"location":"pyenv-scl/","text":"Kapitan on Older Linux Systems Introduction Kapitan requires Python 3.6, and you need to be able to install the dependencies in the requirements file . However, sometimes this isn't entirely straightforward, and you may not be able or willing to install new versions of Python system-wide. We do provide a dockerfile which you can use to run Kapitan in a container, but if this isn't practical or possible either, you may wish to use one of the following options: PyEnv (Linux, distro-agnostic) Software Collections (RHEL-based distros) Both of these projects allow you to use a different version of Python specifically for your work with or on Kapitan. They work similarly to Python Virtual Environments but with more isolation from the lower-level OS-wide Python installation. Both of these projects manipulate your shell environment variables to make sure you're using the right binaries and modules. This document assumes you're using the bash shell. PyEnv and Software Collections are not Google projects, so please exercise your judgment as to whether these projects are suitable for your circumstances. On Debian-based Operating Systems - PyEnv Here at Streams, we use a Debian-based operating system for our day-to-day work. We've found that PyEnv works well for us. Getting Started Take a look at the PyEnv project on Github. There are two options for installing this project. The author makes a separate installer script available at this Github link , or you can manually install it. We recommend using the automated installer unless you have reason to use the manual installation process. The Automated Installer To use the installer, we would recommend downloading the installer script and examining it before you execute it. $ curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer > pyenv-installer Then, once you've checked it, you can execute it. PyEnv doesn't need root privileges, so you can run it without using sudo $ bash ./pyenv-installer Instructions on updating and removing the tool when you've installed it with the installer can be found on the pyenv-installer project page. Pay attention to the output of the installer. It might require you to add lines to your .bashrc file manually. Manually Installing from Github Take a look at the README.md on the PyEnv project page and follow the installation instructions there. Using Kapitan with PyEnv Once you have successfully installed PyEnv, you'll need to restart your shell. Either open a new shell session or source your .bashrc file like so: $ source ~/.bashrc Now that you have PyEnv ready to go, we can check it runs: $ pyenv pyenv 1.2.8 Usage: pyenv <command> [<args>] ... Before you move onto the next step, you'll need to install some dependencies if they aren't already present. To do this, you might need root access: # apt install libssl-dev libffi-dev Let's install Python 3.7.1, which is a stable and up-to-date release at the time of writing. We know Kapitan works with this release. $ pyenv install 3 .7.1 Downloading Python-3.7.1.tar.xz... -> https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz Installing Python-3.7.1... Installed Python-3.7.1 to /home/mikejo/.pyenv/versions/3.7.1 $ pyenv local 3 .7.1 $ python --version Python 3.7.1 $ Once it completes, we can activate the newer Python installation and set about installing Kapitan! Make sure PyEnv is activated using the $ pyenv local 3.7.1 command above and then run the following: $ pip install --user --upgrade kapitan After Kapitan is installed in this way, you might have to add the following to your PATH environment variable: $ { HOME } /.local/bin and you can do this like so: export PATH=${HOME}/.local/bin:${PATH} Add that line to the end of your .bashrc if you'd like it to take effect in all the shell sessions you use. You can now check everything installed correctly and start using Kapitan! $ kapitan usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,refs} ... On RHEL-based Operating Systems - Software Collections PyEnv will work on RHEL-based operating systems (including the upstream Fedora project). Another option is to use the Software Collections project. It's a community project with backing from Red Hat, and it includes both official Red Hat releases of some software collections and third-party contributions. While Kapitan only needs you to install an official Red Hat collection release, please remember this isn't a Google project and to use your judgment as to whether this is appropriate for your circumstances. Installing Software Collections support on your machine Software Collections has installation documentation available here As this procedure needs you to add a repository to the OS package manager, you'll need to be root. Use su or run the following with sudo as appropriate. Once you've completed the installation of the scl tool, install the Python 3.5 SCL package (YUM/DNF package names are identical to the name of the Software Collection). # yum install rh-python35 As of this point, you don't need to be root any more. Return to your regular shell and activate the Python 3.5 software collection you just installed. This command starts a shell that uses the Python 3.5 installation you just carried out: $ scl enable rh-python35 bash Install Kapitan: $ pip install --user --upgrade kapitan After Kapitan is installed in this way, you might have to add the following to your PATH environment variable: $ { HOME } /.local/bin and you can do this like so: export PATH=${HOME}/.local/bin:${PATH} Add that line to the end of your .bashrc if you'd like it to take effect in all the shell sessions you use. You can now check everything installed correctly and start using Kapitan! $ kapitan usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,secrets} ... When you come back to using this method after restarting your shell, you can switch back to the rh-python35 collection either by creating a shell alias for the kapitan command to 'scl enable rh-python35 kapitan' but we recommend that you can use the scl command to start a new shell using '$ scl enable rh-python35 bash' Once you finish using the software collection, exit the shell with exit or Ctrl+D","title":"Set up kapitan on older Python systems"},{"location":"pyenv-scl/#kapitan-on-older-linux-systems","text":"","title":"Kapitan on Older Linux Systems"},{"location":"pyenv-scl/#introduction","text":"Kapitan requires Python 3.6, and you need to be able to install the dependencies in the requirements file . However, sometimes this isn't entirely straightforward, and you may not be able or willing to install new versions of Python system-wide. We do provide a dockerfile which you can use to run Kapitan in a container, but if this isn't practical or possible either, you may wish to use one of the following options: PyEnv (Linux, distro-agnostic) Software Collections (RHEL-based distros) Both of these projects allow you to use a different version of Python specifically for your work with or on Kapitan. They work similarly to Python Virtual Environments but with more isolation from the lower-level OS-wide Python installation. Both of these projects manipulate your shell environment variables to make sure you're using the right binaries and modules. This document assumes you're using the bash shell. PyEnv and Software Collections are not Google projects, so please exercise your judgment as to whether these projects are suitable for your circumstances.","title":"Introduction"},{"location":"pyenv-scl/#on-debian-based-operating-systems-pyenv","text":"Here at Streams, we use a Debian-based operating system for our day-to-day work. We've found that PyEnv works well for us.","title":"On Debian-based Operating Systems - PyEnv"},{"location":"pyenv-scl/#getting-started","text":"Take a look at the PyEnv project on Github. There are two options for installing this project. The author makes a separate installer script available at this Github link , or you can manually install it. We recommend using the automated installer unless you have reason to use the manual installation process.","title":"Getting Started"},{"location":"pyenv-scl/#the-automated-installer","text":"To use the installer, we would recommend downloading the installer script and examining it before you execute it. $ curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer > pyenv-installer Then, once you've checked it, you can execute it. PyEnv doesn't need root privileges, so you can run it without using sudo $ bash ./pyenv-installer Instructions on updating and removing the tool when you've installed it with the installer can be found on the pyenv-installer project page. Pay attention to the output of the installer. It might require you to add lines to your .bashrc file manually.","title":"The Automated Installer"},{"location":"pyenv-scl/#manually-installing-from-github","text":"Take a look at the README.md on the PyEnv project page and follow the installation instructions there.","title":"Manually Installing from Github"},{"location":"pyenv-scl/#using-kapitan-with-pyenv","text":"Once you have successfully installed PyEnv, you'll need to restart your shell. Either open a new shell session or source your .bashrc file like so: $ source ~/.bashrc Now that you have PyEnv ready to go, we can check it runs: $ pyenv pyenv 1.2.8 Usage: pyenv <command> [<args>] ... Before you move onto the next step, you'll need to install some dependencies if they aren't already present. To do this, you might need root access: # apt install libssl-dev libffi-dev Let's install Python 3.7.1, which is a stable and up-to-date release at the time of writing. We know Kapitan works with this release. $ pyenv install 3 .7.1 Downloading Python-3.7.1.tar.xz... -> https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz Installing Python-3.7.1... Installed Python-3.7.1 to /home/mikejo/.pyenv/versions/3.7.1 $ pyenv local 3 .7.1 $ python --version Python 3.7.1 $ Once it completes, we can activate the newer Python installation and set about installing Kapitan! Make sure PyEnv is activated using the $ pyenv local 3.7.1 command above and then run the following: $ pip install --user --upgrade kapitan After Kapitan is installed in this way, you might have to add the following to your PATH environment variable: $ { HOME } /.local/bin and you can do this like so: export PATH=${HOME}/.local/bin:${PATH} Add that line to the end of your .bashrc if you'd like it to take effect in all the shell sessions you use. You can now check everything installed correctly and start using Kapitan! $ kapitan usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,refs} ...","title":"Using Kapitan with PyEnv"},{"location":"pyenv-scl/#on-rhel-based-operating-systems-software-collections","text":"PyEnv will work on RHEL-based operating systems (including the upstream Fedora project). Another option is to use the Software Collections project. It's a community project with backing from Red Hat, and it includes both official Red Hat releases of some software collections and third-party contributions. While Kapitan only needs you to install an official Red Hat collection release, please remember this isn't a Google project and to use your judgment as to whether this is appropriate for your circumstances.","title":"On RHEL-based Operating Systems - Software Collections"},{"location":"pyenv-scl/#installing-software-collections-support-on-your-machine","text":"Software Collections has installation documentation available here As this procedure needs you to add a repository to the OS package manager, you'll need to be root. Use su or run the following with sudo as appropriate. Once you've completed the installation of the scl tool, install the Python 3.5 SCL package (YUM/DNF package names are identical to the name of the Software Collection). # yum install rh-python35 As of this point, you don't need to be root any more. Return to your regular shell and activate the Python 3.5 software collection you just installed. This command starts a shell that uses the Python 3.5 installation you just carried out: $ scl enable rh-python35 bash Install Kapitan: $ pip install --user --upgrade kapitan After Kapitan is installed in this way, you might have to add the following to your PATH environment variable: $ { HOME } /.local/bin and you can do this like so: export PATH=${HOME}/.local/bin:${PATH} Add that line to the end of your .bashrc if you'd like it to take effect in all the shell sessions you use. You can now check everything installed correctly and start using Kapitan! $ kapitan usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,secrets} ... When you come back to using this method after restarting your shell, you can switch back to the rh-python35 collection either by creating a shell alias for the kapitan command to 'scl enable rh-python35 kapitan' but we recommend that you can use the scl command to start a new shell using '$ scl enable rh-python35 bash' Once you finish using the software collection, exit the shell with exit or Ctrl+D","title":"Installing Software Collections support on your machine"},{"location":"secrets/","text":"Kapitan References (Formerly Secrets) Kapitan can manage references and secrets with the following key management services: GPG Google Cloud KMS (beta) AWS KMS (beta) Environment Vaultkv (read only support) If you want to get started with secrets but don't have a GPG or KMS setup, you can also use the secret ref type. Note that ref is not encrypted and is intended for development purposes only. Do not use ref secrets if you're storing sensitive information! Using Secrets The usual flow of creating and using an encrypted secret with kapitan is: 1. Define your GPG recipients, Vault client parameters or KMS key This is done in the inventory under parameters.kapitan.secrets . Just like any other inventory parameters, this can be inherited from a common class or defined per target. For example, common.yml may contain: parameters : kapitan : vars : target : ${target_name} namespace : ${target_name} secrets : gpg : recipients : - name : example@kapitan.dev fingerprint : D9234C61F58BEB3ED8552A57E28DC07A3CBFAE7C gkms : key : 'projects/<project>/locations/<location>/keyRings/<keyRing>/cryptoKeys/<key>' awskms : key : 'alias/nameOfKey' vaultkv : VAULT_ADDR : http://127.0.0.1:8200 auth : token 2. Create Your Secret Manually via command line: $ kapitan refs --write <secret_type>:path/to/secret/file -t <target_name> -f <secret_file> \u200b where <secret_type> can be any of: ref : ref type (not encrypted) gpg : GPG gkms : Google Cloud KMS awskms : AWS KMS vaultkv : Hashicorp Vault with kv/kv-v2 secret engine Kapitan will inherit the secrets configuration for the specified target, and encrypt and save your secret into <path/to/secret/file> . Automatically When referencing your secret in the inventory during compile, you can use the following functions to automatically generate, encrypt and save your secret: randomstr - Generates a random string. You can optionally pass the length you want i.e. `||randomstr:32` base64 - base64 encodes your secret; to be used as a secondary function i.e. `||randomstr|base64` sha256 - sha256 hashes your secret; to be used as a secondary function i.e. `||randomstr|sha256`. You can optionally pass a salt i.e `||randomstr|sha256:salt` -> becomes `sha256(\"salt:<generated random string>\")` reveal - Decrypts a secret; to be used as a secondary function, useful for reuse of a secret like for different encodings i.e `||reveal:path/to/secret|base64` rsa - Generates an RSA 4096 private key (PKCS#8). You can optionally pass the key size i.e. `||rsa:2048` ed25519 - Generates a ed25519 private key (PKCS#8). publickey - Derives the public key from a revealed private key i.e. `||reveal:path/to/encrypted_private_key|publickey` rsapublic - Derives an RSA public key from a revealed private key i.e. `||reveal:path/to/encrypted_private_key|rsapublic` (deprecated, use `publickey` instead) Note : The first operator here || is more similar to a logical OR. If the secret file doesn't exist, kapitan will generate it and apply the functions after the || . If the secret file already exists, no functions will run. Note : If you use |reveal:/path/secret , when changing the /path/secret file make sure you also delete any secrets referencing /path/secret so kapitan can regenerate them. Note : vaultkv can't be used to generate secrets automatically for now, manually create the secret using the command line. 3. Reference your secrets in your classes/targets and run kapitan compile Secrets can be referenced in the format ?{<secret_type>:path/to/secret/file} . For example, assume for now that your GPG-encrypted secret is already stored in a file at targets/secrets/mysql_password . This can be referenced in the inventory in the following format: users : root : # If 'secrets/targets/${target_name}/mysql/password' doesn't exist, we can automatically generate a random b64-encoded password as follows password : ?{gpg:targets/${target_name}/mysql/password|randomstr|base64} During compile, kapitan will search for the path targets/${target_name}/mysql/password . Should it not exist, then it will automatically generate a random base64 password and save it to that path. 4. Reveal and use the secrets You can reveal the secrets referenced in the outputs of kapitan compile via: $ kapitan refs --reveal -f path/to/rendered/template For example, compiled/minikube-mysql/manifests/mysql_secret.yml with the following content: apiVersion : v1 data : MYSQL_ROOT_PASSWORD : ?{gpg:targets/minikube-mysql/mysql/password:ec3d54de} MYSQL_ROOT_PASSWORD_SHA256 : ?{gpg:targets/minikube-mysql/mysql/password_sha256:122d2732} kind : Secret metadata : annotations : {} labels : name : example-mysql name : example-mysql namespace : minikube-mysql type : Opaque can be revealed as follows: $ kapitan refs --reveal -f compiled/minikube-mysql/manifests/mysql_secret.yml This will substitute the referenced secrets with the actual decrypted secrets stored at the referenced paths and display the file content. You can also use: $ kapitan refs --reveal --ref-file refs/targets/all-glob/mysql/password or $ kapitan refs --reveal --tag \"?{base64:targets/all-glob/mysql/password}\" $ # or $ kapitan refs --reveal --tag \"?{base64:targets/all-glob/mysql/password:3192c15c}\" for more convenience. 5. Compile refs in embedded format This allows revealing compiled files without needing access to ref files by using: $ kapitan compile --embed-refs Compiled files containing refs will now have the references embedded in the compiled file under the following format (gkms backend used as an example): ?{gkms:ReallyLongBase64HereZ2FyZ2FiZQo=:embedded} Which means that compiled outputs can now be completely distributed (e.g. in CI/CD systems that apply changes) without the need to access the refs directory. You can also check out Tesoro for Kubernetes which will reveal embedded secret refs in the cluster. Secret Sub-Variables As illustrated above, one file corresponds to one secret. It is now possible for users who would like to reduce the decryption overhead to manually create a yaml file that contains multiple secrets, each of which can be referenced by its object key. For example, consider the secret file refs/mysql_secrets : mysql_passwords : secret_foo : hello_world secret_bar : 54321password This can be manually encrypted by: $ kapitan refs --write gpg:components/secrets/mysql_secrets -t prod -f secrets/mysql_secrets To reference secret_foo inside this file, you can specify it in the inventory as follows: secret_foo: ${gpg:components/secrets/mysql_secrets@mysql_passwords.secret_foo} Environment References Backend It may be useful in some occasions when revealing references to have the values for the reference dynamically come from the environment in which Kapitan is executing. This backend provides such functionality. It will attempt to locate a value for a reference from the environment using a prefixed variable $KAPITAN_VAR_* convention and use this value with the refs command. $ echo \"my_default_value\" | kapitan refs --write env:path/to/secret_inside_kapitan -t <target_name> -f - When this reference is created and then referred to in the parameters, it will use the last path component, from a split, to locate a variable in the current environment to use as the value. If this variable cannot be found in the environment, it will use the default value written to the refs file on the filesystem. parameters : mysql_passwordS : secret_foo : ?{env:my/mysql/mysql_secret_foo} secret_bar : ?{env:my/mysql/mysql_secret_bar} When using the above parameters reference, values would be consulted in the environment from the following variables: $KAPITAN_VAR_mysql_secret_foo $KAPITAN_VAR_mysql_secret_bar Vaultkv Secret Backend (Read Only) - Addons Considering a key-value pair like my_key : my_secret in the path secret/foo/bar in a kv-v2(KV version 2) secret engine on the vault server, to use this as a secret use: $ echo \"foo/bar:my_key\" | kapitan refs --write vaultkv:path/to/secret_inside_kapitan -t <target_name> -f - Parameters in the secret file are collected from the inventory of the target we gave from CLI -t <target_name> . If target isn't provided then kapitan will identify the variables from the environment when revealing secret. Environment variables that can be defined in kapitan inventory are VAULT_ADDR , VAULT_NAMESPACE , VAULT_SKIP_VERIFY , VAULT_CLIENT_CERT , VAULT_CLIENT_KEY , VAULT_CAPATH & VAULT_CACERT . Extra parameters that can be defined in inventory are: * auth : specify which authentication method to use like token , userpass , ldap , github & approle * mount : specify the mount point of key's path. e.g if path= alpha-secret/foo/bar then mount: alpha-secret (default secret ) * engine : secret engine used, either kv-v2 or kv (default kv-v2 ) Environment variables cannot be defined in inventory are VAULT_TOKEN , VAULT_USERNAME , VAULT_PASSWORD , VAULT_ROLE_ID , VAULT_SECRET_ID . parameters : kapitan : secrets : vaultkv : auth : userpass engine : kv-v2 mount : team-alpha-secret VAULT_ADDR : http://127.0.0.1:8200 VAULT_NAMESPACE : CICD-alpha VAULT_SKIP_VERIFY : false VAULT_CLIENT_KEY : /path/to/key VAULT_CLIENT_CERT : /path/to/cert","title":"Secret management"},{"location":"secrets/#kapitan-references-formerly-secrets","text":"Kapitan can manage references and secrets with the following key management services: GPG Google Cloud KMS (beta) AWS KMS (beta) Environment Vaultkv (read only support) If you want to get started with secrets but don't have a GPG or KMS setup, you can also use the secret ref type. Note that ref is not encrypted and is intended for development purposes only. Do not use ref secrets if you're storing sensitive information!","title":"Kapitan References (Formerly Secrets)"},{"location":"secrets/#using-secrets","text":"The usual flow of creating and using an encrypted secret with kapitan is:","title":"Using Secrets"},{"location":"secrets/#1-define-your-gpg-recipients-vault-client-parameters-or-kms-key","text":"This is done in the inventory under parameters.kapitan.secrets . Just like any other inventory parameters, this can be inherited from a common class or defined per target. For example, common.yml may contain: parameters : kapitan : vars : target : ${target_name} namespace : ${target_name} secrets : gpg : recipients : - name : example@kapitan.dev fingerprint : D9234C61F58BEB3ED8552A57E28DC07A3CBFAE7C gkms : key : 'projects/<project>/locations/<location>/keyRings/<keyRing>/cryptoKeys/<key>' awskms : key : 'alias/nameOfKey' vaultkv : VAULT_ADDR : http://127.0.0.1:8200 auth : token","title":"1. Define your GPG recipients, Vault client parameters or KMS key"},{"location":"secrets/#2-create-your-secret","text":"","title":"2. Create Your Secret"},{"location":"secrets/#manually-via-command-line","text":"$ kapitan refs --write <secret_type>:path/to/secret/file -t <target_name> -f <secret_file> \u200b where <secret_type> can be any of: ref : ref type (not encrypted) gpg : GPG gkms : Google Cloud KMS awskms : AWS KMS vaultkv : Hashicorp Vault with kv/kv-v2 secret engine Kapitan will inherit the secrets configuration for the specified target, and encrypt and save your secret into <path/to/secret/file> .","title":"Manually via command line:"},{"location":"secrets/#automatically","text":"When referencing your secret in the inventory during compile, you can use the following functions to automatically generate, encrypt and save your secret: randomstr - Generates a random string. You can optionally pass the length you want i.e. `||randomstr:32` base64 - base64 encodes your secret; to be used as a secondary function i.e. `||randomstr|base64` sha256 - sha256 hashes your secret; to be used as a secondary function i.e. `||randomstr|sha256`. You can optionally pass a salt i.e `||randomstr|sha256:salt` -> becomes `sha256(\"salt:<generated random string>\")` reveal - Decrypts a secret; to be used as a secondary function, useful for reuse of a secret like for different encodings i.e `||reveal:path/to/secret|base64` rsa - Generates an RSA 4096 private key (PKCS#8). You can optionally pass the key size i.e. `||rsa:2048` ed25519 - Generates a ed25519 private key (PKCS#8). publickey - Derives the public key from a revealed private key i.e. `||reveal:path/to/encrypted_private_key|publickey` rsapublic - Derives an RSA public key from a revealed private key i.e. `||reveal:path/to/encrypted_private_key|rsapublic` (deprecated, use `publickey` instead) Note : The first operator here || is more similar to a logical OR. If the secret file doesn't exist, kapitan will generate it and apply the functions after the || . If the secret file already exists, no functions will run. Note : If you use |reveal:/path/secret , when changing the /path/secret file make sure you also delete any secrets referencing /path/secret so kapitan can regenerate them. Note : vaultkv can't be used to generate secrets automatically for now, manually create the secret using the command line.","title":"Automatically"},{"location":"secrets/#3-reference-your-secrets-in-your-classestargets-and-run-kapitan-compile","text":"Secrets can be referenced in the format ?{<secret_type>:path/to/secret/file} . For example, assume for now that your GPG-encrypted secret is already stored in a file at targets/secrets/mysql_password . This can be referenced in the inventory in the following format: users : root : # If 'secrets/targets/${target_name}/mysql/password' doesn't exist, we can automatically generate a random b64-encoded password as follows password : ?{gpg:targets/${target_name}/mysql/password|randomstr|base64} During compile, kapitan will search for the path targets/${target_name}/mysql/password . Should it not exist, then it will automatically generate a random base64 password and save it to that path.","title":"3. Reference your secrets in your classes/targets and run kapitan compile"},{"location":"secrets/#4-reveal-and-use-the-secrets","text":"You can reveal the secrets referenced in the outputs of kapitan compile via: $ kapitan refs --reveal -f path/to/rendered/template For example, compiled/minikube-mysql/manifests/mysql_secret.yml with the following content: apiVersion : v1 data : MYSQL_ROOT_PASSWORD : ?{gpg:targets/minikube-mysql/mysql/password:ec3d54de} MYSQL_ROOT_PASSWORD_SHA256 : ?{gpg:targets/minikube-mysql/mysql/password_sha256:122d2732} kind : Secret metadata : annotations : {} labels : name : example-mysql name : example-mysql namespace : minikube-mysql type : Opaque can be revealed as follows: $ kapitan refs --reveal -f compiled/minikube-mysql/manifests/mysql_secret.yml This will substitute the referenced secrets with the actual decrypted secrets stored at the referenced paths and display the file content. You can also use: $ kapitan refs --reveal --ref-file refs/targets/all-glob/mysql/password or $ kapitan refs --reveal --tag \"?{base64:targets/all-glob/mysql/password}\" $ # or $ kapitan refs --reveal --tag \"?{base64:targets/all-glob/mysql/password:3192c15c}\" for more convenience.","title":"4. Reveal and use the secrets"},{"location":"secrets/#5-compile-refs-in-embedded-format","text":"This allows revealing compiled files without needing access to ref files by using: $ kapitan compile --embed-refs Compiled files containing refs will now have the references embedded in the compiled file under the following format (gkms backend used as an example): ?{gkms:ReallyLongBase64HereZ2FyZ2FiZQo=:embedded} Which means that compiled outputs can now be completely distributed (e.g. in CI/CD systems that apply changes) without the need to access the refs directory. You can also check out Tesoro for Kubernetes which will reveal embedded secret refs in the cluster.","title":"5. Compile refs in embedded format"},{"location":"secrets/#secret-sub-variables","text":"As illustrated above, one file corresponds to one secret. It is now possible for users who would like to reduce the decryption overhead to manually create a yaml file that contains multiple secrets, each of which can be referenced by its object key. For example, consider the secret file refs/mysql_secrets : mysql_passwords : secret_foo : hello_world secret_bar : 54321password This can be manually encrypted by: $ kapitan refs --write gpg:components/secrets/mysql_secrets -t prod -f secrets/mysql_secrets To reference secret_foo inside this file, you can specify it in the inventory as follows: secret_foo: ${gpg:components/secrets/mysql_secrets@mysql_passwords.secret_foo}","title":"Secret Sub-Variables"},{"location":"secrets/#environment-references-backend","text":"It may be useful in some occasions when revealing references to have the values for the reference dynamically come from the environment in which Kapitan is executing. This backend provides such functionality. It will attempt to locate a value for a reference from the environment using a prefixed variable $KAPITAN_VAR_* convention and use this value with the refs command. $ echo \"my_default_value\" | kapitan refs --write env:path/to/secret_inside_kapitan -t <target_name> -f - When this reference is created and then referred to in the parameters, it will use the last path component, from a split, to locate a variable in the current environment to use as the value. If this variable cannot be found in the environment, it will use the default value written to the refs file on the filesystem. parameters : mysql_passwordS : secret_foo : ?{env:my/mysql/mysql_secret_foo} secret_bar : ?{env:my/mysql/mysql_secret_bar} When using the above parameters reference, values would be consulted in the environment from the following variables: $KAPITAN_VAR_mysql_secret_foo $KAPITAN_VAR_mysql_secret_bar","title":"Environment References Backend"},{"location":"secrets/#vaultkv-secret-backend-read-only-addons","text":"Considering a key-value pair like my_key : my_secret in the path secret/foo/bar in a kv-v2(KV version 2) secret engine on the vault server, to use this as a secret use: $ echo \"foo/bar:my_key\" | kapitan refs --write vaultkv:path/to/secret_inside_kapitan -t <target_name> -f - Parameters in the secret file are collected from the inventory of the target we gave from CLI -t <target_name> . If target isn't provided then kapitan will identify the variables from the environment when revealing secret. Environment variables that can be defined in kapitan inventory are VAULT_ADDR , VAULT_NAMESPACE , VAULT_SKIP_VERIFY , VAULT_CLIENT_CERT , VAULT_CLIENT_KEY , VAULT_CAPATH & VAULT_CACERT . Extra parameters that can be defined in inventory are: * auth : specify which authentication method to use like token , userpass , ldap , github & approle * mount : specify the mount point of key's path. e.g if path= alpha-secret/foo/bar then mount: alpha-secret (default secret ) * engine : secret engine used, either kv-v2 or kv (default kv-v2 ) Environment variables cannot be defined in inventory are VAULT_TOKEN , VAULT_USERNAME , VAULT_PASSWORD , VAULT_ROLE_ID , VAULT_SECRET_ID . parameters : kapitan : secrets : vaultkv : auth : userpass engine : kv-v2 mount : team-alpha-secret VAULT_ADDR : http://127.0.0.1:8200 VAULT_NAMESPACE : CICD-alpha VAULT_SKIP_VERIFY : false VAULT_CLIENT_KEY : /path/to/key VAULT_CLIENT_CERT : /path/to/cert","title":"Vaultkv Secret Backend (Read Only) - Addons"},{"location":"usage/","text":"Usage To see all the available commands, run: $ kapitan -h usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,secrets,lint} ... Generic templated configuration management for Kubernetes, Terraform and other things positional arguments: {eval,compile,inventory,searchvar,secrets,lint,init,validate} commands eval evaluate jsonnet file compile compile targets inventory show inventory searchvar show all inventory files where var is declared refs manage secrets lint linter for inventory and secrets init initialize a directory with the recommended kapitan project skeleton. validate validate the compile output against schemas as specified in inventory optional arguments: -h, --help show this help message and exit --version show program's version number and exit Additional parameters are available for each positional argument. For example: $ kapitan compile -h usage: kapitan compile [-h] [--search-paths JPATH [JPATH ...]] [--jinja2-filters FPATH] [--verbose] [--prune] [--quiet] [--output-path PATH] [--fetch] [--validate] [--parallelism INT] [--indent INT] [--refs-path REFS_PATH] [--reveal] [--inventory-path INVENTORY_PATH] [--cache] [--cache-paths PATH [PATH ...]] [--ignore-version-check] [--schemas-path SCHEMAS_PATH] [--targets TARGET [TARGET ...] | --labels [key=value [key=value ...]]] optional arguments: -h, --help show this help message and exit --search-paths JPATH [JPATH ...], -J JPATH [JPATH ...] set search paths, default is [\".\"] --jinja2-filters FPATH, -J2F FPATH load custom jinja2 filters from any file, default is to put them inside lib/jinja2_filters.py --verbose, -v set verbose mode --prune prune jsonnet output --quiet set quiet mode, only critical output --output-path PATH set output path, default is \".\" --fetch fetches external dependencies --validate validate compile output against schemas as specified in inventory --parallelism INT, -p INT Number of concurrent compile processes, default is 4 --indent INT, -i INT Indentation spaces for YAML/JSON, default is 2 --refs-path REFS_PATH set refs path, default is \"./refs\" --reveal reveal refs (warning: this will potentially write sensitive data) --inventory-path INVENTORY_PATH set inventory path, default is \"./inventory\" --cache, -c enable compilation caching to .kapitan_cache, default is False --cache-paths PATH [PATH ...] cache additional paths to .kapitan_cache, default is [] --ignore-version-check ignore the version from .kapitan --schemas-path SCHEMAS_PATH set schema cache path, default is \"./schemas\" --targets TARGET [TARGET ...], -t TARGET [TARGET ...] targets to compile, default is all --labels [key=value [key=value ...]], -l [key=value [key=value ...]] compile targets matching the labels, default is all Selective target compilation If you only want to compile a subset or specific targets, you can use the two kapitan compile flags --targets, -t or --labels, -l . Specific target(s) $ cd examples/kubernetes $ kapitan compile -t minikube-mysql Compiled minikube-mysql (0.43s) Using labels $ cd examples/kubernetes $ cat inventory/classes/component/nginx-kadet.yml # Inherited by minikube-nginx-kadet target parameters: ... kapitan: ... labels: type: kadet $ kapitan compile -l type=kadet Compiled minikube-nginx-kadet (0.14s) Using .kapitan config file These parameters can also be defined in a local .kapitan file per project directory, for example: To enforce the kapitan version used for compilation (for consistency and safety), you can add version to .kapitan : $ cat .kapitan version: 0.21.0 Or to skip all minor version checks: $ cat .kapitan version: 0.21 You can also permanently define all command line flags in the .kapitan config file. For example: $ cat .kapitan version: 0.21 compile: indent: 4 parallelism: 8 would be equivalent to running: kapitan compile --indent 4 --parallelism 8 or $ cat .kapitan version: 0.21 inventory: inventory-path: ./some_path which would be equivalent to always running: kapitan inventory --inventory-path=./some_path","title":"Usage"},{"location":"usage/#usage","text":"To see all the available commands, run: $ kapitan -h usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,secrets,lint} ... Generic templated configuration management for Kubernetes, Terraform and other things positional arguments: {eval,compile,inventory,searchvar,secrets,lint,init,validate} commands eval evaluate jsonnet file compile compile targets inventory show inventory searchvar show all inventory files where var is declared refs manage secrets lint linter for inventory and secrets init initialize a directory with the recommended kapitan project skeleton. validate validate the compile output against schemas as specified in inventory optional arguments: -h, --help show this help message and exit --version show program's version number and exit Additional parameters are available for each positional argument. For example: $ kapitan compile -h usage: kapitan compile [-h] [--search-paths JPATH [JPATH ...]] [--jinja2-filters FPATH] [--verbose] [--prune] [--quiet] [--output-path PATH] [--fetch] [--validate] [--parallelism INT] [--indent INT] [--refs-path REFS_PATH] [--reveal] [--inventory-path INVENTORY_PATH] [--cache] [--cache-paths PATH [PATH ...]] [--ignore-version-check] [--schemas-path SCHEMAS_PATH] [--targets TARGET [TARGET ...] | --labels [key=value [key=value ...]]] optional arguments: -h, --help show this help message and exit --search-paths JPATH [JPATH ...], -J JPATH [JPATH ...] set search paths, default is [\".\"] --jinja2-filters FPATH, -J2F FPATH load custom jinja2 filters from any file, default is to put them inside lib/jinja2_filters.py --verbose, -v set verbose mode --prune prune jsonnet output --quiet set quiet mode, only critical output --output-path PATH set output path, default is \".\" --fetch fetches external dependencies --validate validate compile output against schemas as specified in inventory --parallelism INT, -p INT Number of concurrent compile processes, default is 4 --indent INT, -i INT Indentation spaces for YAML/JSON, default is 2 --refs-path REFS_PATH set refs path, default is \"./refs\" --reveal reveal refs (warning: this will potentially write sensitive data) --inventory-path INVENTORY_PATH set inventory path, default is \"./inventory\" --cache, -c enable compilation caching to .kapitan_cache, default is False --cache-paths PATH [PATH ...] cache additional paths to .kapitan_cache, default is [] --ignore-version-check ignore the version from .kapitan --schemas-path SCHEMAS_PATH set schema cache path, default is \"./schemas\" --targets TARGET [TARGET ...], -t TARGET [TARGET ...] targets to compile, default is all --labels [key=value [key=value ...]], -l [key=value [key=value ...]] compile targets matching the labels, default is all","title":"Usage"},{"location":"usage/#selective-target-compilation","text":"If you only want to compile a subset or specific targets, you can use the two kapitan compile flags --targets, -t or --labels, -l .","title":"Selective target compilation"},{"location":"usage/#specific-targets","text":"$ cd examples/kubernetes $ kapitan compile -t minikube-mysql Compiled minikube-mysql (0.43s)","title":"Specific target(s)"},{"location":"usage/#using-labels","text":"$ cd examples/kubernetes $ cat inventory/classes/component/nginx-kadet.yml # Inherited by minikube-nginx-kadet target parameters: ... kapitan: ... labels: type: kadet $ kapitan compile -l type=kadet Compiled minikube-nginx-kadet (0.14s)","title":"Using labels"},{"location":"usage/#using-kapitan-config-file","text":"These parameters can also be defined in a local .kapitan file per project directory, for example: To enforce the kapitan version used for compilation (for consistency and safety), you can add version to .kapitan : $ cat .kapitan version: 0.21.0 Or to skip all minor version checks: $ cat .kapitan version: 0.21 You can also permanently define all command line flags in the .kapitan config file. For example: $ cat .kapitan version: 0.21 compile: indent: 4 parallelism: 8 would be equivalent to running: kapitan compile --indent 4 --parallelism 8 or $ cat .kapitan version: 0.21 inventory: inventory-path: ./some_path which would be equivalent to always running: kapitan inventory --inventory-path=./some_path","title":"Using .kapitan config file"},{"location":"validate/","text":"kapitan validate Validates the schema of compiled output. Validate options are specified in the inventory under parameters.kapitan.validate . Supported types are: kubernetes manifests Usage Manually via command line after compile: $ kapitan validate Automatically, together with compile: $ kapitan compile --validate Kubernetes manifests Overview Kubernetes resources are identified by their kind . For example, they are: service deployment statefulset The manifest for each kind has certain restrictions such as required properties. Using kapitan, you can validate against the schemas to confirm that your compiled output indeed is a valid kubernetes manifest. First time they are used, the schemas for kubernetes manifests are dynamically downloaded from https://kubernetesjsonschema.dev . Those schemas will be cached into ./schemas/ by default, which can be modified using --schemas-path option. However, it is recommended to use .kapitan configuration as follows to avoid the need of typing down this option for every command: $ cat .kapitan # other options abbreviated for clarity validate: schemas-path: custom/schemas/cache/path Example Refer to the minikube-es inventory in kapitan inventory . To validate the schema of the compiled StatefulSet manifest at compiled/minikube-es/manifests/es-client.yml (created by components/elasticsearch/main.jsonnet ), add kapitan.validate parameters in minikube-es inventory: kapitan : vars : target : ${target_name} namespace : ${target_name} compile : - output_path : manifests input_type : jsonnet input_paths : - components/elasticsearch/main.jsonnet ### other inputs abbreviated for clarity ### validate : - output_paths : - manifests/es-client.yml type : kubernetes kind : statefulset # note that it is in lowercase version : 1.14.0 # optional, defaults to 1.14.0 Then run: $ kapitan validate -t minikube-es invalid 'statefulset' manifest at ./compiled/minikube-es/manifests/es-client.yml ['spec'] 'selector' is a required property","title":"Manifest validation"},{"location":"validate/#kapitan-validate","text":"Validates the schema of compiled output. Validate options are specified in the inventory under parameters.kapitan.validate . Supported types are: kubernetes manifests","title":"kapitan validate"},{"location":"validate/#usage","text":"Manually via command line after compile: $ kapitan validate Automatically, together with compile: $ kapitan compile --validate","title":"Usage"},{"location":"validate/#kubernetes-manifests","text":"","title":"Kubernetes manifests"},{"location":"validate/#overview","text":"Kubernetes resources are identified by their kind . For example, they are: service deployment statefulset The manifest for each kind has certain restrictions such as required properties. Using kapitan, you can validate against the schemas to confirm that your compiled output indeed is a valid kubernetes manifest. First time they are used, the schemas for kubernetes manifests are dynamically downloaded from https://kubernetesjsonschema.dev . Those schemas will be cached into ./schemas/ by default, which can be modified using --schemas-path option. However, it is recommended to use .kapitan configuration as follows to avoid the need of typing down this option for every command: $ cat .kapitan # other options abbreviated for clarity validate: schemas-path: custom/schemas/cache/path","title":"Overview"},{"location":"validate/#example","text":"Refer to the minikube-es inventory in kapitan inventory . To validate the schema of the compiled StatefulSet manifest at compiled/minikube-es/manifests/es-client.yml (created by components/elasticsearch/main.jsonnet ), add kapitan.validate parameters in minikube-es inventory: kapitan : vars : target : ${target_name} namespace : ${target_name} compile : - output_path : manifests input_type : jsonnet input_paths : - components/elasticsearch/main.jsonnet ### other inputs abbreviated for clarity ### validate : - output_paths : - manifests/es-client.yml type : kubernetes kind : statefulset # note that it is in lowercase version : 1.14.0 # optional, defaults to 1.14.0 Then run: $ kapitan validate -t minikube-es invalid 'statefulset' manifest at ./compiled/minikube-es/manifests/es-client.yml ['spec'] 'selector' is a required property","title":"Example"},{"location":"kap_proposals/kap_0_kadet/","text":"Kadet This introduces a new experimental input type called Kadet. Kadet is essentially a Python module offering a set of classes and functions to define objects which will compile to JSON or YAML. A complete example is available in examples/kubernetes/components/nginx . Author: @ramaro Overview BaseObj BaseObj implements the basic object implementation that compiles into JSON or YAML. Setting keys in self.root means they will be in the compiled output. Keys can be set as an hierarchy of attributes (courtesy of addict ) The self.body() method is reserved for setting self.root on instantiation: The example below: class MyApp ( BaseObj ): def body ( self ): self . root . name = \"myapp\" self . root . inner . foo = \"bar\" self . root . list = [ 1 , 2 , 3 ] compiles into: --- name : myapp inner : foo : bar list : - 1 - 2 - 3 The self.new() method can be used to define a basic constructor. self.need() checks if a key is set and errors if it isn't (with an optional custom error message). kwargs that are passed onto a new instance of BaseObj are always accessible via self.kwargs In this example, MyApp needs name and foo to be passed as kwargs. class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) def body ( self ): self . root . name = self . kwargs . name self . root . inner . foo = self . kwargs . foo self . root . list = [ 1 , 2 , 3 ] obj = MyApp ( name = \"myapp\" , foo = \"bar\" ) Setting a skeleton Defining a large body with Python can be quite hard and repetitive to read and write. The self.update_root() method allows importing a YAML/JSON file to set the skeleton of self.root. MyApp's skeleton can be set instead like this: #skel.yml --- name : myapp inner : foo : bar list : - 1 - 2 - 3 class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) self . update_root ( \"path/to/skel.yml\" ) Extending a skeleton'd MyApp is possible just by implementing self.body() : class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) self . update_root ( \"path/to/skel.yml\" ) def body ( self ): self . set_replicas () self . root . metadata . labels = { \"app\" : \"mylabel\" } def set_replicas ( self ): self . root . spec . replicas = 5 Inheritance Python inheritance will work as expected: class MyOtherApp ( MyApp ): def new ( self ): super () . new () # MyApp's new() self . need ( \"size\" ) def body ( self ): super () . body () # we want to extend MyApp's body self . root . size = self . kwargs . size del self . root . list # get rid of \"list\" obj = MyOtherApp ( name = \"otherapp1\" , foo = \"bar2\" , size = 3 ) compiles to: --- name : otherapp1 inner : foo : bar2 replicas : 5 size : 3 Components A component in Kadet is a python module that must implement a main() function returning an instance of BaseObj . The inventory is also available via the inventory() function. For example, a tinyapp component: # components/tinyapp/__init__.py from kapitan.inputs.kadet import BaseOBj , inventory inv = inventory () # returns inventory for target being compiled class TinyApp ( BaseObj ): def body ( self ): self . root . foo = \"bar\" self . root . replicas = inv . parameters . tinyapp . replicas def main (): obj = BaseOb () obj . root . deployment = TinyApp () # will compile into deployment.yml return obj An inventory class must be created for tinyapp : # inventory/classes/components/tinyapp.yml parameters : tinyapp : replicas : 1 kapitan : compile : - output_path : manifests input_type : kadet output_type : yaml input_paths : - components/tinyapp Common components A library in --search-paths (which now defaults to . and lib/ ) can also be a module that kadet components import. It is loaded using the load_from_search_paths() : kubelib = load_from_search_paths ( \"kubelib\" ) # lib/kubelib/__init__.py def main (): obj = BaseObj () obj . root . example_app_deployment = kubelib . Deployment ( name = \"example-app\" ) return obj","title":"Kadet"},{"location":"kap_proposals/kap_0_kadet/#kadet","text":"This introduces a new experimental input type called Kadet. Kadet is essentially a Python module offering a set of classes and functions to define objects which will compile to JSON or YAML. A complete example is available in examples/kubernetes/components/nginx . Author: @ramaro","title":"Kadet"},{"location":"kap_proposals/kap_0_kadet/#overview","text":"","title":"Overview"},{"location":"kap_proposals/kap_0_kadet/#baseobj","text":"BaseObj implements the basic object implementation that compiles into JSON or YAML. Setting keys in self.root means they will be in the compiled output. Keys can be set as an hierarchy of attributes (courtesy of addict ) The self.body() method is reserved for setting self.root on instantiation: The example below: class MyApp ( BaseObj ): def body ( self ): self . root . name = \"myapp\" self . root . inner . foo = \"bar\" self . root . list = [ 1 , 2 , 3 ] compiles into: --- name : myapp inner : foo : bar list : - 1 - 2 - 3 The self.new() method can be used to define a basic constructor. self.need() checks if a key is set and errors if it isn't (with an optional custom error message). kwargs that are passed onto a new instance of BaseObj are always accessible via self.kwargs In this example, MyApp needs name and foo to be passed as kwargs. class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) def body ( self ): self . root . name = self . kwargs . name self . root . inner . foo = self . kwargs . foo self . root . list = [ 1 , 2 , 3 ] obj = MyApp ( name = \"myapp\" , foo = \"bar\" )","title":"BaseObj"},{"location":"kap_proposals/kap_0_kadet/#setting-a-skeleton","text":"Defining a large body with Python can be quite hard and repetitive to read and write. The self.update_root() method allows importing a YAML/JSON file to set the skeleton of self.root. MyApp's skeleton can be set instead like this: #skel.yml --- name : myapp inner : foo : bar list : - 1 - 2 - 3 class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) self . update_root ( \"path/to/skel.yml\" ) Extending a skeleton'd MyApp is possible just by implementing self.body() : class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) self . update_root ( \"path/to/skel.yml\" ) def body ( self ): self . set_replicas () self . root . metadata . labels = { \"app\" : \"mylabel\" } def set_replicas ( self ): self . root . spec . replicas = 5","title":"Setting a skeleton"},{"location":"kap_proposals/kap_0_kadet/#inheritance","text":"Python inheritance will work as expected: class MyOtherApp ( MyApp ): def new ( self ): super () . new () # MyApp's new() self . need ( \"size\" ) def body ( self ): super () . body () # we want to extend MyApp's body self . root . size = self . kwargs . size del self . root . list # get rid of \"list\" obj = MyOtherApp ( name = \"otherapp1\" , foo = \"bar2\" , size = 3 ) compiles to: --- name : otherapp1 inner : foo : bar2 replicas : 5 size : 3","title":"Inheritance"},{"location":"kap_proposals/kap_0_kadet/#components","text":"A component in Kadet is a python module that must implement a main() function returning an instance of BaseObj . The inventory is also available via the inventory() function. For example, a tinyapp component: # components/tinyapp/__init__.py from kapitan.inputs.kadet import BaseOBj , inventory inv = inventory () # returns inventory for target being compiled class TinyApp ( BaseObj ): def body ( self ): self . root . foo = \"bar\" self . root . replicas = inv . parameters . tinyapp . replicas def main (): obj = BaseOb () obj . root . deployment = TinyApp () # will compile into deployment.yml return obj An inventory class must be created for tinyapp : # inventory/classes/components/tinyapp.yml parameters : tinyapp : replicas : 1 kapitan : compile : - output_path : manifests input_type : kadet output_type : yaml input_paths : - components/tinyapp","title":"Components"},{"location":"kap_proposals/kap_0_kadet/#common-components","text":"A library in --search-paths (which now defaults to . and lib/ ) can also be a module that kadet components import. It is loaded using the load_from_search_paths() : kubelib = load_from_search_paths ( \"kubelib\" ) # lib/kubelib/__init__.py def main (): obj = BaseObj () obj . root . example_app_deployment = kubelib . Deployment ( name = \"example-app\" ) return obj","title":"Common components"},{"location":"kap_proposals/kap_1_external_dependencies/","text":"External dependencies This features allows kapitan to fetch files from online repositories/sources during compile and store in a particular target directory. Author: @yoshi-1224 Specification Specify the files to be fetched as follows: parameters : kapitan : dependencies : - type : git | http[s] output_path : <output_path> source : <git/http[s]_url> The output path is the path to save the dependency into. For example, it could be /components/external/manifest.jsonnet . Then, the user can specify the fetched file as a kapitan.compile item along with the locally-created files. Git type may also include ref and subdir parameters as illustrated below: - type : git output_path : <output_path> source : <git_url> subdir : relative/path/in/repository ref : <commit_hash/branch/tag> If the file already exists at output_path , the fetch will be skipped. For fresh fetch of the dependencies, users may add --fetch option as follows: $ kapitan compile --fetch Users can also add the fetch_always: true option to the kapitan.compile in the inventory in order to force fresh fetch of the dependencies every time. Implementation details Dependencies GitPython module (and git executable) for git type requests module for http[s] (optional) tqdm for reporting download progress","title":"External dependencies"},{"location":"kap_proposals/kap_1_external_dependencies/#external-dependencies","text":"This features allows kapitan to fetch files from online repositories/sources during compile and store in a particular target directory. Author: @yoshi-1224","title":"External dependencies"},{"location":"kap_proposals/kap_1_external_dependencies/#specification","text":"Specify the files to be fetched as follows: parameters : kapitan : dependencies : - type : git | http[s] output_path : <output_path> source : <git/http[s]_url> The output path is the path to save the dependency into. For example, it could be /components/external/manifest.jsonnet . Then, the user can specify the fetched file as a kapitan.compile item along with the locally-created files. Git type may also include ref and subdir parameters as illustrated below: - type : git output_path : <output_path> source : <git_url> subdir : relative/path/in/repository ref : <commit_hash/branch/tag> If the file already exists at output_path , the fetch will be skipped. For fresh fetch of the dependencies, users may add --fetch option as follows: $ kapitan compile --fetch Users can also add the fetch_always: true option to the kapitan.compile in the inventory in order to force fresh fetch of the dependencies every time.","title":"Specification"},{"location":"kap_proposals/kap_1_external_dependencies/#implementation-details","text":"","title":"Implementation details"},{"location":"kap_proposals/kap_1_external_dependencies/#dependencies","text":"GitPython module (and git executable) for git type requests module for http[s] (optional) tqdm for reporting download progress","title":"Dependencies"},{"location":"kap_proposals/kap_2_helm_charts_input_type/","text":"Helm Charts Input Type This will allow kapitan, during compilation, to overwrite the values in user-specified helm charts using its inventory by calling the Go & Sprig template libraries. The helm charts can be specified via local path, and users may download the helm chart via external-dependency feature (of http[s] type). Author: @yoshi-1224 Specification This feature basically follows the helm template command available. This will run after the fetching of the external dependencies takes place, such that users can simultaneously specify the fetch as well as the import of a helm chart dependency. Semantics kapitan : compile : - input_type : helm input_path : <path_to_chart_dir> output_path : <output_path> set-file : - <optional_file_path> - ... values_file : <optional_values_file> namespace : <optional_namespace> This mostly maps to the options available to helm template command (refer to here ). Implementation details C-binding between Helm (Go) and Kapitan (Python) will be created. Helm makes use of two template libraries, namely, text/template and Sprig. The code for helm template command will be converted into shared object (.so) using CGo, which exposes C interface that kapitan (i.e. CPython) could use. The source code for helm template command is found here . This file will be modified to 1. remove redundant options 2. expose C-interface for Kapitan Dependencies (possibly) pybindgen","title":"Helm Charts Input Type"},{"location":"kap_proposals/kap_2_helm_charts_input_type/#helm-charts-input-type","text":"This will allow kapitan, during compilation, to overwrite the values in user-specified helm charts using its inventory by calling the Go & Sprig template libraries. The helm charts can be specified via local path, and users may download the helm chart via external-dependency feature (of http[s] type). Author: @yoshi-1224","title":"Helm Charts Input Type"},{"location":"kap_proposals/kap_2_helm_charts_input_type/#specification","text":"This feature basically follows the helm template command available. This will run after the fetching of the external dependencies takes place, such that users can simultaneously specify the fetch as well as the import of a helm chart dependency.","title":"Specification"},{"location":"kap_proposals/kap_2_helm_charts_input_type/#semantics","text":"kapitan : compile : - input_type : helm input_path : <path_to_chart_dir> output_path : <output_path> set-file : - <optional_file_path> - ... values_file : <optional_values_file> namespace : <optional_namespace> This mostly maps to the options available to helm template command (refer to here ).","title":"Semantics"},{"location":"kap_proposals/kap_2_helm_charts_input_type/#implementation-details","text":"C-binding between Helm (Go) and Kapitan (Python) will be created. Helm makes use of two template libraries, namely, text/template and Sprig. The code for helm template command will be converted into shared object (.so) using CGo, which exposes C interface that kapitan (i.e. CPython) could use. The source code for helm template command is found here . This file will be modified to 1. remove redundant options 2. expose C-interface for Kapitan","title":"Implementation details"},{"location":"kap_proposals/kap_2_helm_charts_input_type/#dependencies","text":"(possibly) pybindgen","title":"Dependencies"},{"location":"kap_proposals/kap_3_schema_validation/","text":"Schema Validation (for k8s) If a yaml/json output is to be used as k8s manifest, users may specify its kind and have kapitan validate its structure during kapitan compile . The plan is to have this validation feature extendable to other outputs as well, such as terraform. Author: @yoshi-1224 Specification The following inventory will validate the structure of Kubernetes Service manifest file in . parameters : kapitan : validate : - output_type : kubernetes.service version : 1.6.6 output_path : relative/path/in/target version parameter is optional: if omitted, the version will be set to the stable release of kubernetes (tbc). Implementation The schemas will be downloaded by requests from this repository . Caching of schema will also be implemented. Dependencies jsonschema to validate the output yaml/json against the correct schema","title":"Schema Validation (for k8s)"},{"location":"kap_proposals/kap_3_schema_validation/#schema-validation-for-k8s","text":"If a yaml/json output is to be used as k8s manifest, users may specify its kind and have kapitan validate its structure during kapitan compile . The plan is to have this validation feature extendable to other outputs as well, such as terraform. Author: @yoshi-1224","title":"Schema Validation (for k8s)"},{"location":"kap_proposals/kap_3_schema_validation/#specification","text":"The following inventory will validate the structure of Kubernetes Service manifest file in . parameters : kapitan : validate : - output_type : kubernetes.service version : 1.6.6 output_path : relative/path/in/target version parameter is optional: if omitted, the version will be set to the stable release of kubernetes (tbc).","title":"Specification"},{"location":"kap_proposals/kap_3_schema_validation/#implementation","text":"The schemas will be downloaded by requests from this repository . Caching of schema will also be implemented.","title":"Implementation"},{"location":"kap_proposals/kap_3_schema_validation/#dependencies","text":"jsonschema to validate the output yaml/json against the correct schema","title":"Dependencies"},{"location":"kap_proposals/kap_4_standalone_executable/","text":"Standalone Kapitan Executable Create a portable (i.e. static) kapitan binary for users. This executable will be made available for each release on Github. The target/tested platform is Debian 9 (possibly Windows to be supported in the future). Criteria: - speed of the resulting binary - size of the resulting binary - portability of the binary (single-file executable or has an accompanying folder) - cross-platform - actively maintained - supports Python 3.6, 3.7 Author: @yoshi-1224 Tools to be explored (tentative first-choice) Pyinstaller (Alternative) nuitka (also part of GSoC 2019. It might soon support single-file executable output ).","title":"Standalone Kapitan Executable"},{"location":"kap_proposals/kap_4_standalone_executable/#standalone-kapitan-executable","text":"Create a portable (i.e. static) kapitan binary for users. This executable will be made available for each release on Github. The target/tested platform is Debian 9 (possibly Windows to be supported in the future). Criteria: - speed of the resulting binary - size of the resulting binary - portability of the binary (single-file executable or has an accompanying folder) - cross-platform - actively maintained - supports Python 3.6, 3.7 Author: @yoshi-1224","title":"Standalone Kapitan Executable"},{"location":"kap_proposals/kap_4_standalone_executable/#tools-to-be-explored","text":"(tentative first-choice) Pyinstaller (Alternative) nuitka (also part of GSoC 2019. It might soon support single-file executable output ).","title":"Tools to be explored"},{"location":"kap_proposals/kap_5_ref_types_redesign/","text":"Ref Types Redesign Redesign Kapitan Secrets and rename them as References or Ref . Breaking changes: $ kapitan secrets is replaced with $ kapitan refs the default secrets directory ./secrets/ changes to ./refs/ the --secrets-path flag changes to --refs-path ref ref type is renamed to base64 e.g. ?{ref:some/ref} into ?{base64:some/ref} Status: In progress Author: @ramaro Proposal Rename Secrets into Ref (or References ) to improve consistency and meaning of the backend types by removing the ref backend and introducting new backends: Type Description Encrypted? Compiles To gpg GnuPG Yes hashed tag gkms Google KMS Yes hashed tag awskms Amazon KMS Yes hashed tag base64 base64 No hashed tag plain plain text No plain text The type value will now need to be representative of the way a reference is stored via its backend. A new plain backend type is introduced and will compile into revealed state instead of a hashed tag. A new base64 backend type will store a base64 encoded value as the backend suggests (replacing the old badly named ref backend). The command line for secrets will be instead: $ kapitan refs --write gpg:my/secret1 ... $ kapitan refs --write base64:my/file ... $ kapitan refs --write plain:my/info ... plain backend The plain backend type will allow referring to external state by updating refs programmatically (e.g. in your pipeline) For example, one can update the value of an environment variable and use ?{plain:my/user} as a reference in a template: $ echo $USER | kapitan refs --write plain:my/user -f - Or update a docker image value as ref ?{plain:images/dev/envoy} : $ echo 'envoyproxy/envoy:v1.10.0' | kapitan refs --write plain:images/dev/envoy -f - These references will be compiled into their values instead of hashed tags. base64 backend The base64 backend type will function as the original ref type. Except that this time, the name is representative of what is actually happening :) Refs path Refs will be stored by default in the ./refs path set by --refs-path replacing the --secrets-path flag. Background Kapitan Secrets Kapitan Secrets allow referring to restricted information (passwords, private keys, etc...) in templates while also securely storing them. On compile, secret tags are updated into hashed tags which validate and instruct Kapitan how to reveal tags into decrypted or encoded information. Kapitan Secrets example The following command creates a GPG encrypted secret with the contents of file.txt for recipient ramaro@google.com to read: $ kapitan secrets --write gpg:my/secret1 -f file.txt --recipients ramaro@google.com This secret can be referred to in a jsonnet compoment: { \"type\" : \"app\" , \"name\" : \"test_app\" , \"username\" : \"user_one\" , \"password\" : \"?{gpg:my/secret1}\" } When this compoment is compiled, it looks like (note the hashed tag): type : app name : test_app username : user_one password : ?{gpg:my/secret1:deadbeef} A user with the required permissions can reveal the compiled component: $ kapitan secrets --reveal -f compiled/mytarget/manifests/component.yml type: app name: test_app username: user_one password: secret_content_of_file.txt Secret Backend Comparison Kapitan today offers multiple secret backends: Type Description Encrypted? Compiles To gpg GnuPG Yes hashed tag gkms Google KMS Yes hashed tag awskms Amazon KMS Yes hashed tag ref base64 No hashed tag However, not all backends are encrypted - this is not consistent! The ref type is not encrypted as its purpose is to allow getting started with the Kapitan Secrets workflow without the need of setting up the encryption backends tooling (gpg, gcloud, boto, etc...)","title":"Ref Types Redesign"},{"location":"kap_proposals/kap_5_ref_types_redesign/#ref-types-redesign","text":"Redesign Kapitan Secrets and rename them as References or Ref . Breaking changes: $ kapitan secrets is replaced with $ kapitan refs the default secrets directory ./secrets/ changes to ./refs/ the --secrets-path flag changes to --refs-path ref ref type is renamed to base64 e.g. ?{ref:some/ref} into ?{base64:some/ref} Status: In progress Author: @ramaro","title":"Ref Types Redesign"},{"location":"kap_proposals/kap_5_ref_types_redesign/#proposal","text":"Rename Secrets into Ref (or References ) to improve consistency and meaning of the backend types by removing the ref backend and introducting new backends: Type Description Encrypted? Compiles To gpg GnuPG Yes hashed tag gkms Google KMS Yes hashed tag awskms Amazon KMS Yes hashed tag base64 base64 No hashed tag plain plain text No plain text The type value will now need to be representative of the way a reference is stored via its backend. A new plain backend type is introduced and will compile into revealed state instead of a hashed tag. A new base64 backend type will store a base64 encoded value as the backend suggests (replacing the old badly named ref backend). The command line for secrets will be instead: $ kapitan refs --write gpg:my/secret1 ... $ kapitan refs --write base64:my/file ... $ kapitan refs --write plain:my/info ...","title":"Proposal"},{"location":"kap_proposals/kap_5_ref_types_redesign/#plain-backend","text":"The plain backend type will allow referring to external state by updating refs programmatically (e.g. in your pipeline) For example, one can update the value of an environment variable and use ?{plain:my/user} as a reference in a template: $ echo $USER | kapitan refs --write plain:my/user -f - Or update a docker image value as ref ?{plain:images/dev/envoy} : $ echo 'envoyproxy/envoy:v1.10.0' | kapitan refs --write plain:images/dev/envoy -f - These references will be compiled into their values instead of hashed tags.","title":"plain backend"},{"location":"kap_proposals/kap_5_ref_types_redesign/#base64-backend","text":"The base64 backend type will function as the original ref type. Except that this time, the name is representative of what is actually happening :)","title":"base64 backend"},{"location":"kap_proposals/kap_5_ref_types_redesign/#refs-path","text":"Refs will be stored by default in the ./refs path set by --refs-path replacing the --secrets-path flag.","title":"Refs path"},{"location":"kap_proposals/kap_5_ref_types_redesign/#background","text":"","title":"Background"},{"location":"kap_proposals/kap_5_ref_types_redesign/#kapitan-secrets","text":"Kapitan Secrets allow referring to restricted information (passwords, private keys, etc...) in templates while also securely storing them. On compile, secret tags are updated into hashed tags which validate and instruct Kapitan how to reveal tags into decrypted or encoded information.","title":"Kapitan Secrets"},{"location":"kap_proposals/kap_5_ref_types_redesign/#kapitan-secrets-example","text":"The following command creates a GPG encrypted secret with the contents of file.txt for recipient ramaro@google.com to read: $ kapitan secrets --write gpg:my/secret1 -f file.txt --recipients ramaro@google.com This secret can be referred to in a jsonnet compoment: { \"type\" : \"app\" , \"name\" : \"test_app\" , \"username\" : \"user_one\" , \"password\" : \"?{gpg:my/secret1}\" } When this compoment is compiled, it looks like (note the hashed tag): type : app name : test_app username : user_one password : ?{gpg:my/secret1:deadbeef} A user with the required permissions can reveal the compiled component: $ kapitan secrets --reveal -f compiled/mytarget/manifests/component.yml type: app name: test_app username: user_one password: secret_content_of_file.txt","title":"Kapitan Secrets example"},{"location":"kap_proposals/kap_5_ref_types_redesign/#secret-backend-comparison","text":"Kapitan today offers multiple secret backends: Type Description Encrypted? Compiles To gpg GnuPG Yes hashed tag gkms Google KMS Yes hashed tag awskms Amazon KMS Yes hashed tag ref base64 No hashed tag However, not all backends are encrypted - this is not consistent! The ref type is not encrypted as its purpose is to allow getting started with the Kapitan Secrets workflow without the need of setting up the encryption backends tooling (gpg, gcloud, boto, etc...)","title":"Secret Backend Comparison"},{"location":"kap_proposals/kap_6_hashicorp_vault/","text":"Hashicorp Vault This feature allows the user to fetch secrets from Hashicorp Vault , with the new secret backend keyword 'vaultkv'. Author: @vaibahvk @daminisatya Specification The following variables need to be exported to the environment(depending on authentication used) where you will run kapitan refs --reveal in order to authenticate to your HashiCorp Vault instance: * VAULT_ADDR: URL for vault * VAULT_SKIP_VERIFY=true: if set, do not verify presented TLS certificate before communicating with Vault server. Setting this variable is not recommended except during testing * VAULT_TOKEN: token for vault or file (~/.vault-tokens) * VAULT_ROLE_ID: required by approle * VAULT_SECRET_ID: required by approle * VAULT_USERNAME: username to login to vault * VAULT_PASSWORD: password to login to vault * VAULT_CLIENT_KEY: the path to an unencrypted PEM-encoded private key matching the client certificate * VAULT_CLIENT_CERT: the path to a PEM-encoded client certificate for TLS authentication to the Vault server * VAULT_CACERT: the path to a PEM-encoded CA cert file to use to verify the Vault server TLS certificate * VAULT_CAPATH: the path to a directory of PEM-encoded CA cert files to verify the Vault server TLS certificate * VAULT_NAMESPACE: specify the Vault Namespace, if you have one Considering a key-value pair like my_key : my_secret ( in our case let\u2019s store hello : batman inside the vault ) in the path secret/foo in a kv-v2(KV version 2) secret engine on the vault server, to use this as a secret either follow: $ echo \"foo:hello\" > somefile.txt $ kapitan refs --write vaultkv:path/to/secret_inside_kapitan --file somefile.txt --target dev-sea or in a single line $ echo \"foo:hello\" | kapitan refs --write vaultkv:path/to/secret_inside_kapitan -t dev-sea -f - The entire string \"foo:hello\" is base64 encoded and stored in the secret_inside_kapitan. Now secret_inside_kapitan contains the following data : Zm9vOmhlbGxvCg== encoding : original type : vaultkv vault_params : auth : token Encoding tells the type of data given to kapitan, if it is original then after decoding base64 we'll get the original secret and if it is base64 then after decoding once we still have a base64 encoded secret and have to decode again. Parameters in the secret file are collected from the inventory of the target we gave from CLI --target dev-sea . If target isn't provided then kapitan will identify the variables from the environment, but providing auth is necessary as a key inside target parameters like the one shown: parameters : kapitan : secrets : vaultkv : auth : userpass engine : kv-v2 mount : team-alpha-secret VAULT_ADDR : http://127.0.0.1:8200 VAULT_NAMESPACE : CICD-alpha VAULT_SKIP_VERIFY : false VAULT_CLIENT_KEY : /path/to/key VAULT_CLIENT_CERT : /path/to/cert Environment variables that can be defined in kapitan inventory are VAULT_ADDR , VAULT_NAMESPACE , VAULT_SKIP_VERIFY , VAULT_CLIENT_CERT , VAULT_CLIENT_KEY , VAULT_CAPATH & VAULT_CACERT . Extra parameters that can be defined in inventory are: * auth : specify which authentication method to use like token , userpass , ldap , github & approle * mount : specify the mount point of key's path. e.g if path= alpha-secret/foo/bar then mount: alpha-secret (default secret ) * engine : secret engine used, either kv-v2 or kv (default kv-v2 ) Environment variables cannot be defined in inventory are VAULT_TOKEN , VAULT_USERNAME , VAULT_PASSWORD , VAULT_ROLE_ID , VAULT_SECRET_ID . This makes the secret_inside_kapitan file accessible throughout the inventory, where we can use the secret whenever necessary like ?{vaultkv:path/to/secret_inside_kapitan} Following is the example file having a secret and pointing to the vault ?{vaultkv:path/to/secret_inside_kapitan} parameters : releases : cod : latest cod : image : alledm/cod:${cod:release} release : ${releases:cod} replicas : ${replicas} args : - --verbose=${verbose} - --password=?{vaultkv:path/to/secret_inside_kapitan} when ?{vaultkv:path/to/secret_inside_kapitan} is compiled, it will look same with an 8 character prefix of sha256 hash added at the end like: kind : Deployment metadata : name : cod namespace : dev-sea spec : replicas : 1 template : metadata : labels : app : cod spec : containers : - args : - --verbose=True - --password=?{vaultkv:path/to/secret_inside_kapitan:57d6f9b7} image : alledm/cod:v2.0.0 name : cod Only the user with the required tokens/permissions can reveal the secrets. Please note that the roles and permissions will be handled at the Vault level. We need not worry about it within Kapitan. Use the following command to reveal the secrets: $ kapitan refs --reveal -f compile/file/containing/secret Following is the result of the cod-deployment.md file after Kapitan reveal. kind : Deployment metadata : name : cod namespace : dev-sea spec : replicas : 1 template : metadata : labels : app : cod spec : containers : - args : - --verbose=True - --password=batman image : alledm/cod:v2.0.0 name : cod Dependencies hvac is a python client for Hashicorp Vault","title":"Hashicorp Vault"},{"location":"kap_proposals/kap_6_hashicorp_vault/#hashicorp-vault","text":"This feature allows the user to fetch secrets from Hashicorp Vault , with the new secret backend keyword 'vaultkv'. Author: @vaibahvk @daminisatya","title":"Hashicorp Vault"},{"location":"kap_proposals/kap_6_hashicorp_vault/#specification","text":"The following variables need to be exported to the environment(depending on authentication used) where you will run kapitan refs --reveal in order to authenticate to your HashiCorp Vault instance: * VAULT_ADDR: URL for vault * VAULT_SKIP_VERIFY=true: if set, do not verify presented TLS certificate before communicating with Vault server. Setting this variable is not recommended except during testing * VAULT_TOKEN: token for vault or file (~/.vault-tokens) * VAULT_ROLE_ID: required by approle * VAULT_SECRET_ID: required by approle * VAULT_USERNAME: username to login to vault * VAULT_PASSWORD: password to login to vault * VAULT_CLIENT_KEY: the path to an unencrypted PEM-encoded private key matching the client certificate * VAULT_CLIENT_CERT: the path to a PEM-encoded client certificate for TLS authentication to the Vault server * VAULT_CACERT: the path to a PEM-encoded CA cert file to use to verify the Vault server TLS certificate * VAULT_CAPATH: the path to a directory of PEM-encoded CA cert files to verify the Vault server TLS certificate * VAULT_NAMESPACE: specify the Vault Namespace, if you have one Considering a key-value pair like my_key : my_secret ( in our case let\u2019s store hello : batman inside the vault ) in the path secret/foo in a kv-v2(KV version 2) secret engine on the vault server, to use this as a secret either follow: $ echo \"foo:hello\" > somefile.txt $ kapitan refs --write vaultkv:path/to/secret_inside_kapitan --file somefile.txt --target dev-sea or in a single line $ echo \"foo:hello\" | kapitan refs --write vaultkv:path/to/secret_inside_kapitan -t dev-sea -f - The entire string \"foo:hello\" is base64 encoded and stored in the secret_inside_kapitan. Now secret_inside_kapitan contains the following data : Zm9vOmhlbGxvCg== encoding : original type : vaultkv vault_params : auth : token Encoding tells the type of data given to kapitan, if it is original then after decoding base64 we'll get the original secret and if it is base64 then after decoding once we still have a base64 encoded secret and have to decode again. Parameters in the secret file are collected from the inventory of the target we gave from CLI --target dev-sea . If target isn't provided then kapitan will identify the variables from the environment, but providing auth is necessary as a key inside target parameters like the one shown: parameters : kapitan : secrets : vaultkv : auth : userpass engine : kv-v2 mount : team-alpha-secret VAULT_ADDR : http://127.0.0.1:8200 VAULT_NAMESPACE : CICD-alpha VAULT_SKIP_VERIFY : false VAULT_CLIENT_KEY : /path/to/key VAULT_CLIENT_CERT : /path/to/cert Environment variables that can be defined in kapitan inventory are VAULT_ADDR , VAULT_NAMESPACE , VAULT_SKIP_VERIFY , VAULT_CLIENT_CERT , VAULT_CLIENT_KEY , VAULT_CAPATH & VAULT_CACERT . Extra parameters that can be defined in inventory are: * auth : specify which authentication method to use like token , userpass , ldap , github & approle * mount : specify the mount point of key's path. e.g if path= alpha-secret/foo/bar then mount: alpha-secret (default secret ) * engine : secret engine used, either kv-v2 or kv (default kv-v2 ) Environment variables cannot be defined in inventory are VAULT_TOKEN , VAULT_USERNAME , VAULT_PASSWORD , VAULT_ROLE_ID , VAULT_SECRET_ID . This makes the secret_inside_kapitan file accessible throughout the inventory, where we can use the secret whenever necessary like ?{vaultkv:path/to/secret_inside_kapitan} Following is the example file having a secret and pointing to the vault ?{vaultkv:path/to/secret_inside_kapitan} parameters : releases : cod : latest cod : image : alledm/cod:${cod:release} release : ${releases:cod} replicas : ${replicas} args : - --verbose=${verbose} - --password=?{vaultkv:path/to/secret_inside_kapitan} when ?{vaultkv:path/to/secret_inside_kapitan} is compiled, it will look same with an 8 character prefix of sha256 hash added at the end like: kind : Deployment metadata : name : cod namespace : dev-sea spec : replicas : 1 template : metadata : labels : app : cod spec : containers : - args : - --verbose=True - --password=?{vaultkv:path/to/secret_inside_kapitan:57d6f9b7} image : alledm/cod:v2.0.0 name : cod Only the user with the required tokens/permissions can reveal the secrets. Please note that the roles and permissions will be handled at the Vault level. We need not worry about it within Kapitan. Use the following command to reveal the secrets: $ kapitan refs --reveal -f compile/file/containing/secret Following is the result of the cod-deployment.md file after Kapitan reveal. kind : Deployment metadata : name : cod namespace : dev-sea spec : replicas : 1 template : metadata : labels : app : cod spec : containers : - args : - --verbose=True - --password=batman image : alledm/cod:v2.0.0 name : cod","title":"Specification"},{"location":"kap_proposals/kap_6_hashicorp_vault/#dependencies","text":"hvac is a python client for Hashicorp Vault","title":"Dependencies"},{"location":"kap_proposals/kap_7_remote_inventory/","text":"Remote Inventory Federation This feature would add the ability to Kapitan to fetch parts of the inventory from remote locations (https/git). This would allow users to combine different inventories from different sources and build modular infrastructure reusable across various repos. Author: @alpharoy14 Specification The configuration and declaration of remote inventories would be done in the inventory files. The file specifications are as follows: parameters : kapitan : inventory : - type : <inventory_type> #git\\https source : <source_of_inventory> output_path : <relative_output_path> On executing the $ kapitan compile command, first the remote inventories will be fetched followed by fetching of external dependencies and finally reclass will be used to compile. Copying inventory files to the output location The output path is the path to save the inventory items into. The path is relative to the inventory/ directory. For example, it could be /classes/ . The contents of the fetched inventory will be recursively copied. The fetched inventory files will be cached in the .kapitan_cache directory. Force fetching While fetching, the output path will be recursively checked to see if it contains any file with the same name. If so, kapitan will skip fetching it. To overwrite the files with the newly downloaded inventory items, we can add the --force flag to the compile command, as shown below. $ kapitan compile --force URL type The URL type can be either git or http(s). Depending on the URL type, the configuration file may have additional arguments. E.g Git type may also include aditional ref parameter as illustrated below: inventory : - type : git #git\\https source : <source_of_inventory> output_path : <output_path> ref : <commit_hash/branch/tag> Implementation details TODO Dependencies GitPython module (and git executable) for git type requests module for http[s]","title":"Remote Inventory Federation"},{"location":"kap_proposals/kap_7_remote_inventory/#remote-inventory-federation","text":"This feature would add the ability to Kapitan to fetch parts of the inventory from remote locations (https/git). This would allow users to combine different inventories from different sources and build modular infrastructure reusable across various repos. Author: @alpharoy14","title":"Remote Inventory Federation"},{"location":"kap_proposals/kap_7_remote_inventory/#specification","text":"The configuration and declaration of remote inventories would be done in the inventory files. The file specifications are as follows: parameters : kapitan : inventory : - type : <inventory_type> #git\\https source : <source_of_inventory> output_path : <relative_output_path> On executing the $ kapitan compile command, first the remote inventories will be fetched followed by fetching of external dependencies and finally reclass will be used to compile.","title":"Specification"},{"location":"kap_proposals/kap_7_remote_inventory/#copying-inventory-files-to-the-output-location","text":"The output path is the path to save the inventory items into. The path is relative to the inventory/ directory. For example, it could be /classes/ . The contents of the fetched inventory will be recursively copied. The fetched inventory files will be cached in the .kapitan_cache directory.","title":"Copying inventory files to the output location"},{"location":"kap_proposals/kap_7_remote_inventory/#force-fetching","text":"While fetching, the output path will be recursively checked to see if it contains any file with the same name. If so, kapitan will skip fetching it. To overwrite the files with the newly downloaded inventory items, we can add the --force flag to the compile command, as shown below. $ kapitan compile --force","title":"Force fetching"},{"location":"kap_proposals/kap_7_remote_inventory/#url-type","text":"The URL type can be either git or http(s). Depending on the URL type, the configuration file may have additional arguments. E.g Git type may also include aditional ref parameter as illustrated below: inventory : - type : git #git\\https source : <source_of_inventory> output_path : <output_path> ref : <commit_hash/branch/tag>","title":"URL type"},{"location":"kap_proposals/kap_7_remote_inventory/#implementation-details","text":"TODO","title":"Implementation details"},{"location":"kap_proposals/kap_7_remote_inventory/#dependencies","text":"GitPython module (and git executable) for git type requests module for http[s]","title":"Dependencies"}]}