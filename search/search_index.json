{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kapitan: Generic templated configuration management for Kubernetes, Terraform and other things Kapitan is a tool to manage complex deployments using jsonnet, kadet (alpha) and jinja2. Use Kapitan to manage your Kubernetes manifests, your documentation, your Terraform configuration or even simplify your scripts. Community Main Blog, articles and tutorials : Kapitan Blog Slack #kapitan Website https://kapitan.dev London Meetup Group London Kapitan Meetup How is it different from Helm ? Please look at our FAQ ! Main Features Use the Inventory as the single source of truth to tie together deployments, resources and documentation. based on reclass Use Jsonnet or Kadet (alpha) to create json/yaml based configurations (e.g. Kubernetes, Terraform); Use Jinja2 to create text based templates for scripts and documentation; Manage secrets with GPG, AWS KMS or gCloud KMS and define who can access them, without compromising collaboration with other users. Create dynamically generated documentation about a single deployment (i.e. ad-hoc instructions) or all deployments at once (i.e. global state of deployments) Quickstart Docker (recommended) docker run -t --rm -v $(pwd):/src:delegated deepmind/kapitan -h On Linux you can add -u $(id -u) to docker run to preserve file permissions. For CI/CD usage, check out CI.md Pip Kapitan needs Python 3.6. Install Python 3.6: Linux: sudo apt-get update && sudo apt-get install -y python3.6-dev python3-pip python3-yaml Mac: brew install python3 libyaml Install Kapitan: User ( $HOME/.local/lib/python3.6/bin on Linux or $HOME/Library/Python/3.6/bin on macOS): pip3 install --user --upgrade kapitan System-wide (not recommended): sudo pip3 install --upgrade kapitan Example The example below compiles 2 targets inside the examples/kubernetes folder. Each target represents a different namespace in a minikube cluster. These targets generate the following resources: Kubernetes Namespace for the targets Kubernetes StatefulSet for ElasticSearch Master node Kubernetes StatefulSet for ElasticSearch Client node Kubernetes StatefulSet for ElasticSearch Data node Kubernetes Service to expose ElasticSearch discovery port Kubernetes Service to expose ElasticSearch service port Kubernetes StatefulSet for MySQL Kubernetes Service to expose MySQL service port Kubernetes Secret for MySQL credentials Scripts to configure kubectl context to control the targets and helpers to apply/delete objects. Documentation $ cd examples/kubernetes $ kapitan compile Compiled minikube-mysql Compiled minikube-es Documentation Getting Started Kapitan Overview Understanding inventory Compile operation Kapitan features Secret management Manifest validation External dependencies management Miscellaneous Usage Continuous Integration Set up kapitan on older Python systems Credits Jsonnet Jinja2 reclass FAQ Why do we prefer Kapitan to Helm ? Before developing Kapitan, we turned to Helm in an attempt to improve our old Jinja based templating system. We quickly discovered that Helm did not fit well with our workflow, for the following reasons (which were true at the time of the evaluation): * Helm uses Go templates to define Kubernetes (yaml) manifests. We were already unsatisfied by using Jinja and we did not see a huge improvement from our previous system, the main reason being: YAML files are not suitable to be managed by text templating frameworks. * Helm does not have a solution for sharing values across charts, if not through subcharts. We wanted to be able to have one single place to define all values for all our templates. Sharing data between charts felt awkward and complicated. * Helm is component/chart based. We wanted to have something that would treat all our deployments as a whole. * We did not fancy the dependency on the tiller. In short, we feel Helm is trying to be apt-get for Kubernetes charts, while we are trying to take you further than that. Why do I need Kapitan? With Kapitan, we worked to de-compose several problems that most of the other solutions are treating as one. 1) Kubernetes manifests : We like the jsonnet approach of using json as the working language. Jsonnet allows us to use inheritance and composition, and hide complexity at higher levels. 2) Configuration files : Most solutions will assume this problem is solved somewhere else. We feel Jinja (or your template engine of choice) have the upper hand here. 3) Hierarchical inventory : This is the feature that sets us apart from other solutions. We use the inventory (based on reclass ) to define variables and properties that can be reused across different projects/deployments. This allows us to limit repetition, but also to define a nicer interface with developers (or CI tools) which will only need to understand YAML to operate changes. 4) Secrets : We manage most of our secrets with kapitan using the GPG, Google Cloud KMS and AWS KMS integrations. Keys can be setup per class, per target or shared so you can easily and flexibly manage access per environment. They can also be dynamically generated on compilation, if you don't feel like generating random passwords or RSA private keys, and they can be referenced in the inventory like any other variables. We have plans to support other providers such as Vault, in addition to GPG, Google Cloud KMS and AWS KMS. 5) Canned scripts : We treat scripts as text templates, so that we can craft pre-canned scripts for the specific target we are working on. This can be used for instance to define scripts that setup clusters, contexts or allow running kubectl with all the correct settings. Most other solutions require you to define contexts and call kubectl with the correct settings. We take care of that for you. Less ambiguity, less mistakes. 6) Documentation : We also use templates to create documentation for the targets we deploy. Documentation lived alongside everything else and it is treated as a first class citizen. We feel most other solutions are pushing the limits of their capacity in order to provide for the above problems. Helm treats everything as a text template, while jsonnet tries to do everything as json. We believe that these approaches can be blended in a powerful new way, glued together by the inventory. Related projects sublime-jsonnet-syntax - Jsonnet syntax highlighting for Sublime Text language-jsonnet - Jsonnet syntax highlighting for Atom vim-jsonnet - Jsonnet plugin for Vim (requires a vim plugin manager)","title":"Home"},{"location":"#kapitan-generic-templated-configuration-management-for-kubernetes-terraform-and-other-things","text":"Kapitan is a tool to manage complex deployments using jsonnet, kadet (alpha) and jinja2. Use Kapitan to manage your Kubernetes manifests, your documentation, your Terraform configuration or even simplify your scripts.","title":"Kapitan: Generic templated configuration management for Kubernetes, Terraform and other things"},{"location":"#community","text":"Main Blog, articles and tutorials : Kapitan Blog Slack #kapitan Website https://kapitan.dev London Meetup Group London Kapitan Meetup How is it different from Helm ? Please look at our FAQ !","title":"Community"},{"location":"#main-features","text":"Use the Inventory as the single source of truth to tie together deployments, resources and documentation. based on reclass Use Jsonnet or Kadet (alpha) to create json/yaml based configurations (e.g. Kubernetes, Terraform); Use Jinja2 to create text based templates for scripts and documentation; Manage secrets with GPG, AWS KMS or gCloud KMS and define who can access them, without compromising collaboration with other users. Create dynamically generated documentation about a single deployment (i.e. ad-hoc instructions) or all deployments at once (i.e. global state of deployments)","title":"Main Features"},{"location":"#quickstart","text":"","title":"Quickstart"},{"location":"#docker-recommended","text":"docker run -t --rm -v $(pwd):/src:delegated deepmind/kapitan -h On Linux you can add -u $(id -u) to docker run to preserve file permissions. For CI/CD usage, check out CI.md","title":"Docker (recommended)"},{"location":"#pip","text":"Kapitan needs Python 3.6. Install Python 3.6: Linux: sudo apt-get update && sudo apt-get install -y python3.6-dev python3-pip python3-yaml Mac: brew install python3 libyaml Install Kapitan: User ( $HOME/.local/lib/python3.6/bin on Linux or $HOME/Library/Python/3.6/bin on macOS): pip3 install --user --upgrade kapitan System-wide (not recommended): sudo pip3 install --upgrade kapitan","title":"Pip"},{"location":"#example","text":"The example below compiles 2 targets inside the examples/kubernetes folder. Each target represents a different namespace in a minikube cluster. These targets generate the following resources: Kubernetes Namespace for the targets Kubernetes StatefulSet for ElasticSearch Master node Kubernetes StatefulSet for ElasticSearch Client node Kubernetes StatefulSet for ElasticSearch Data node Kubernetes Service to expose ElasticSearch discovery port Kubernetes Service to expose ElasticSearch service port Kubernetes StatefulSet for MySQL Kubernetes Service to expose MySQL service port Kubernetes Secret for MySQL credentials Scripts to configure kubectl context to control the targets and helpers to apply/delete objects. Documentation $ cd examples/kubernetes $ kapitan compile Compiled minikube-mysql Compiled minikube-es","title":"Example"},{"location":"#documentation","text":"","title":"Documentation"},{"location":"#getting-started","text":"Kapitan Overview Understanding inventory Compile operation","title":"Getting Started"},{"location":"#kapitan-features","text":"Secret management Manifest validation External dependencies management","title":"Kapitan features"},{"location":"#miscellaneous","text":"Usage Continuous Integration Set up kapitan on older Python systems","title":"Miscellaneous"},{"location":"#credits","text":"Jsonnet Jinja2 reclass","title":"Credits"},{"location":"#faq","text":"","title":"FAQ"},{"location":"#why-do-we-prefer-kapitan-to-helm","text":"Before developing Kapitan, we turned to Helm in an attempt to improve our old Jinja based templating system. We quickly discovered that Helm did not fit well with our workflow, for the following reasons (which were true at the time of the evaluation): * Helm uses Go templates to define Kubernetes (yaml) manifests. We were already unsatisfied by using Jinja and we did not see a huge improvement from our previous system, the main reason being: YAML files are not suitable to be managed by text templating frameworks. * Helm does not have a solution for sharing values across charts, if not through subcharts. We wanted to be able to have one single place to define all values for all our templates. Sharing data between charts felt awkward and complicated. * Helm is component/chart based. We wanted to have something that would treat all our deployments as a whole. * We did not fancy the dependency on the tiller. In short, we feel Helm is trying to be apt-get for Kubernetes charts, while we are trying to take you further than that.","title":"Why do we prefer Kapitan to Helm?"},{"location":"#why-do-i-need-kapitan","text":"With Kapitan, we worked to de-compose several problems that most of the other solutions are treating as one. 1) Kubernetes manifests : We like the jsonnet approach of using json as the working language. Jsonnet allows us to use inheritance and composition, and hide complexity at higher levels. 2) Configuration files : Most solutions will assume this problem is solved somewhere else. We feel Jinja (or your template engine of choice) have the upper hand here. 3) Hierarchical inventory : This is the feature that sets us apart from other solutions. We use the inventory (based on reclass ) to define variables and properties that can be reused across different projects/deployments. This allows us to limit repetition, but also to define a nicer interface with developers (or CI tools) which will only need to understand YAML to operate changes. 4) Secrets : We manage most of our secrets with kapitan using the GPG, Google Cloud KMS and AWS KMS integrations. Keys can be setup per class, per target or shared so you can easily and flexibly manage access per environment. They can also be dynamically generated on compilation, if you don't feel like generating random passwords or RSA private keys, and they can be referenced in the inventory like any other variables. We have plans to support other providers such as Vault, in addition to GPG, Google Cloud KMS and AWS KMS. 5) Canned scripts : We treat scripts as text templates, so that we can craft pre-canned scripts for the specific target we are working on. This can be used for instance to define scripts that setup clusters, contexts or allow running kubectl with all the correct settings. Most other solutions require you to define contexts and call kubectl with the correct settings. We take care of that for you. Less ambiguity, less mistakes. 6) Documentation : We also use templates to create documentation for the targets we deploy. Documentation lived alongside everything else and it is treated as a first class citizen. We feel most other solutions are pushing the limits of their capacity in order to provide for the above problems. Helm treats everything as a text template, while jsonnet tries to do everything as json. We believe that these approaches can be blended in a powerful new way, glued together by the inventory.","title":"Why do I need Kapitan?"},{"location":"#related-projects","text":"sublime-jsonnet-syntax - Jsonnet syntax highlighting for Sublime Text language-jsonnet - Jsonnet syntax highlighting for Atom vim-jsonnet - Jsonnet plugin for Vim (requires a vim plugin manager)","title":"Related projects"},{"location":"CI/","text":"Kapitan: CI/CD usage The Docker image ( deepmind/kapitan:ci ) ( Dockerfile ) comes pre-packaged with gcloud , gsutil , bq , kubectl , terraform , promtool and kapitan . Example workflow - Deploy to GKE The following commands are run using the deepmind/kapitan:ci Docker image. Compile: kapitan compile Compiled app ( 2 .23s ) Setup gcloud and GKE credentials: echo \" $GCP_SA_KEY_FROM_CI_SECRETS \" > service_account_key.json gcloud auth activate-service-account --key-file service_account_key.json gcloud container clusters get-credentials CLUSTER --zone ZONE --project GCP_PROJECT_ID Setup kubectl: kubectl config set-context CLUSTER_CONTEXT --cluster CLUSTER --user USER --namespace NAMESPACE kubectl config use-context CLUSTER_CONTEXT Deploy: kubectl apply -f compiled/app/manifests/","title":"Continuous Integration"},{"location":"CI/#kapitan-cicd-usage","text":"The Docker image ( deepmind/kapitan:ci ) ( Dockerfile ) comes pre-packaged with gcloud , gsutil , bq , kubectl , terraform , promtool and kapitan .","title":"Kapitan: CI/CD usage"},{"location":"CI/#example-workflow-deploy-to-gke","text":"The following commands are run using the deepmind/kapitan:ci Docker image. Compile: kapitan compile Compiled app ( 2 .23s ) Setup gcloud and GKE credentials: echo \" $GCP_SA_KEY_FROM_CI_SECRETS \" > service_account_key.json gcloud auth activate-service-account --key-file service_account_key.json gcloud container clusters get-credentials CLUSTER --zone ZONE --project GCP_PROJECT_ID Setup kubectl: kubectl config set-context CLUSTER_CONTEXT --cluster CLUSTER --user USER --namespace NAMESPACE kubectl config use-context CLUSTER_CONTEXT Deploy: kubectl apply -f compiled/app/manifests/","title":"Example workflow - Deploy to GKE"},{"location":"compile/","text":"Kapitan compile Note: make sure to read up on inventory before moving on. Specifying inputs and outputs Input types can be specified in the inventory under kapitan.compile in the following format: parameters : kapitan : compile : - output_path : <output_path_in_target_dir> input_type : jinja | jsonnet | kadet | helm input_paths : - path/to/input/dir/or/file output_type : <output_type_specific_to_input_type> Supported input types Kapitan supports the following input template types: jinja jsonnet kadet (alpha) helm (optional) jinja This renders jinja2 templates, typically stored in templates/ directory, such as README, scripts and config files. Refer to jinja2 docs to understand how the template engine works. For Jinja2, input_paths can be either a file or a directory: in case of a directory, all the templates in the directory will be rendered and outputted to output_path . Supported output types : N/A (no need to specify output_type ) Using the inventory in jinja2 Jinja2 types will pass the \"inventory\" and whatever target vars as context keys in your template. This snippet renders the same java_opts for the elasticsearch data role: java_opts for elasticsearch data role are: {{ inventory.parameters.elasticsearch.roles.data.java_opts }} Jinja2 custom filters We support the following custom filters for use in Jinja2 templates: sha256 - SHA256 hashing of text e.g. {{ text | sha256 }} yaml - Dump text as YAML e.g. {{ text | yaml }} b64encode - base64 encode text e.g. {{ text | b64encode }} b64decode - base64 decode text e.g. {{ text | b64decode }} fileglob - return list of matched regular files for glob e.g. {{ ./path/file* | fileglob }} bool - return the bool for value e.g. {{ yes | bool }} to_datetime - return datetime object for string e.g. {{ \"2019-03-07 13:37:00\" | to_datetime }} strftime - return current date string for format e.g. {{ \"%a, %d %b %Y %H:%M\" | strftime }} regex_replace - perform a re.sub returning a string e.g. {{ hello world | regex_replace(pattern=\"world\", replacement=\"kapitan\") }} regex_escape - escape all regular expressions special characters from string e.g. {{ \"+s[a-z].*\" | regex_escape }} regex_search - perform re.search and return the list of matches or a backref e.g. {{ hello world | regex_search(\"world.*\") }} regex_findall - perform re.findall and return the list of matches as array e.g. {{ hello world | regex_findall(\"world.*\") }} ternary - value ? true_val : false_val e.g. {{ condition | ternary(\"yes\", \"no\") }} shuffle - randomly shuffle elements of a list {{ [1, 2, 3, 4, 5] | shuffle }} reveal_maybe - reveal ref/secret tag only if `compile --reveal` flag is set e.g. {{ \"?{ref:my_ref}\" | reveal_maybe }} You can also provide path to your custom filter modules in CLI. By default, you can put your filters in lib/jinja2_filters.py and they will automatically get loaded. jsonnet Jsonnet is a superset of json format that includes features such as conditionals, variables and imports. Refer to jsonnet docs to understand how it works. Note that unlike jinja2 templates, one jsonnet template can output multiple files (one per object declared in the file). Supported output types: yaml (default) json Using the inventory in jsonnet Typical jsonnet files would start as follows: local kap = import \"lib/kapitan.libjsonnet\"; local inventory = kap.inventory(); The first line is required to access the kapitan inventory values. On the second line, inventory() callback is used to initialise a local variable through which inventory values for this target can be referenced. For example, the script below local kap = import \"lib/kapitan.libjsonnet\"; local inventory = kap.inventory(); { \"data_java_opts\": inventory.parameters.elasticsearch.roles.data.java_opts, } imports the inventory for the target you're compiling and returns the java_opts for the elasticsearch data role. Callback functions In addition, importing kapitan.libjsonnet makes available the following native_callback functions gluing reclass to jsonnet (amongst others): yaml_load - returns a json string of the specified yaml file yaml_dump - returns a string yaml from a json string file_read - reads the file specified jinja2_template - renders the jinja2 file with context specified sha256_string - returns sha256 of string gzip_b64 - returns base64 encoded gzip of obj inventory - returns a dictionary with the inventory for target Jinja2 jsonnet templating The following jsonnet snippet renders the jinja2 template in templates/got.j2 : local kap = import \"lib/kapitan.libjsonnet\"; { \"jon_snow\": kap.jinja2_template(\"templates/got.j2\", { is_dead: false }), } It's up to you to decide what the output is. kadet This input type is experimental. See https://github.com/deepmind/kapitan/pull/190 for its usage. Supported output types: yaml (default) json helm This is a Python binding to helm template command for users with helm charts. Unlike any other input types, Helm input types support the following additional parameters under kapitan.compile : parameters : kapitan : compile : - output_path : <output_path> input_type : helm input_paths : - <chart_path> helm_values : <object_with_values_to_override> helm_params : namespace : <substitutes_.Release.Namespace> name_template : <namespace_template> release_name : <chart_release_name> helm_values is an object containing values specified that will override the default values in the input chart. This has exactly the same effect as specifying --values custom_values.yml for helm template command where custom_values.yml structure mirrors that of helm_values . helm_params correspond to the options for helm template as follows: namespace: equivalent of --namespace option: note that due to the restriction on helm template command, specifying the namespace does not automatically add metadata.namespace property to the resources. Therefore, users are encourage to explicitly specify in all resources: metadata : namespace : {{ .Release.Namespace }} # or any other custom values name_template: equivalent of --name-template option release_name: equivalent of --name option See the helm doc for further detail. Building the binding from source Run cd kapitan/inputs/helm ./build.sh This requires Go >= 1.12. Helm subcharts This binding supports helm subcharts. However, since the external dependency manager does not parse requirements.yaml in order to detect chart dependencies, you are required to manually download the entire chart including the parent charts. Supported output types: N/A (no need to specify output_type )","title":"Compiling components/templates"},{"location":"compile/#kapitan-compile","text":"Note: make sure to read up on inventory before moving on.","title":"Kapitan compile"},{"location":"compile/#specifying-inputs-and-outputs","text":"Input types can be specified in the inventory under kapitan.compile in the following format: parameters : kapitan : compile : - output_path : <output_path_in_target_dir> input_type : jinja | jsonnet | kadet | helm input_paths : - path/to/input/dir/or/file output_type : <output_type_specific_to_input_type>","title":"Specifying inputs and outputs"},{"location":"compile/#supported-input-types","text":"Kapitan supports the following input template types: jinja jsonnet kadet (alpha) helm (optional)","title":"Supported input types"},{"location":"compile/#jinja","text":"This renders jinja2 templates, typically stored in templates/ directory, such as README, scripts and config files. Refer to jinja2 docs to understand how the template engine works. For Jinja2, input_paths can be either a file or a directory: in case of a directory, all the templates in the directory will be rendered and outputted to output_path . Supported output types : N/A (no need to specify output_type )","title":"jinja"},{"location":"compile/#using-the-inventory-in-jinja2","text":"Jinja2 types will pass the \"inventory\" and whatever target vars as context keys in your template. This snippet renders the same java_opts for the elasticsearch data role: java_opts for elasticsearch data role are: {{ inventory.parameters.elasticsearch.roles.data.java_opts }}","title":"Using the inventory in jinja2"},{"location":"compile/#jinja2-custom-filters","text":"We support the following custom filters for use in Jinja2 templates: sha256 - SHA256 hashing of text e.g. {{ text | sha256 }} yaml - Dump text as YAML e.g. {{ text | yaml }} b64encode - base64 encode text e.g. {{ text | b64encode }} b64decode - base64 decode text e.g. {{ text | b64decode }} fileglob - return list of matched regular files for glob e.g. {{ ./path/file* | fileglob }} bool - return the bool for value e.g. {{ yes | bool }} to_datetime - return datetime object for string e.g. {{ \"2019-03-07 13:37:00\" | to_datetime }} strftime - return current date string for format e.g. {{ \"%a, %d %b %Y %H:%M\" | strftime }} regex_replace - perform a re.sub returning a string e.g. {{ hello world | regex_replace(pattern=\"world\", replacement=\"kapitan\") }} regex_escape - escape all regular expressions special characters from string e.g. {{ \"+s[a-z].*\" | regex_escape }} regex_search - perform re.search and return the list of matches or a backref e.g. {{ hello world | regex_search(\"world.*\") }} regex_findall - perform re.findall and return the list of matches as array e.g. {{ hello world | regex_findall(\"world.*\") }} ternary - value ? true_val : false_val e.g. {{ condition | ternary(\"yes\", \"no\") }} shuffle - randomly shuffle elements of a list {{ [1, 2, 3, 4, 5] | shuffle }} reveal_maybe - reveal ref/secret tag only if `compile --reveal` flag is set e.g. {{ \"?{ref:my_ref}\" | reveal_maybe }} You can also provide path to your custom filter modules in CLI. By default, you can put your filters in lib/jinja2_filters.py and they will automatically get loaded.","title":"Jinja2 custom filters"},{"location":"compile/#jsonnet","text":"Jsonnet is a superset of json format that includes features such as conditionals, variables and imports. Refer to jsonnet docs to understand how it works. Note that unlike jinja2 templates, one jsonnet template can output multiple files (one per object declared in the file). Supported output types: yaml (default) json","title":"jsonnet"},{"location":"compile/#using-the-inventory-in-jsonnet","text":"Typical jsonnet files would start as follows: local kap = import \"lib/kapitan.libjsonnet\"; local inventory = kap.inventory(); The first line is required to access the kapitan inventory values. On the second line, inventory() callback is used to initialise a local variable through which inventory values for this target can be referenced. For example, the script below local kap = import \"lib/kapitan.libjsonnet\"; local inventory = kap.inventory(); { \"data_java_opts\": inventory.parameters.elasticsearch.roles.data.java_opts, } imports the inventory for the target you're compiling and returns the java_opts for the elasticsearch data role.","title":"Using the inventory in jsonnet"},{"location":"compile/#callback-functions","text":"In addition, importing kapitan.libjsonnet makes available the following native_callback functions gluing reclass to jsonnet (amongst others): yaml_load - returns a json string of the specified yaml file yaml_dump - returns a string yaml from a json string file_read - reads the file specified jinja2_template - renders the jinja2 file with context specified sha256_string - returns sha256 of string gzip_b64 - returns base64 encoded gzip of obj inventory - returns a dictionary with the inventory for target","title":"Callback functions"},{"location":"compile/#jinja2-jsonnet-templating","text":"The following jsonnet snippet renders the jinja2 template in templates/got.j2 : local kap = import \"lib/kapitan.libjsonnet\"; { \"jon_snow\": kap.jinja2_template(\"templates/got.j2\", { is_dead: false }), } It's up to you to decide what the output is.","title":"Jinja2 jsonnet templating"},{"location":"compile/#kadet","text":"This input type is experimental. See https://github.com/deepmind/kapitan/pull/190 for its usage. Supported output types: yaml (default) json","title":"kadet"},{"location":"compile/#helm","text":"This is a Python binding to helm template command for users with helm charts. Unlike any other input types, Helm input types support the following additional parameters under kapitan.compile : parameters : kapitan : compile : - output_path : <output_path> input_type : helm input_paths : - <chart_path> helm_values : <object_with_values_to_override> helm_params : namespace : <substitutes_.Release.Namespace> name_template : <namespace_template> release_name : <chart_release_name> helm_values is an object containing values specified that will override the default values in the input chart. This has exactly the same effect as specifying --values custom_values.yml for helm template command where custom_values.yml structure mirrors that of helm_values . helm_params correspond to the options for helm template as follows: namespace: equivalent of --namespace option: note that due to the restriction on helm template command, specifying the namespace does not automatically add metadata.namespace property to the resources. Therefore, users are encourage to explicitly specify in all resources: metadata : namespace : {{ .Release.Namespace }} # or any other custom values name_template: equivalent of --name-template option release_name: equivalent of --name option See the helm doc for further detail.","title":"helm"},{"location":"compile/#building-the-binding-from-source","text":"Run cd kapitan/inputs/helm ./build.sh This requires Go >= 1.12.","title":"Building the binding from source"},{"location":"compile/#helm-subcharts","text":"This binding supports helm subcharts. However, since the external dependency manager does not parse requirements.yaml in order to detect chart dependencies, you are required to manually download the entire chart including the parent charts. Supported output types: N/A (no need to specify output_type )","title":"Helm subcharts"},{"location":"contributing/","text":"How to Contribute We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow. Submitting changes We would like ask you to fork Kapitan project and create a Pull Request targeting master branch. All submissions, including submissions by project members, require review. Setting up environment We highly recommend that you create a dedicated Python environment for Kapitan. There are multiple solutions: pyenv virtualenv venv Once you've done it, please install all Kapitan's dependencies: pip3 install -r requirements.txt Because we are using a pinned version of reclass which is added as a submodule into Kapitan's repository, you need to pull it separately by executing the command below: git submodule update --init Testing Run make test to run all tests. If you modify anything in the examples/ folder make sure you replicate the compiled result of that in tests/test_kubernetes_compiled . If you add new features, run make test_coverage to make sure the test coverage remains at current or better levels. If you would like to evaluate your changes by running your version of Kapitan, you can do that by running bin/kapitan from this repository or even setting an alias to it. Code Style Try to fix warnings from make codestyle before submitting to make sure you adhere to the Style Guide for Python (PEP8) . Releasing Create a branch named release-v<NUMBER> . Use v0.*.*-rc.* if you want pre-release versions to be uploaded. Update CHANGELOG.md with the release changes. Once reviewed and merged, Travis will auto-release. The merge has to happen with a merge commit not with squash/rebase so that the commit message still mentions deepmind/release-v* inside. Updating gh-pages docs Local testing # from project root pip install mkdocs mkdocs-material pymdown-extensions markdown-include mkdocs build mkdocs serve # open http://127.0.0.1:8000 Deploying to Github # locally, on master branch (which has your updated docs) mkdocs gh-deploy -m \"Commit message\" -f ./mkdocs.yml -b remote_branch_name # after it's pushed, create a PR that targets the gh-pages branch Contributor License Agreement Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution, this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributing"},{"location":"contributing/#how-to-contribute","text":"We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.","title":"How to Contribute"},{"location":"contributing/#submitting-changes","text":"We would like ask you to fork Kapitan project and create a Pull Request targeting master branch. All submissions, including submissions by project members, require review.","title":"Submitting changes"},{"location":"contributing/#setting-up-environment","text":"We highly recommend that you create a dedicated Python environment for Kapitan. There are multiple solutions: pyenv virtualenv venv Once you've done it, please install all Kapitan's dependencies: pip3 install -r requirements.txt Because we are using a pinned version of reclass which is added as a submodule into Kapitan's repository, you need to pull it separately by executing the command below: git submodule update --init","title":"Setting up environment"},{"location":"contributing/#testing","text":"Run make test to run all tests. If you modify anything in the examples/ folder make sure you replicate the compiled result of that in tests/test_kubernetes_compiled . If you add new features, run make test_coverage to make sure the test coverage remains at current or better levels. If you would like to evaluate your changes by running your version of Kapitan, you can do that by running bin/kapitan from this repository or even setting an alias to it.","title":"Testing"},{"location":"contributing/#code-style","text":"Try to fix warnings from make codestyle before submitting to make sure you adhere to the Style Guide for Python (PEP8) .","title":"Code Style"},{"location":"contributing/#releasing","text":"Create a branch named release-v<NUMBER> . Use v0.*.*-rc.* if you want pre-release versions to be uploaded. Update CHANGELOG.md with the release changes. Once reviewed and merged, Travis will auto-release. The merge has to happen with a merge commit not with squash/rebase so that the commit message still mentions deepmind/release-v* inside.","title":"Releasing"},{"location":"contributing/#updating-gh-pages-docs","text":"","title":"Updating gh-pages docs"},{"location":"contributing/#local-testing","text":"# from project root pip install mkdocs mkdocs-material pymdown-extensions markdown-include mkdocs build mkdocs serve # open http://127.0.0.1:8000","title":"Local testing"},{"location":"contributing/#deploying-to-github","text":"# locally, on master branch (which has your updated docs) mkdocs gh-deploy -m \"Commit message\" -f ./mkdocs.yml -b remote_branch_name # after it's pushed, create a PR that targets the gh-pages branch","title":"Deploying to Github"},{"location":"contributing/#contributor-license-agreement","text":"Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution, this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributor License Agreement"},{"location":"external_dependencies/","text":"Fetching external dependencies Kapitan is capable of fetching components stored in remote locations. This feature can be used by specifying those dependencies in the inventory under parameters.kapitan.dependencies . Supported types are: git type http type Some use cases of this feature may include: using templates/jsonnet libraries hosted remotely using values in remote files via file_read jsonnet callback Usage parameters : kapitan : dependencies : - type : <dependency_type> output_path : path/to/file/or/dir source : <source_of_dependency> # other type-specific parameters, if any Use --fetch option to fetch the dependencies: $ kapitan compile --fetch This will download the dependencies and store them at their respective output_path . Dependencies whose output_path already exists will be skipped. Git type Git types can fetch external dependencies available at git:// URL. This is useful for fetching repositories or their sub-directories. Note : git types require git binary available on your system. Usage parameters : kapitan : dependencies : - type : git output_path : path/to/dir source : git://<url> subdir : relative/path/from/repo/root (optional) ref : tag, commit, branch etc. (optional) HTTP type http[s] types can fetch external dependencies available at http:// or https:// URL. Usage parameters : kapitan : dependencies : - type : http | https output_path : path/to/file source : http[s]://<url> unpack : True | False output_path must fully specify the file name. For example: parameters : kapitan : dependencies : - type : https output_path : foo.txt source : https://example.com/foo.txt unpack : True | False Setting unpack: True will unpack zip or tar files onto the output_path.","title":"External dependency management"},{"location":"external_dependencies/#fetching-external-dependencies","text":"Kapitan is capable of fetching components stored in remote locations. This feature can be used by specifying those dependencies in the inventory under parameters.kapitan.dependencies . Supported types are: git type http type Some use cases of this feature may include: using templates/jsonnet libraries hosted remotely using values in remote files via file_read jsonnet callback","title":"Fetching external dependencies"},{"location":"external_dependencies/#usage","text":"parameters : kapitan : dependencies : - type : <dependency_type> output_path : path/to/file/or/dir source : <source_of_dependency> # other type-specific parameters, if any Use --fetch option to fetch the dependencies: $ kapitan compile --fetch This will download the dependencies and store them at their respective output_path . Dependencies whose output_path already exists will be skipped.","title":"Usage"},{"location":"external_dependencies/#git-type","text":"Git types can fetch external dependencies available at git:// URL. This is useful for fetching repositories or their sub-directories. Note : git types require git binary available on your system.","title":"Git type"},{"location":"external_dependencies/#usage_1","text":"parameters : kapitan : dependencies : - type : git output_path : path/to/dir source : git://<url> subdir : relative/path/from/repo/root (optional) ref : tag, commit, branch etc. (optional)","title":"Usage"},{"location":"external_dependencies/#http-type","text":"http[s] types can fetch external dependencies available at http:// or https:// URL.","title":"HTTP type"},{"location":"external_dependencies/#usage_2","text":"parameters : kapitan : dependencies : - type : http | https output_path : path/to/file source : http[s]://<url> unpack : True | False output_path must fully specify the file name. For example: parameters : kapitan : dependencies : - type : https output_path : foo.txt source : https://example.com/foo.txt unpack : True | False Setting unpack: True will unpack zip or tar files onto the output_path.","title":"Usage"},{"location":"inventory/","text":"Inventory Overview Inventory is a hierarchical database of variables that are passed to the targets during compilation. By default, Kapitan will look for an inventory/ directory to render the inventory from. There are 2 types of objects inside the inventory; inventory classes and inventory targets . Inventory Classes Classes define variables that are shared across many targets. You can have, for example, a component.elasticsearch class with all the default values for targets using elasticsearch. Or a production or dev class to enable / disable certain features based on the type of target. You can always override values further up the tree (i.e. in the inventory target file or in a class that inherits another class) Classifying almost anything will help you avoid repetition (DRY) and will force you to organise parameters hierarchically. Example: elasticsearch For example, the snippet below, taken from the example elasticsearch class, declares what parameters are needed for the elasticsearch component: $ cat inventory/classes/component/elasticsearch.yml parameters: elasticsearch: image: \"quay.io/pires/docker-elasticsearch-kubernetes:5.5.0\" java_opts: \"-Xms512m -Xmx512m\" replicas: 1 masters: 1 roles: master: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} data: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} client: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} ingest: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} As shown above, within the inventory, you can refer to the values with the syntax ${obj_name:key_name} (no need to specify the parameters key). Example: mysql Or in the mysql class example, we declare the generic variables that will be shared by all targets that import the component and what to compile. We include a secret that is referencing a GPG encrypted value in secrets/targets/minikube-mysql/mysql/password , or if the file doesn't exist, it will dynamically generate a random b64-encoded password, encrypt it and save it into the file. $ cat inventory/classes/component/mysql.yml parameters: mysql: storage: 10G storage_class: standard image: mysql:latest users: root: # If 'secrets/targets/${target_name}/mysql/password' doesn't exist, it will gen a random b64-encoded password password: ?{gpg:targets/${target_name}/mysql/password|randomstr|base64} # password: ?{gkms:targets/${target_name}/mysql/password|randomstr|base64} # password: ?{awskms:targets/${target_name}/mysql/password|randomstr|base64} # Generates the sha256 checksum of the previously declared B64'ed password # It's base64'ed again so that it can be used in kubernetes secrets password_sha256: ?{gpg:targets/${target_name}/mysql/password_sha256|reveal:targets/${target_name}/mysql/password|sha256|base64} kapitan: compile: - output_path: manifests input_type: jsonnet input_paths: - components/mysql/main.jsonnet output_type: yaml - output_path: scripts input_type: jinja2 input_paths: - scripts - output_path: . output_type: yaml input_type: jinja2 input_paths: - docs/mysql/README.md Inventory Targets A target usually represents a single namespace in a kubernetes cluster and defines all components, scripts and documentation that will be generated for that target. Kapitan will recognise files in inventory/targets as targets. When you run kapitan compile , kapitan will generate compiled directory whose sub-directories are named after the targets, each of which contains all the compiled output defined under parameters.kapitan.compile for a target. Inside the inventory target files you can include classes and define new values or override any values inherited from the included classes. For example: $ cat inventory/targets/minikube-es.yml classes: - common - cluster.minikube - component.elasticsearch parameters: target_name: minikube-es elasticsearch: replicas: 2 Targets can also be defined inside the inventory . Note : Each target must contain the property parameters.kapitan.vars.target whose value equals to the name of the target file. For example, for the target inventory/targets/minikube-es.yml , the rendered inventory must contain: parameters : kapitan : vars : target : minikube-es kapitan-specific inventory values: parameters.kapitan Values under parameters.kapitan , such as parameters.kapitan.vars as mentioned above, are special values that kapitan parses and processes. These include: kapitan.compile items which indicate which files to compile kapitan.secrets which contains secret encryption/decryption information kapitan.validate items which indicate which compiled output to validate kapitan.vars which are also passed down to jsonnet and jinja2 templates as contexts Useful commands kapitan inventory Renders the resulting inventory values for a specific target. For example, rendering the inventory for the minikube-es target: $ kapitan inventory -t minikube-es ... classes: - component.namespace - cluster.common - common - cluster.minikube - component.elasticsearch environment: base exports: {} parameters: _reclass_: environment: base name: full: minikube-es short: minikube-es cluster: id: minikube name: minikube type: minikube user: minikube elasticsearch: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 roles: client: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 data: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 ingest: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 master: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 kapitan: compile: - input_paths: - components/namespace/main.jsonnet input_type: jsonnet output_path: pre-deploy output_type: yaml - input_paths: - components/elasticsearch/main.jsonnet input_type: jsonnet output_path: manifests output_type: yaml - input_paths: - scripts input_type: jinja2 output_path: scripts - input_paths: - docs/elasticsearch/README.md input_type: jinja2 output_path: . secrets: gpg: recipients: - fingerprint: D9234C61F58BEB3ED8552A57E28DC07A3CBFAE7C name: example@kapitan.dev vars: namespace: minikube-es target: minikube-es kubectl: insecure_skip_tls_verify: false minikube: cpus: 4 memory: 4096 version: v0.25.0 mysql: hostname: localhost namespace: minikube-es target_name: minikube-es vault: address: https://localhost:8200 Use kapitan lint to checkup on your inventory/secrets. kapitan searchvar Shows all inventory files where a variable is declared: $ kapitan searchvar parameters.elasticsearch.replicas ./inventory/targets/minikube-es.yml 2 ./inventory/classes/component/elasticsearch.yml 1","title":"Inventory"},{"location":"inventory/#inventory","text":"","title":"Inventory"},{"location":"inventory/#overview","text":"Inventory is a hierarchical database of variables that are passed to the targets during compilation. By default, Kapitan will look for an inventory/ directory to render the inventory from. There are 2 types of objects inside the inventory; inventory classes and inventory targets .","title":"Overview"},{"location":"inventory/#inventory-classes","text":"Classes define variables that are shared across many targets. You can have, for example, a component.elasticsearch class with all the default values for targets using elasticsearch. Or a production or dev class to enable / disable certain features based on the type of target. You can always override values further up the tree (i.e. in the inventory target file or in a class that inherits another class) Classifying almost anything will help you avoid repetition (DRY) and will force you to organise parameters hierarchically.","title":"Inventory Classes"},{"location":"inventory/#example-elasticsearch","text":"For example, the snippet below, taken from the example elasticsearch class, declares what parameters are needed for the elasticsearch component: $ cat inventory/classes/component/elasticsearch.yml parameters: elasticsearch: image: \"quay.io/pires/docker-elasticsearch-kubernetes:5.5.0\" java_opts: \"-Xms512m -Xmx512m\" replicas: 1 masters: 1 roles: master: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} data: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} client: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} ingest: image: ${elasticsearch:image} java_opts: ${elasticsearch:java_opts} replicas: ${elasticsearch:replicas} masters: ${elasticsearch:masters} As shown above, within the inventory, you can refer to the values with the syntax ${obj_name:key_name} (no need to specify the parameters key).","title":"Example: elasticsearch"},{"location":"inventory/#example-mysql","text":"Or in the mysql class example, we declare the generic variables that will be shared by all targets that import the component and what to compile. We include a secret that is referencing a GPG encrypted value in secrets/targets/minikube-mysql/mysql/password , or if the file doesn't exist, it will dynamically generate a random b64-encoded password, encrypt it and save it into the file. $ cat inventory/classes/component/mysql.yml parameters: mysql: storage: 10G storage_class: standard image: mysql:latest users: root: # If 'secrets/targets/${target_name}/mysql/password' doesn't exist, it will gen a random b64-encoded password password: ?{gpg:targets/${target_name}/mysql/password|randomstr|base64} # password: ?{gkms:targets/${target_name}/mysql/password|randomstr|base64} # password: ?{awskms:targets/${target_name}/mysql/password|randomstr|base64} # Generates the sha256 checksum of the previously declared B64'ed password # It's base64'ed again so that it can be used in kubernetes secrets password_sha256: ?{gpg:targets/${target_name}/mysql/password_sha256|reveal:targets/${target_name}/mysql/password|sha256|base64} kapitan: compile: - output_path: manifests input_type: jsonnet input_paths: - components/mysql/main.jsonnet output_type: yaml - output_path: scripts input_type: jinja2 input_paths: - scripts - output_path: . output_type: yaml input_type: jinja2 input_paths: - docs/mysql/README.md","title":"Example: mysql"},{"location":"inventory/#inventory-targets","text":"A target usually represents a single namespace in a kubernetes cluster and defines all components, scripts and documentation that will be generated for that target. Kapitan will recognise files in inventory/targets as targets. When you run kapitan compile , kapitan will generate compiled directory whose sub-directories are named after the targets, each of which contains all the compiled output defined under parameters.kapitan.compile for a target. Inside the inventory target files you can include classes and define new values or override any values inherited from the included classes. For example: $ cat inventory/targets/minikube-es.yml classes: - common - cluster.minikube - component.elasticsearch parameters: target_name: minikube-es elasticsearch: replicas: 2 Targets can also be defined inside the inventory . Note : Each target must contain the property parameters.kapitan.vars.target whose value equals to the name of the target file. For example, for the target inventory/targets/minikube-es.yml , the rendered inventory must contain: parameters : kapitan : vars : target : minikube-es","title":"Inventory Targets"},{"location":"inventory/#kapitan-specific-inventory-values-parameterskapitan","text":"Values under parameters.kapitan , such as parameters.kapitan.vars as mentioned above, are special values that kapitan parses and processes. These include: kapitan.compile items which indicate which files to compile kapitan.secrets which contains secret encryption/decryption information kapitan.validate items which indicate which compiled output to validate kapitan.vars which are also passed down to jsonnet and jinja2 templates as contexts","title":"kapitan-specific inventory values: parameters.kapitan"},{"location":"inventory/#useful-commands","text":"","title":"Useful commands"},{"location":"inventory/#kapitan-inventory","text":"Renders the resulting inventory values for a specific target. For example, rendering the inventory for the minikube-es target: $ kapitan inventory -t minikube-es ... classes: - component.namespace - cluster.common - common - cluster.minikube - component.elasticsearch environment: base exports: {} parameters: _reclass_: environment: base name: full: minikube-es short: minikube-es cluster: id: minikube name: minikube type: minikube user: minikube elasticsearch: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 roles: client: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 data: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 ingest: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 master: image: quay.io/pires/docker-elasticsearch-kubernetes:5.5.0 java_opts: -Xms512m -Xmx512m masters: 1 replicas: 2 kapitan: compile: - input_paths: - components/namespace/main.jsonnet input_type: jsonnet output_path: pre-deploy output_type: yaml - input_paths: - components/elasticsearch/main.jsonnet input_type: jsonnet output_path: manifests output_type: yaml - input_paths: - scripts input_type: jinja2 output_path: scripts - input_paths: - docs/elasticsearch/README.md input_type: jinja2 output_path: . secrets: gpg: recipients: - fingerprint: D9234C61F58BEB3ED8552A57E28DC07A3CBFAE7C name: example@kapitan.dev vars: namespace: minikube-es target: minikube-es kubectl: insecure_skip_tls_verify: false minikube: cpus: 4 memory: 4096 version: v0.25.0 mysql: hostname: localhost namespace: minikube-es target_name: minikube-es vault: address: https://localhost:8200 Use kapitan lint to checkup on your inventory/secrets.","title":"kapitan inventory"},{"location":"inventory/#kapitan-searchvar","text":"Shows all inventory files where a variable is declared: $ kapitan searchvar parameters.elasticsearch.replicas ./inventory/targets/minikube-es.yml 2 ./inventory/classes/component/elasticsearch.yml 1","title":"kapitan searchvar"},{"location":"kapitan_overview/","text":"Main concepts Inventory Inventory is a hierarchical database of variables, defined in yaml files, that are passed to the targets during compilation. This will be explained in detail in the inventory section of the documentation. Components (templates) Components will receive the inventory values for each individual target and gets rendered and saved into the compiled directory. The types of available templates and how to use them is discussed in the compile operation section of the documentation. Typical Folder Structure kapitan init To start off a kapitan project, you can run kapitan init --directory <directory> to populate a new directory with the recommended kapitan folder structure. The bare minimum structure that makes use of kapitan features may look as follows: . \u251c\u2500\u2500 components \u2502 \u251c\u2500\u2500 mycomponent.jsonnet \u251c\u2500\u2500 templates \u251c\u2500\u2500 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 inventory \u2502 \u251c\u2500\u2500 classes \u2502 \u2502 \u251c\u2500\u2500 common.yml \u2502 \u2514\u2500\u2500 targets \u2502 \u251c\u2500\u2500 dev.yml \u2502 \u251c\u2500\u2500 staging.yml \u2502 \u2514\u2500\u2500 prod.yml \u251c\u2500\u2500 secrets \u2502 \u251c\u2500\u2500 targets \u2502 \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2502 \u2514\u2500\u2500 password \u2514\u2500\u2500\u2500\u251c\u2500\u2500 common \u2514\u2500\u2500 example-com-tls.key components : stores jsonnet files each of which corresponds to an application (for example) templates : stores Jinja2 and Kadet templates inventory/targets : stores target files inventory/classes : stores inventory values to be inherited by targets secrets : stores secrets referenced inside the inventory Example: kubernetes deployment Refer to the structure below for more production-like uses of kapitan for kubernetes deployment: . \u251c\u2500\u2500 components \u2502 \u251c\u2500\u2500 elasticsearch \u2502 \u2502 \u251c\u2500\u2500 configmap.jsonnet \u2502 \u2502 \u251c\u2500\u2500 deployment.jsonnet \u2502 \u2502 \u251c\u2500\u2500 main.jsonnet \u2502 \u2502 \u2514\u2500\u2500 service.jsonnet \u2502 \u2514\u2500\u2500 nginx \u2502 \u251c\u2500\u2500 configmap.jsonnet \u2502 \u251c\u2500\u2500 deployment.jsonnet \u2502 \u251c\u2500\u2500 main.jsonnet \u2502 \u251c\u2500\u2500 nginx.conf.j2 \u2502 \u2514\u2500\u2500 service.jsonnet \u251c\u2500\u2500 inventory \u2502 \u251c\u2500\u2500 classes \u2502 \u2502 \u251c\u2500\u2500 cluster \u2502 \u2502 \u2502 \u251c\u2500\u2500 cluster1.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 cluster2.yml \u2502 \u2502 \u251c\u2500\u2500 component \u2502 \u2502 \u2502 \u251c\u2500\u2500 elasticsearch.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 nginx.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 zookeeper.yml \u2502 \u2502 \u2514\u2500\u2500 environment \u2502 \u2502 \u251c\u2500\u2500 dev.yml \u2502 \u2502 \u2514\u2500\u2500 prod.yml \u2502 \u2514\u2500\u2500 targets \u2502 \u251c\u2500\u2500 dev-cluster1-elasticsearch.yml \u2502 \u251c\u2500\u2500 prod-cluster1-elasticsearch.yml \u2502 \u2514\u2500\u2500 prod-cluster2-frontend.yml \u251c\u2500\u2500 secrets \u2502 \u251c\u2500\u2500 targets \u2502 \u2502 \u251c\u2500\u2500 prod-cluster1-elasticsearch \u2502 \u2502 \u2502 \u2514\u2500\u2500 password \u2502 \u251c\u2500\u2500 common \u2502 \u2502 \u2514\u2500\u2500 example-com-tls.key \u251c\u2500\u2500 lib \u251c\u2500\u2500 kapitan.libjsonnet \u2514\u2500\u2500 kube.libjsonnet The use of each file in this folder will become clear in the the subsequent documentations.","title":"Kapitan Overview"},{"location":"kapitan_overview/#main-concepts","text":"","title":"Main concepts"},{"location":"kapitan_overview/#inventory","text":"Inventory is a hierarchical database of variables, defined in yaml files, that are passed to the targets during compilation. This will be explained in detail in the inventory section of the documentation.","title":"Inventory"},{"location":"kapitan_overview/#components-templates","text":"Components will receive the inventory values for each individual target and gets rendered and saved into the compiled directory. The types of available templates and how to use them is discussed in the compile operation section of the documentation.","title":"Components (templates)"},{"location":"kapitan_overview/#typical-folder-structure","text":"","title":"Typical Folder Structure"},{"location":"kapitan_overview/#kapitan-init","text":"To start off a kapitan project, you can run kapitan init --directory <directory> to populate a new directory with the recommended kapitan folder structure. The bare minimum structure that makes use of kapitan features may look as follows: . \u251c\u2500\u2500 components \u2502 \u251c\u2500\u2500 mycomponent.jsonnet \u251c\u2500\u2500 templates \u251c\u2500\u2500 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 inventory \u2502 \u251c\u2500\u2500 classes \u2502 \u2502 \u251c\u2500\u2500 common.yml \u2502 \u2514\u2500\u2500 targets \u2502 \u251c\u2500\u2500 dev.yml \u2502 \u251c\u2500\u2500 staging.yml \u2502 \u2514\u2500\u2500 prod.yml \u251c\u2500\u2500 secrets \u2502 \u251c\u2500\u2500 targets \u2502 \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2502 \u2514\u2500\u2500 password \u2514\u2500\u2500\u2500\u251c\u2500\u2500 common \u2514\u2500\u2500 example-com-tls.key components : stores jsonnet files each of which corresponds to an application (for example) templates : stores Jinja2 and Kadet templates inventory/targets : stores target files inventory/classes : stores inventory values to be inherited by targets secrets : stores secrets referenced inside the inventory","title":"kapitan init"},{"location":"kapitan_overview/#example-kubernetes-deployment","text":"Refer to the structure below for more production-like uses of kapitan for kubernetes deployment: . \u251c\u2500\u2500 components \u2502 \u251c\u2500\u2500 elasticsearch \u2502 \u2502 \u251c\u2500\u2500 configmap.jsonnet \u2502 \u2502 \u251c\u2500\u2500 deployment.jsonnet \u2502 \u2502 \u251c\u2500\u2500 main.jsonnet \u2502 \u2502 \u2514\u2500\u2500 service.jsonnet \u2502 \u2514\u2500\u2500 nginx \u2502 \u251c\u2500\u2500 configmap.jsonnet \u2502 \u251c\u2500\u2500 deployment.jsonnet \u2502 \u251c\u2500\u2500 main.jsonnet \u2502 \u251c\u2500\u2500 nginx.conf.j2 \u2502 \u2514\u2500\u2500 service.jsonnet \u251c\u2500\u2500 inventory \u2502 \u251c\u2500\u2500 classes \u2502 \u2502 \u251c\u2500\u2500 cluster \u2502 \u2502 \u2502 \u251c\u2500\u2500 cluster1.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 cluster2.yml \u2502 \u2502 \u251c\u2500\u2500 component \u2502 \u2502 \u2502 \u251c\u2500\u2500 elasticsearch.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 nginx.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 zookeeper.yml \u2502 \u2502 \u2514\u2500\u2500 environment \u2502 \u2502 \u251c\u2500\u2500 dev.yml \u2502 \u2502 \u2514\u2500\u2500 prod.yml \u2502 \u2514\u2500\u2500 targets \u2502 \u251c\u2500\u2500 dev-cluster1-elasticsearch.yml \u2502 \u251c\u2500\u2500 prod-cluster1-elasticsearch.yml \u2502 \u2514\u2500\u2500 prod-cluster2-frontend.yml \u251c\u2500\u2500 secrets \u2502 \u251c\u2500\u2500 targets \u2502 \u2502 \u251c\u2500\u2500 prod-cluster1-elasticsearch \u2502 \u2502 \u2502 \u2514\u2500\u2500 password \u2502 \u251c\u2500\u2500 common \u2502 \u2502 \u2514\u2500\u2500 example-com-tls.key \u251c\u2500\u2500 lib \u251c\u2500\u2500 kapitan.libjsonnet \u2514\u2500\u2500 kube.libjsonnet The use of each file in this folder will become clear in the the subsequent documentations.","title":"Example: kubernetes deployment"},{"location":"pyenv-scl/","text":"Kapitan on Older Linux Systems Introduction Kapitan requires Python 3.6, and you need to be able to install the dependencies in the requirements file . However, sometimes this isn't entirely straightforward, and you may not be able or willing to install new versions of Python system-wide. We do provide a dockerfile which you can use to run Kapitan in a container, but if this isn't practical or possible either, you may wish to use one of the following options: PyEnv (Linux, distro-agnostic) Software Collections (RHEL-based distros) Both of these projects allow you to use a different version of Python specifically for your work with or on Kapitan. They work similarly to Python Virtual Environments but with more isolation from the lower-level OS-wide Python installation. Both of these projects manipulate your shell environment variables to make sure you're using the right binaries and modules. This document assumes you're using the bash shell. PyEnv and Software Collections are not Google projects, so please exercise your judgment as to whether these projects are suitable for your circumstances. On Debian-based Operating Systems - PyEnv Here at Streams, we use a Debian-based operating system for our day-to-day work. We've found that PyEnv works well for us. Getting Started Take a look at the PyEnv project on Github. There are two options for installing this project. The author makes a separate installer script available at this Github link , or you can manually install it. We recommend using the automated installer unless you have reason to use the manual installation process. The Automated Installer To use the installer, we would recommend downloading the installer script and examining it before you execute it. $ curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer > pyenv-installer Then, once you've checked it, you can execute it. PyEnv doesn't need root privileges, so you can run it without using sudo $ bash ./pyenv-installer Instructions on updating and removing the tool when you've installed it with the installer can be found on the pyenv-installer project page. Pay attention to the output of the installer. It might require you to add lines to your .bashrc file manually. Manually Installing from Github Take a look at the README.md on the PyEnv project page and follow the installation instructions there. Using Kapitan with PyEnv Once you have successfully installed PyEnv, you'll need to restart your shell. Either open a new shell session or source your .bashrc file like so: $ source ~/.bashrc Now that you have PyEnv ready to go, we can check it runs: $ pyenv pyenv 1.2.8 Usage: pyenv <command> [<args>] ... Before you move onto the next step, you'll need to install some dependencies if they aren't already present. To do this, you might need root access: # apt install libssl-dev libffi-dev Let's install Python 3.7.1, which is a stable and up-to-date release at the time of writing. We know Kapitan works with this release. $ pyenv install 3 .7.1 Downloading Python-3.7.1.tar.xz... -> https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz Installing Python-3.7.1... Installed Python-3.7.1 to /home/mikejo/.pyenv/versions/3.7.1 $ pyenv local 3 .7.1 $ python --version Python 3.7.1 $ Once it completes, we can activate the newer Python installation and set about installing Kapitan! Make sure PyEnv is activated using the $ pyenv local 3.7.1 command above and then run the following: $ pip install --user --upgrade kapitan After Kapitan is installed in this way, you might have to add the following to your PATH environment variable: $ { HOME } /.local/bin and you can do this like so: export PATH=${HOME}/.local/bin:${PATH} Add that line to the end of your .bashrc if you'd like it to take effect in all the shell sessions you use. You can now check everything installed correctly and start using Kapitan! $ kapitan usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,secrets} ... On RHEL-based Operating Systems - Software Collections PyEnv will work on RHEL-based operating systems (including the upstream Fedora project). Another option is to use the Software Collections project. It's a community project with backing from Red Hat, and it includes both official Red Hat releases of some software collections and third-party contributions. While Kapitan only needs you to install an official Red Hat collection release, please remember this isn't a Google project and to use your judgment as to whether this is appropriate for your circumstances. Installing Software Collections support on your machine Software Collections has installation documentation available here As this procedure needs you to add a repository to the OS package manager, you'll need to be root. Use su or run the following with sudo as appropriate. Once you've completed the installation of the scl tool, install the Python 3.5 SCL package (YUM/DNF package names are identical to the name of the Software Collection). # yum install rh-python35 As of this point, you don't need to be root any more. Return to your regular shell and activate the Python 3.5 software collection you just installed. This command starts a shell that uses the Python 3.5 installation you just carried out: $ scl enable rh-python35 bash Install Kapitan: $ pip install --user --upgrade kapitan After Kapitan is installed in this way, you might have to add the following to your PATH environment variable: $ { HOME } /.local/bin and you can do this like so: export PATH=${HOME}/.local/bin:${PATH} Add that line to the end of your .bashrc if you'd like it to take effect in all the shell sessions you use. You can now check everything installed correctly and start using Kapitan! $ kapitan usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,secrets} ... When you come back to using this method after restarting your shell, you can switch back to the rh-python35 collection either by creating a shell alias for the kapitan command to 'scl enable rh-python35 kapitan' but we recommend that you can use the scl command to start a new shell using '$ scl enable rh-python35 bash' Once you finish using the software collection, exit the shell with exit or Ctrl+D","title":"Set up kapitan on older Python systems"},{"location":"pyenv-scl/#kapitan-on-older-linux-systems","text":"","title":"Kapitan on Older Linux Systems"},{"location":"pyenv-scl/#introduction","text":"Kapitan requires Python 3.6, and you need to be able to install the dependencies in the requirements file . However, sometimes this isn't entirely straightforward, and you may not be able or willing to install new versions of Python system-wide. We do provide a dockerfile which you can use to run Kapitan in a container, but if this isn't practical or possible either, you may wish to use one of the following options: PyEnv (Linux, distro-agnostic) Software Collections (RHEL-based distros) Both of these projects allow you to use a different version of Python specifically for your work with or on Kapitan. They work similarly to Python Virtual Environments but with more isolation from the lower-level OS-wide Python installation. Both of these projects manipulate your shell environment variables to make sure you're using the right binaries and modules. This document assumes you're using the bash shell. PyEnv and Software Collections are not Google projects, so please exercise your judgment as to whether these projects are suitable for your circumstances.","title":"Introduction"},{"location":"pyenv-scl/#on-debian-based-operating-systems-pyenv","text":"Here at Streams, we use a Debian-based operating system for our day-to-day work. We've found that PyEnv works well for us.","title":"On Debian-based Operating Systems - PyEnv"},{"location":"pyenv-scl/#getting-started","text":"Take a look at the PyEnv project on Github. There are two options for installing this project. The author makes a separate installer script available at this Github link , or you can manually install it. We recommend using the automated installer unless you have reason to use the manual installation process.","title":"Getting Started"},{"location":"pyenv-scl/#the-automated-installer","text":"To use the installer, we would recommend downloading the installer script and examining it before you execute it. $ curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer > pyenv-installer Then, once you've checked it, you can execute it. PyEnv doesn't need root privileges, so you can run it without using sudo $ bash ./pyenv-installer Instructions on updating and removing the tool when you've installed it with the installer can be found on the pyenv-installer project page. Pay attention to the output of the installer. It might require you to add lines to your .bashrc file manually.","title":"The Automated Installer"},{"location":"pyenv-scl/#manually-installing-from-github","text":"Take a look at the README.md on the PyEnv project page and follow the installation instructions there.","title":"Manually Installing from Github"},{"location":"pyenv-scl/#using-kapitan-with-pyenv","text":"Once you have successfully installed PyEnv, you'll need to restart your shell. Either open a new shell session or source your .bashrc file like so: $ source ~/.bashrc Now that you have PyEnv ready to go, we can check it runs: $ pyenv pyenv 1.2.8 Usage: pyenv <command> [<args>] ... Before you move onto the next step, you'll need to install some dependencies if they aren't already present. To do this, you might need root access: # apt install libssl-dev libffi-dev Let's install Python 3.7.1, which is a stable and up-to-date release at the time of writing. We know Kapitan works with this release. $ pyenv install 3 .7.1 Downloading Python-3.7.1.tar.xz... -> https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz Installing Python-3.7.1... Installed Python-3.7.1 to /home/mikejo/.pyenv/versions/3.7.1 $ pyenv local 3 .7.1 $ python --version Python 3.7.1 $ Once it completes, we can activate the newer Python installation and set about installing Kapitan! Make sure PyEnv is activated using the $ pyenv local 3.7.1 command above and then run the following: $ pip install --user --upgrade kapitan After Kapitan is installed in this way, you might have to add the following to your PATH environment variable: $ { HOME } /.local/bin and you can do this like so: export PATH=${HOME}/.local/bin:${PATH} Add that line to the end of your .bashrc if you'd like it to take effect in all the shell sessions you use. You can now check everything installed correctly and start using Kapitan! $ kapitan usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,secrets} ...","title":"Using Kapitan with PyEnv"},{"location":"pyenv-scl/#on-rhel-based-operating-systems-software-collections","text":"PyEnv will work on RHEL-based operating systems (including the upstream Fedora project). Another option is to use the Software Collections project. It's a community project with backing from Red Hat, and it includes both official Red Hat releases of some software collections and third-party contributions. While Kapitan only needs you to install an official Red Hat collection release, please remember this isn't a Google project and to use your judgment as to whether this is appropriate for your circumstances.","title":"On RHEL-based Operating Systems - Software Collections"},{"location":"pyenv-scl/#installing-software-collections-support-on-your-machine","text":"Software Collections has installation documentation available here As this procedure needs you to add a repository to the OS package manager, you'll need to be root. Use su or run the following with sudo as appropriate. Once you've completed the installation of the scl tool, install the Python 3.5 SCL package (YUM/DNF package names are identical to the name of the Software Collection). # yum install rh-python35 As of this point, you don't need to be root any more. Return to your regular shell and activate the Python 3.5 software collection you just installed. This command starts a shell that uses the Python 3.5 installation you just carried out: $ scl enable rh-python35 bash Install Kapitan: $ pip install --user --upgrade kapitan After Kapitan is installed in this way, you might have to add the following to your PATH environment variable: $ { HOME } /.local/bin and you can do this like so: export PATH=${HOME}/.local/bin:${PATH} Add that line to the end of your .bashrc if you'd like it to take effect in all the shell sessions you use. You can now check everything installed correctly and start using Kapitan! $ kapitan usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,secrets} ... When you come back to using this method after restarting your shell, you can switch back to the rh-python35 collection either by creating a shell alias for the kapitan command to 'scl enable rh-python35 kapitan' but we recommend that you can use the scl command to start a new shell using '$ scl enable rh-python35 bash' Once you finish using the software collection, exit the shell with exit or Ctrl+D","title":"Installing Software Collections support on your machine"},{"location":"secrets/","text":"Kapitan secrets Kapitan can manage secrets with the following key management services: GPG Google Cloud KMS (beta) AWS KMS (beta) Vault (TBD) If you want to get started with secrets but don't have a GPG or KMS setup, you can also use the secret ref type. Note that ref is not encrypted and is intended for development purposes only. Do not use ref secrets if you're storing sensitive information! Using secrets The usual flow of creating and using an encrypted secret with kapitan is: 1. Define your GPG recipients or KMS key This is done in the inventory under parameters.kapitan.secrets . Just like any other inventory parameters, this can be inherited from a common class or defined per target. For example, common.yml may contain: parameters : kapitan : vars : target : ${target_name} namespace : ${target_name} secrets : gpg : recipients : - name : example@kapitan.dev fingerprint : D9234C61F58BEB3ED8552A57E28DC07A3CBFAE7C gkms : key : 'projects/<project>/locations/<location>/keyRings/<keyRing>/cryptoKeys/<key>' awskms : key : 'alias/nameOfKey' 2. Create your secret Manually via command line: $ kapitan secrets --write <secret_type>:path/to/secret/file -t <target_name> -f <secret_file> \u200b where <secret_type> can be any of: ref : ref type (not encrypted) gpg : GPG gkms : Google Cloud KMS awskms : AWS KMS vault (TBC) Kapitan will inherit the secrets configuration for the specified target, and encrypt and save your secret into <path/to/secret/file> . Automatically When referencing your secret in the inventory during compile, you can use the following functions to automatically generate, encrypt and save your secret: randomstr - Generates a random string. You can optionally pass the length you want i.e. `|randomstr:32` base64 - base64 encodes your secret; to be used as a secondary function i.e. `|randomstr|base64` sha256 - sha256 hashes your secret; to be used as a secondary function i.e. `|randomstr|sha256`. You can optionally pass a salt i.e `|randomstr|sha256:salt` -> becomes `sha256(\"salt:<generated random string>\")` reveal - Decrypts a secret; to be used as a secondary function, useful for reuse of a secret like for different encodings i.e `|reveal:path/to/secret|base64` rsa - Generates an RSA 4096 private key (PKCS#8). You can optionally pass the key size i.e. `|rsa:2048` rsapublic - Derives an RSA public key from a revealed private key i.e. `|reveal:path/to/encrypted_private_key|rsapublic` Note : If you use |reveal:/path/secret , when changing the /path/secret file make sure you also delete any secrets referencing /path/secret so kapitan can regenerate them. 3. Reference your secrets in your classes/targets and run kapitan compile Secrets can be referenced in the format ?{<secret_type>:path/to/secret/file} . For example, assume for now that your GPG-encrypted secret is already stored in a file at targets/secrets/mysql_password . This can be referenced in the inventory in the following format: users : root : # If 'secrets/targets/${target_name}/mysql/password' doesn't exist, we can automatically generate a random b64-encoded password as follows password : ?{gpg:targets/${target_name}/mysql/password|randomstr|base64} During compile, kapitan will search for the path targets/${target_name}/mysql/password . Should it not exist, then it will automatically generate a random base64 password and save it to that path. 4. Reveal and use the secrets You can reveal the secrets referenced in the outputs of kapitan compile via: $ kapitan secrets --reveal -f path/to/rendered/template For example, compiled/minikube-mysql/manifests/mysql_secret.yml with the following content: apiVersion : v1 data : MYSQL_ROOT_PASSWORD : ?{gpg:targets/minikube-mysql/mysql/password:ec3d54de} MYSQL_ROOT_PASSWORD_SHA256 : ?{gpg:targets/minikube-mysql/mysql/password_sha256:122d2732} kind : Secret metadata : annotations : {} labels : name : example-mysql name : example-mysql namespace : minikube-mysql type : Opaque can be revealed as follows: $ kapitan secrets --reveal -f compiled/minikube-mysql/manifests/mysql_secret.yml This will substitute the referenced secrets with the actual decrypted secrets stored at the referenced paths and display the file content. Secret sub-variables As illustrated above, one file corresponds to one secret. It is now possible for users who would like to reduce the decryption overhead to manually create a yaml file that contains multiple secrets, each of which can be referenced by its object key. For example, consider the secret file secrets/mysql_secrets : mysql_passwords : secret_foo : hello_world secret_bar : 54321password This can be manually encrypted by: $ kapitan secrets --write gpg:components/secrets/mysql_secrets -t prod -f secrets/mysql_secrets To reference secret_foo inside this file, you can specify it in the inventory as follows: secret_foo: ${gpg:components/secrets/mysql_secrets@mysql_passwords.secret_foo}","title":"Secret management"},{"location":"secrets/#kapitan-secrets","text":"Kapitan can manage secrets with the following key management services: GPG Google Cloud KMS (beta) AWS KMS (beta) Vault (TBD) If you want to get started with secrets but don't have a GPG or KMS setup, you can also use the secret ref type. Note that ref is not encrypted and is intended for development purposes only. Do not use ref secrets if you're storing sensitive information!","title":"Kapitan secrets"},{"location":"secrets/#using-secrets","text":"The usual flow of creating and using an encrypted secret with kapitan is:","title":"Using secrets"},{"location":"secrets/#1-define-your-gpg-recipients-or-kms-key","text":"This is done in the inventory under parameters.kapitan.secrets . Just like any other inventory parameters, this can be inherited from a common class or defined per target. For example, common.yml may contain: parameters : kapitan : vars : target : ${target_name} namespace : ${target_name} secrets : gpg : recipients : - name : example@kapitan.dev fingerprint : D9234C61F58BEB3ED8552A57E28DC07A3CBFAE7C gkms : key : 'projects/<project>/locations/<location>/keyRings/<keyRing>/cryptoKeys/<key>' awskms : key : 'alias/nameOfKey'","title":"1. Define your GPG recipients or KMS key"},{"location":"secrets/#2-create-your-secret","text":"","title":"2. Create your secret"},{"location":"secrets/#manually-via-command-line","text":"$ kapitan secrets --write <secret_type>:path/to/secret/file -t <target_name> -f <secret_file> \u200b where <secret_type> can be any of: ref : ref type (not encrypted) gpg : GPG gkms : Google Cloud KMS awskms : AWS KMS vault (TBC) Kapitan will inherit the secrets configuration for the specified target, and encrypt and save your secret into <path/to/secret/file> .","title":"Manually via command line:"},{"location":"secrets/#automatically","text":"When referencing your secret in the inventory during compile, you can use the following functions to automatically generate, encrypt and save your secret: randomstr - Generates a random string. You can optionally pass the length you want i.e. `|randomstr:32` base64 - base64 encodes your secret; to be used as a secondary function i.e. `|randomstr|base64` sha256 - sha256 hashes your secret; to be used as a secondary function i.e. `|randomstr|sha256`. You can optionally pass a salt i.e `|randomstr|sha256:salt` -> becomes `sha256(\"salt:<generated random string>\")` reveal - Decrypts a secret; to be used as a secondary function, useful for reuse of a secret like for different encodings i.e `|reveal:path/to/secret|base64` rsa - Generates an RSA 4096 private key (PKCS#8). You can optionally pass the key size i.e. `|rsa:2048` rsapublic - Derives an RSA public key from a revealed private key i.e. `|reveal:path/to/encrypted_private_key|rsapublic` Note : If you use |reveal:/path/secret , when changing the /path/secret file make sure you also delete any secrets referencing /path/secret so kapitan can regenerate them.","title":"Automatically"},{"location":"secrets/#3-reference-your-secrets-in-your-classestargets-and-run-kapitan-compile","text":"Secrets can be referenced in the format ?{<secret_type>:path/to/secret/file} . For example, assume for now that your GPG-encrypted secret is already stored in a file at targets/secrets/mysql_password . This can be referenced in the inventory in the following format: users : root : # If 'secrets/targets/${target_name}/mysql/password' doesn't exist, we can automatically generate a random b64-encoded password as follows password : ?{gpg:targets/${target_name}/mysql/password|randomstr|base64} During compile, kapitan will search for the path targets/${target_name}/mysql/password . Should it not exist, then it will automatically generate a random base64 password and save it to that path.","title":"3. Reference your secrets in your classes/targets and run kapitan compile"},{"location":"secrets/#4-reveal-and-use-the-secrets","text":"You can reveal the secrets referenced in the outputs of kapitan compile via: $ kapitan secrets --reveal -f path/to/rendered/template For example, compiled/minikube-mysql/manifests/mysql_secret.yml with the following content: apiVersion : v1 data : MYSQL_ROOT_PASSWORD : ?{gpg:targets/minikube-mysql/mysql/password:ec3d54de} MYSQL_ROOT_PASSWORD_SHA256 : ?{gpg:targets/minikube-mysql/mysql/password_sha256:122d2732} kind : Secret metadata : annotations : {} labels : name : example-mysql name : example-mysql namespace : minikube-mysql type : Opaque can be revealed as follows: $ kapitan secrets --reveal -f compiled/minikube-mysql/manifests/mysql_secret.yml This will substitute the referenced secrets with the actual decrypted secrets stored at the referenced paths and display the file content.","title":"4. Reveal and use the secrets"},{"location":"secrets/#secret-sub-variables","text":"As illustrated above, one file corresponds to one secret. It is now possible for users who would like to reduce the decryption overhead to manually create a yaml file that contains multiple secrets, each of which can be referenced by its object key. For example, consider the secret file secrets/mysql_secrets : mysql_passwords : secret_foo : hello_world secret_bar : 54321password This can be manually encrypted by: $ kapitan secrets --write gpg:components/secrets/mysql_secrets -t prod -f secrets/mysql_secrets To reference secret_foo inside this file, you can specify it in the inventory as follows: secret_foo: ${gpg:components/secrets/mysql_secrets@mysql_passwords.secret_foo}","title":"Secret sub-variables"},{"location":"usage/","text":"Usage To see all the available commands, run: $ kapitan -h usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,secrets,lint} ... Generic templated configuration management for Kubernetes, Terraform and other things positional arguments: {eval,compile,inventory,searchvar,secrets,lint,init,validate} commands eval evaluate jsonnet file compile compile targets inventory show inventory searchvar show all inventory files where var is declared secrets manage secrets lint linter for inventory and secrets init initialize a directory with the recommended kapitan project skeleton. validate validate the compile output against schemas as specified in inventory optional arguments: -h, --help show this help message and exit --version show program's version number and exit Additional parameters are available for each positional argument. For example: $ kapitan compile -h usage: kapitan compile [-h] [--search-paths JPATH [JPATH ...]] [--verbose] [--prune] [--quiet] [--output-path PATH] [--targets TARGET [TARGET ...]] [--parallelism INT] [--indent INT] [--secrets-path SECRETS_PATH] [--reveal] [--inventory-path INVENTORY_PATH] [--cache] [--cache-paths PATH [PATH ...]] [--ignore-version-check] optional arguments: -h, --help show this help message and exit --search-paths JPATH [JPATH ...], -J JPATH [JPATH ...] set search paths, default is [\".\"] --jinja2-filters FPATH, -J2F FPATH load custom jinja2 filters from any file, default is to put them inside lib/jinja2_filters.py --verbose, -v set verbose mode --prune prune jsonnet output --quiet set quiet mode, only critical output --output-path PATH set output path, default is \".\" --targets TARGET [TARGET ...], -t TARGET [TARGET ...] targets to compile, default is all --parallelism INT, -p INT Number of concurrent compile processes, default is 4 --indent INT, -i INT Indentation spaces for YAML/JSON, default is 2 --secrets-path SECRETS_PATH set secrets path, default is \"./secrets\" --reveal reveal secrets (warning: this will write sensitive data) --inventory-path INVENTORY_PATH set inventory path, default is \"./inventory\" --cache, -c enable compilation caching to .kapitan_cache, default is False --cache-paths PATH [PATH ...] cache additional paths to .kapitan_cache, default is [] --ignore-version-check ignore the version from .kapitan Using .kapitan config file These parameters can also be defined in a local .kapitan file, for example: $ cat .kapitan compile: indent: 4 parallelism: 8 This is equivalent to running: kapitan compile --indent 4 --parallelism 8 To enforce the kapitan version used for compilation (for consistency and safety), you can add version to .kapitan : $ cat .kapitan version: 0.21.0 Or to skip all minor version checks: $ cat .kapitan version: 0.21","title":"Usage"},{"location":"usage/#usage","text":"To see all the available commands, run: $ kapitan -h usage: kapitan [-h] [--version] {eval,compile,inventory,searchvar,secrets,lint} ... Generic templated configuration management for Kubernetes, Terraform and other things positional arguments: {eval,compile,inventory,searchvar,secrets,lint,init,validate} commands eval evaluate jsonnet file compile compile targets inventory show inventory searchvar show all inventory files where var is declared secrets manage secrets lint linter for inventory and secrets init initialize a directory with the recommended kapitan project skeleton. validate validate the compile output against schemas as specified in inventory optional arguments: -h, --help show this help message and exit --version show program's version number and exit Additional parameters are available for each positional argument. For example: $ kapitan compile -h usage: kapitan compile [-h] [--search-paths JPATH [JPATH ...]] [--verbose] [--prune] [--quiet] [--output-path PATH] [--targets TARGET [TARGET ...]] [--parallelism INT] [--indent INT] [--secrets-path SECRETS_PATH] [--reveal] [--inventory-path INVENTORY_PATH] [--cache] [--cache-paths PATH [PATH ...]] [--ignore-version-check] optional arguments: -h, --help show this help message and exit --search-paths JPATH [JPATH ...], -J JPATH [JPATH ...] set search paths, default is [\".\"] --jinja2-filters FPATH, -J2F FPATH load custom jinja2 filters from any file, default is to put them inside lib/jinja2_filters.py --verbose, -v set verbose mode --prune prune jsonnet output --quiet set quiet mode, only critical output --output-path PATH set output path, default is \".\" --targets TARGET [TARGET ...], -t TARGET [TARGET ...] targets to compile, default is all --parallelism INT, -p INT Number of concurrent compile processes, default is 4 --indent INT, -i INT Indentation spaces for YAML/JSON, default is 2 --secrets-path SECRETS_PATH set secrets path, default is \"./secrets\" --reveal reveal secrets (warning: this will write sensitive data) --inventory-path INVENTORY_PATH set inventory path, default is \"./inventory\" --cache, -c enable compilation caching to .kapitan_cache, default is False --cache-paths PATH [PATH ...] cache additional paths to .kapitan_cache, default is [] --ignore-version-check ignore the version from .kapitan","title":"Usage"},{"location":"usage/#using-kapitan-config-file","text":"These parameters can also be defined in a local .kapitan file, for example: $ cat .kapitan compile: indent: 4 parallelism: 8 This is equivalent to running: kapitan compile --indent 4 --parallelism 8 To enforce the kapitan version used for compilation (for consistency and safety), you can add version to .kapitan : $ cat .kapitan version: 0.21.0 Or to skip all minor version checks: $ cat .kapitan version: 0.21","title":"Using .kapitan config file"},{"location":"validate/","text":"kapitan validate Validates the schema of compiled output. Validate options are specified in the inventory under parameters.kapitan.validate . Supported types are: kubernetes manifests Usage Manually via command line after compile: $ kapitan validate Automatically, together with compile: $ kapitan compile --validate Kubernetes manifests Overview Kubernetes resources are identified by their kind . For example, they are: service deployment statefulset The manifest for each kind has certain restrictions such as required properties. Using kapitan, you can validate against the schemas to confirm that your compiled output indeed is a valid kubernetes manifest. First time they are used, the schemas for kubernetes manifests are dynamically downloaded from https://kubernetesjsonschema.dev . Those schemas will be cached into ./schemas/ by default, which can be modified using --schemas-path option. However, it is recommended to use .kapitan configuration as follows to avoid the need of typing down this option for every command: $ cat .kapitan # other options abbreviated for clarity validate: schemas-path: custom/schemas/cache/path Example Refer to the minikube-es inventory in kapitan inventory . To validate the schema of the compiled StatefulSet manifest at compiled/minikube-es/manifests/es-client.yml (created by components/elasticsearch/main.jsonnet ), add kapitan.validate parameters in minikube-es inventory: kapitan : vars : target : ${target_name} namespace : ${target_name} compile : - output_path : manifests input_type : jsonnet input_paths : - components/elasticsearch/main.jsonnet ### other inputs abbreviated for clarity ### validate : - output_paths : - manifests/es-client.yml type : kubernetes kind : statefulset # note that it is in lowercase version : 1.14.0 # optional, defaults to 1.14.0 Then run: $ kapitan validate -t minikube-es invalid 'statefulset' manifest at ./compiled/minikube-es/manifests/es-client.yml ['spec'] 'selector' is a required property","title":"Manifest validation"},{"location":"validate/#kapitan-validate","text":"Validates the schema of compiled output. Validate options are specified in the inventory under parameters.kapitan.validate . Supported types are: kubernetes manifests","title":"kapitan validate"},{"location":"validate/#usage","text":"Manually via command line after compile: $ kapitan validate Automatically, together with compile: $ kapitan compile --validate","title":"Usage"},{"location":"validate/#kubernetes-manifests","text":"","title":"Kubernetes manifests"},{"location":"validate/#overview","text":"Kubernetes resources are identified by their kind . For example, they are: service deployment statefulset The manifest for each kind has certain restrictions such as required properties. Using kapitan, you can validate against the schemas to confirm that your compiled output indeed is a valid kubernetes manifest. First time they are used, the schemas for kubernetes manifests are dynamically downloaded from https://kubernetesjsonschema.dev . Those schemas will be cached into ./schemas/ by default, which can be modified using --schemas-path option. However, it is recommended to use .kapitan configuration as follows to avoid the need of typing down this option for every command: $ cat .kapitan # other options abbreviated for clarity validate: schemas-path: custom/schemas/cache/path","title":"Overview"},{"location":"validate/#example","text":"Refer to the minikube-es inventory in kapitan inventory . To validate the schema of the compiled StatefulSet manifest at compiled/minikube-es/manifests/es-client.yml (created by components/elasticsearch/main.jsonnet ), add kapitan.validate parameters in minikube-es inventory: kapitan : vars : target : ${target_name} namespace : ${target_name} compile : - output_path : manifests input_type : jsonnet input_paths : - components/elasticsearch/main.jsonnet ### other inputs abbreviated for clarity ### validate : - output_paths : - manifests/es-client.yml type : kubernetes kind : statefulset # note that it is in lowercase version : 1.14.0 # optional, defaults to 1.14.0 Then run: $ kapitan validate -t minikube-es invalid 'statefulset' manifest at ./compiled/minikube-es/manifests/es-client.yml ['spec'] 'selector' is a required property","title":"Example"},{"location":"kap_proposals/kap_0_kadet/","text":"Kadet This introduces a new experimental input type called Kadet. Kadet is essentially a Python module offering a set of classes and functions to define objects which will compile to JSON or YAML. A complete example is available in examples/kubernetes/components/nginx . Author: @ramaro Overview BaseObj BaseObj implements the basic object implementation that compiles into JSON or YAML. Setting keys in self.root means they will be in the compiled output. Keys can be set as an hierarchy of attributes (courtesy of addict ) The self.body() method is reserved for setting self.root on instantiation: The example below: class MyApp ( BaseObj ): def body ( self ): self . root . name = \"myapp\" self . root . inner . foo = \"bar\" self . root . list = [ 1 , 2 , 3 ] compiles into: --- name : myapp inner : foo : bar list : - 1 - 2 - 3 The self.new() method can be used to define a basic constructor. self.need() checks if a key is set and errors if it isn't (with an optional custom error message). kwargs that are passed onto a new instance of BaseObj are always accessible via self.kwargs In this example, MyApp needs name and foo to be passed as kwargs. class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) def body ( self ): self . root . name = self . kwargs . name self . root . inner . foo = self . kwargs . foo self . root . list = [ 1 , 2 , 3 ] obj = MyApp ( name = \"myapp\" , foo = \"bar\" ) Setting a skeleton Defining a large body with Python can be quite hard and repetitive to read and write. The self.update_root() method allows importing a YAML/JSON file to set the skeleton of self.root. MyApp's skeleton can be set instead like this: #skel.yml --- name : myapp inner : foo : bar list : - 1 - 2 - 3 class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) self . update_root ( \"path/to/skel.yml\" ) Extending a skeleton'd MyApp is possible just by implementing self.body() : class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) self . update_root ( \"path/to/skel.yml\" ) def body ( self ): self . set_replicas () self . root . metadata . labels = { \"app\" : \"mylabel\" } def set_replicas ( self ): self . root . spec . replicas = 5 Inheritance Python inheritance will work as expected: class MyOtherApp ( MyApp ): def new ( self ): super () . new () # MyApp's new() self . need ( \"size\" ) def body ( self ): super () . body () # we want to extend MyApp's body self . root . size = self . kwargs . size del self . root . list # get rid of \"list\" obj = MyOtherApp ( name = \"otherapp1\" , foo = \"bar2\" , size = 3 ) compiles to: --- name : otherapp1 inner : foo : bar2 replicas : 5 size : 3 Components A component in Kadet is a python module that must implement a main() function returning an instance of BaseObj . The inventory is also available via the inventory() function. For example, a tinyapp component: # components/tinyapp/__init__.py from kapitan.inputs.kadet import BaseOBj , inventory inv = inventory () # returns inventory for target being compiled class TinyApp ( BaseObj ): def body ( self ): self . root . foo = \"bar\" self . root . replicas = inv . parameters . tinyapp . replicas def main (): obj = BaseOb () obj . root . deployment = TinyApp () # will compile into deployment.yml return obj An inventory class must be created for tinyapp : # inventory/classes/components/tinyapp.yml parameters : tinyapp : replicas : 1 kapitan : compile : - output_path : manifests input_type : kadet output_type : yaml input_paths : - components/tinyapp Common components A library in --search-paths (which now defaults to . and lib/ ) can also be a module that kadet components import. It is loaded using the load_from_search_paths() : kubelib = load_from_search_paths ( \"kubelib\" ) # lib/kubelib/__init__.py def main (): obj = BaseObj () obj . root . example_app_deployment = kubelib . Deployment ( name = \"example-app\" ) return obj","title":"Kadet"},{"location":"kap_proposals/kap_0_kadet/#kadet","text":"This introduces a new experimental input type called Kadet. Kadet is essentially a Python module offering a set of classes and functions to define objects which will compile to JSON or YAML. A complete example is available in examples/kubernetes/components/nginx . Author: @ramaro","title":"Kadet"},{"location":"kap_proposals/kap_0_kadet/#overview","text":"","title":"Overview"},{"location":"kap_proposals/kap_0_kadet/#baseobj","text":"BaseObj implements the basic object implementation that compiles into JSON or YAML. Setting keys in self.root means they will be in the compiled output. Keys can be set as an hierarchy of attributes (courtesy of addict ) The self.body() method is reserved for setting self.root on instantiation: The example below: class MyApp ( BaseObj ): def body ( self ): self . root . name = \"myapp\" self . root . inner . foo = \"bar\" self . root . list = [ 1 , 2 , 3 ] compiles into: --- name : myapp inner : foo : bar list : - 1 - 2 - 3 The self.new() method can be used to define a basic constructor. self.need() checks if a key is set and errors if it isn't (with an optional custom error message). kwargs that are passed onto a new instance of BaseObj are always accessible via self.kwargs In this example, MyApp needs name and foo to be passed as kwargs. class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) def body ( self ): self . root . name = self . kwargs . name self . root . inner . foo = self . kwargs . foo self . root . list = [ 1 , 2 , 3 ] obj = MyApp ( name = \"myapp\" , foo = \"bar\" )","title":"BaseObj"},{"location":"kap_proposals/kap_0_kadet/#setting-a-skeleton","text":"Defining a large body with Python can be quite hard and repetitive to read and write. The self.update_root() method allows importing a YAML/JSON file to set the skeleton of self.root. MyApp's skeleton can be set instead like this: #skel.yml --- name : myapp inner : foo : bar list : - 1 - 2 - 3 class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) self . update_root ( \"path/to/skel.yml\" ) Extending a skeleton'd MyApp is possible just by implementing self.body() : class MyApp ( BaseObj ): def new ( self ): self . need ( \"name\" ) self . need ( \"foo\" , msg = \"please provide a value for foo\" ) self . update_root ( \"path/to/skel.yml\" ) def body ( self ): self . set_replicas () self . root . metadata . labels = { \"app\" : \"mylabel\" } def set_replicas ( self ): self . root . spec . replicas = 5","title":"Setting a skeleton"},{"location":"kap_proposals/kap_0_kadet/#inheritance","text":"Python inheritance will work as expected: class MyOtherApp ( MyApp ): def new ( self ): super () . new () # MyApp's new() self . need ( \"size\" ) def body ( self ): super () . body () # we want to extend MyApp's body self . root . size = self . kwargs . size del self . root . list # get rid of \"list\" obj = MyOtherApp ( name = \"otherapp1\" , foo = \"bar2\" , size = 3 ) compiles to: --- name : otherapp1 inner : foo : bar2 replicas : 5 size : 3","title":"Inheritance"},{"location":"kap_proposals/kap_0_kadet/#components","text":"A component in Kadet is a python module that must implement a main() function returning an instance of BaseObj . The inventory is also available via the inventory() function. For example, a tinyapp component: # components/tinyapp/__init__.py from kapitan.inputs.kadet import BaseOBj , inventory inv = inventory () # returns inventory for target being compiled class TinyApp ( BaseObj ): def body ( self ): self . root . foo = \"bar\" self . root . replicas = inv . parameters . tinyapp . replicas def main (): obj = BaseOb () obj . root . deployment = TinyApp () # will compile into deployment.yml return obj An inventory class must be created for tinyapp : # inventory/classes/components/tinyapp.yml parameters : tinyapp : replicas : 1 kapitan : compile : - output_path : manifests input_type : kadet output_type : yaml input_paths : - components/tinyapp","title":"Components"},{"location":"kap_proposals/kap_0_kadet/#common-components","text":"A library in --search-paths (which now defaults to . and lib/ ) can also be a module that kadet components import. It is loaded using the load_from_search_paths() : kubelib = load_from_search_paths ( \"kubelib\" ) # lib/kubelib/__init__.py def main (): obj = BaseObj () obj . root . example_app_deployment = kubelib . Deployment ( name = \"example-app\" ) return obj","title":"Common components"},{"location":"kap_proposals/kap_1_external_dependencies/","text":"External dependencies This features allows kapitan to fetch files from online repositories/sources during compile and store in a particular target directory. Author: @yoshi-1224 Specification Specify the files to be fetched as follows: parameters : kapitan : dependencies : - type : git | http[s] output_path : <output_path> source : <git/http[s]_url> The output path is the path to save the dependency into. For example, it could be /components/external/manifest.jsonnet . Then, the user can specify the fetched file as a kapitan.compile item along with the locally-created files. Git type may also include ref and subdir parameters as illustrated below: - type : git output_path : <output_path> source : <git_url> subdir : relative/path/in/repository ref : <commit_hash/branch/tag> If the file already exists at output_path , the fetch will be skipped. For fresh fetch of the dependencies, users may add --fetch option as follows: $ kapitan compile --fetch Users can also add the fetch_always: true option to the kapitan.compile in the inventory in order to force fresh fetch of the dependencies every time. Implementation details Dependencies GitPython module (and git executable) for git type requests module for http[s] (optional) tqdm for reporting download progress","title":"External dependencies"},{"location":"kap_proposals/kap_1_external_dependencies/#external-dependencies","text":"This features allows kapitan to fetch files from online repositories/sources during compile and store in a particular target directory. Author: @yoshi-1224","title":"External dependencies"},{"location":"kap_proposals/kap_1_external_dependencies/#specification","text":"Specify the files to be fetched as follows: parameters : kapitan : dependencies : - type : git | http[s] output_path : <output_path> source : <git/http[s]_url> The output path is the path to save the dependency into. For example, it could be /components/external/manifest.jsonnet . Then, the user can specify the fetched file as a kapitan.compile item along with the locally-created files. Git type may also include ref and subdir parameters as illustrated below: - type : git output_path : <output_path> source : <git_url> subdir : relative/path/in/repository ref : <commit_hash/branch/tag> If the file already exists at output_path , the fetch will be skipped. For fresh fetch of the dependencies, users may add --fetch option as follows: $ kapitan compile --fetch Users can also add the fetch_always: true option to the kapitan.compile in the inventory in order to force fresh fetch of the dependencies every time.","title":"Specification"},{"location":"kap_proposals/kap_1_external_dependencies/#implementation-details","text":"","title":"Implementation details"},{"location":"kap_proposals/kap_1_external_dependencies/#dependencies","text":"GitPython module (and git executable) for git type requests module for http[s] (optional) tqdm for reporting download progress","title":"Dependencies"},{"location":"kap_proposals/kap_2_helm_charts_input_type/","text":"Helm Charts Input Type This will allow kapitan, during compilation, to overwrite the values in user-specified helm charts using its inventory by calling the Go & Sprig template libraries. The helm charts can be specified via local path, and users may download the helm chart via external-dependency feature (of http[s] type). Author: @yoshi-1224 Specification This feature basically follows the helm template command available. This will run after the fetching of the external dependencies takes place, such that users can simultaneously specify the fetch as well as the import of a helm chart dependency. Semantics kapitan : compile : - input_type : helm input_path : <path_to_chart_dir> output_path : <output_path> set-file : - <optional_file_path> - ... values_file : <optional_values_file> namespace : <optional_namespace> This mostly maps to the options available to helm template command (refer to here ). Implementation details C-binding between Helm (Go) and Kapitan (Python) will be created. Helm makes use of two template libraries, namely, text/template and Sprig. The code for helm template command will be converted into shared object (.so) using CGo, which exposes C interface that kapitan (i.e. CPython) could use. The source code for helm template command is found here . This file will be modified to 1. remove redundant options 2. expose C-interface for Kapitan Dependencies (possibly) pybindgen","title":"Helm Charts Input Type"},{"location":"kap_proposals/kap_2_helm_charts_input_type/#helm-charts-input-type","text":"This will allow kapitan, during compilation, to overwrite the values in user-specified helm charts using its inventory by calling the Go & Sprig template libraries. The helm charts can be specified via local path, and users may download the helm chart via external-dependency feature (of http[s] type). Author: @yoshi-1224","title":"Helm Charts Input Type"},{"location":"kap_proposals/kap_2_helm_charts_input_type/#specification","text":"This feature basically follows the helm template command available. This will run after the fetching of the external dependencies takes place, such that users can simultaneously specify the fetch as well as the import of a helm chart dependency.","title":"Specification"},{"location":"kap_proposals/kap_2_helm_charts_input_type/#semantics","text":"kapitan : compile : - input_type : helm input_path : <path_to_chart_dir> output_path : <output_path> set-file : - <optional_file_path> - ... values_file : <optional_values_file> namespace : <optional_namespace> This mostly maps to the options available to helm template command (refer to here ).","title":"Semantics"},{"location":"kap_proposals/kap_2_helm_charts_input_type/#implementation-details","text":"C-binding between Helm (Go) and Kapitan (Python) will be created. Helm makes use of two template libraries, namely, text/template and Sprig. The code for helm template command will be converted into shared object (.so) using CGo, which exposes C interface that kapitan (i.e. CPython) could use. The source code for helm template command is found here . This file will be modified to 1. remove redundant options 2. expose C-interface for Kapitan","title":"Implementation details"},{"location":"kap_proposals/kap_2_helm_charts_input_type/#dependencies","text":"(possibly) pybindgen","title":"Dependencies"},{"location":"kap_proposals/kap_3_schema_validation/","text":"Schema Validation (for k8s) If a yaml/json output is to be used as k8s manifest, users may specify its kind and have kapitan validate its structure during kapitan compile . The plan is to have this validation feature extendable to other outputs as well, such as terraform. Author: @yoshi-1224 Specification The following inventory will validate the structure of Kubernetes Service manifest file in . parameters : kapitan : validate : - output_type : kubernetes.service version : 1.6.6 output_path : relative/path/in/target version parameter is optional: if omitted, the version will be set to the stable release of kubernetes (tbc). Implementation The schemas will be downloaded by requests from this repository . Caching of schema will also be implemented. Dependencies jsonschema to validate the output yaml/json against the correct schema","title":"Schema Validation (for k8s)"},{"location":"kap_proposals/kap_3_schema_validation/#schema-validation-for-k8s","text":"If a yaml/json output is to be used as k8s manifest, users may specify its kind and have kapitan validate its structure during kapitan compile . The plan is to have this validation feature extendable to other outputs as well, such as terraform. Author: @yoshi-1224","title":"Schema Validation (for k8s)"},{"location":"kap_proposals/kap_3_schema_validation/#specification","text":"The following inventory will validate the structure of Kubernetes Service manifest file in . parameters : kapitan : validate : - output_type : kubernetes.service version : 1.6.6 output_path : relative/path/in/target version parameter is optional: if omitted, the version will be set to the stable release of kubernetes (tbc).","title":"Specification"},{"location":"kap_proposals/kap_3_schema_validation/#implementation","text":"The schemas will be downloaded by requests from this repository . Caching of schema will also be implemented.","title":"Implementation"},{"location":"kap_proposals/kap_3_schema_validation/#dependencies","text":"jsonschema to validate the output yaml/json against the correct schema","title":"Dependencies"},{"location":"kap_proposals/kap_4_standalone_executable/","text":"Standalone Kapitan Executable Create a portable (i.e. static) kapitan binary for users. This executable will be made available for each release on Github. The target/tested platform is Debian 9 (possibly Windows to be supported in the future). Criteria: - speed of the resulting binary - size of the resulting binary - portability of the binary (single-file executable or has an accompanying folder) - cross-platform - actively maintained - supports Python 3.6, 3.7 Author: @yoshi-1224 Tools to be explored (tentative first-choice) Pyinstaller (Alternative) nuitka (also part of GSoC 2019. It might soon support single-file executable output ).","title":"Standalone Kapitan Executable"},{"location":"kap_proposals/kap_4_standalone_executable/#standalone-kapitan-executable","text":"Create a portable (i.e. static) kapitan binary for users. This executable will be made available for each release on Github. The target/tested platform is Debian 9 (possibly Windows to be supported in the future). Criteria: - speed of the resulting binary - size of the resulting binary - portability of the binary (single-file executable or has an accompanying folder) - cross-platform - actively maintained - supports Python 3.6, 3.7 Author: @yoshi-1224","title":"Standalone Kapitan Executable"},{"location":"kap_proposals/kap_4_standalone_executable/#tools-to-be-explored","text":"(tentative first-choice) Pyinstaller (Alternative) nuitka (also part of GSoC 2019. It might soon support single-file executable output ).","title":"Tools to be explored"},{"location":"kap_proposals/kap_5_ref_types_redesign/","text":"Ref Types Redesign Redesign Kapitan Secrets and rename them as References or Ref . This will likely be a breaking change. Author: @ramaro Proposal Rename Secrets into Ref (or References ) to improve consistency and meaning of the backend types by removing the ref backend and introducting new backends: Type Description Encrypted? Compiles To gpg GnuPG Yes hashed tag gkms Google KMS Yes hashed tag awskms Amazon KMS Yes hashed tag base64 base64 No hashed tag plain plain text No plain text The type value will now need to be representative of the way a reference is stored via its backend. A new plain backend type is introduced and will compile into revealed state instead of a hashed tag. A new base64 backend type will store a base64 encoded value as the backend suggests (replacing the old badly named ref backend). The command line for secrets will be instead: $ kapitan refs --write gpg:my/secret1 ... $ kapitan refs --write base64:my/file ... $ kapitan refs --write plain:my/info ... plain backend The plain backend type will allow referring to external state by updating refs programmatically (e.g. in your pipeline) For example, one can update the value of an environment variable and use ?{plain:my/user} as a reference in a template: $ echo $USER | kapitan refs --write plain:my/user -f - Or update a docker image value as ref ?{plain:images/dev/envoy} : $ echo 'envoyproxy/envoy:v1.10.0' | kapitan refs --write plain:images/dev/envoy -f - These references will be compiled into their values instead of hashed tags. base64 backend The base64 backend type will function as the original ref type. Except that this time, the name is representative of what is actually happening :) Refs path Refs will be stored by default in the ./refs path set by --refs-path replacing the --secrets-path flag. Background Kapitan Secrets Kapitan Secrets allow referring to restricted information (passwords, private keys, etc...) in templates while also securely storing them. On compile, secret tags are updated into hashed tags which validate and instruct Kapitan how to reveal tags into decrypted or encoded information. Kapitan Secrets example The following command creates a GPG encrypted secret with the contents of file.txt for recipient ramaro@google.com to read: $ kapitan secrets --write gpg:my/secret1 -f file.txt --recipients ramaro@google.com This secret can be referred to in a jsonnet compoment: { \"type\" : \"app\" , \"name\" : \"test_app\" , \"username\" : \"user_one\" , \"password\" : \"?{gpg:my/secret1}\" } When this compoment is compiled, it looks like (note the hashed tag): type : app name : test_app username : user_one password : ?{gpg:my/secret1:deadbeef} A user with the required permissions can reveal the compiled component: $ kapitan secrets --reveal -f compiled/mytarget/manifests/component.yml type: app name: test_app username: user_one password: secret_content_of_file.txt Secret Backend Comparison Kapitan today offers multiple secret backends: Type Description Encrypted? Compiles To gpg GnuPG Yes hashed tag gkms Google KMS Yes hashed tag awskms Amazon KMS Yes hashed tag ref base64 No hashed tag However, not all backends are encrypted - this is not consistent! The ref type is not encrypted as its purpose is to allow getting started with the Kapitan Secrets workflow without the need of setting up the encryption backends tooling (gpg, gcloud, boto, etc...)","title":"Ref Types Redesign"},{"location":"kap_proposals/kap_5_ref_types_redesign/#ref-types-redesign","text":"Redesign Kapitan Secrets and rename them as References or Ref . This will likely be a breaking change. Author: @ramaro","title":"Ref Types Redesign"},{"location":"kap_proposals/kap_5_ref_types_redesign/#proposal","text":"Rename Secrets into Ref (or References ) to improve consistency and meaning of the backend types by removing the ref backend and introducting new backends: Type Description Encrypted? Compiles To gpg GnuPG Yes hashed tag gkms Google KMS Yes hashed tag awskms Amazon KMS Yes hashed tag base64 base64 No hashed tag plain plain text No plain text The type value will now need to be representative of the way a reference is stored via its backend. A new plain backend type is introduced and will compile into revealed state instead of a hashed tag. A new base64 backend type will store a base64 encoded value as the backend suggests (replacing the old badly named ref backend). The command line for secrets will be instead: $ kapitan refs --write gpg:my/secret1 ... $ kapitan refs --write base64:my/file ... $ kapitan refs --write plain:my/info ...","title":"Proposal"},{"location":"kap_proposals/kap_5_ref_types_redesign/#plain-backend","text":"The plain backend type will allow referring to external state by updating refs programmatically (e.g. in your pipeline) For example, one can update the value of an environment variable and use ?{plain:my/user} as a reference in a template: $ echo $USER | kapitan refs --write plain:my/user -f - Or update a docker image value as ref ?{plain:images/dev/envoy} : $ echo 'envoyproxy/envoy:v1.10.0' | kapitan refs --write plain:images/dev/envoy -f - These references will be compiled into their values instead of hashed tags.","title":"plain backend"},{"location":"kap_proposals/kap_5_ref_types_redesign/#base64-backend","text":"The base64 backend type will function as the original ref type. Except that this time, the name is representative of what is actually happening :)","title":"base64 backend"},{"location":"kap_proposals/kap_5_ref_types_redesign/#refs-path","text":"Refs will be stored by default in the ./refs path set by --refs-path replacing the --secrets-path flag.","title":"Refs path"},{"location":"kap_proposals/kap_5_ref_types_redesign/#background","text":"","title":"Background"},{"location":"kap_proposals/kap_5_ref_types_redesign/#kapitan-secrets","text":"Kapitan Secrets allow referring to restricted information (passwords, private keys, etc...) in templates while also securely storing them. On compile, secret tags are updated into hashed tags which validate and instruct Kapitan how to reveal tags into decrypted or encoded information.","title":"Kapitan Secrets"},{"location":"kap_proposals/kap_5_ref_types_redesign/#kapitan-secrets-example","text":"The following command creates a GPG encrypted secret with the contents of file.txt for recipient ramaro@google.com to read: $ kapitan secrets --write gpg:my/secret1 -f file.txt --recipients ramaro@google.com This secret can be referred to in a jsonnet compoment: { \"type\" : \"app\" , \"name\" : \"test_app\" , \"username\" : \"user_one\" , \"password\" : \"?{gpg:my/secret1}\" } When this compoment is compiled, it looks like (note the hashed tag): type : app name : test_app username : user_one password : ?{gpg:my/secret1:deadbeef} A user with the required permissions can reveal the compiled component: $ kapitan secrets --reveal -f compiled/mytarget/manifests/component.yml type: app name: test_app username: user_one password: secret_content_of_file.txt","title":"Kapitan Secrets example"},{"location":"kap_proposals/kap_5_ref_types_redesign/#secret-backend-comparison","text":"Kapitan today offers multiple secret backends: Type Description Encrypted? Compiles To gpg GnuPG Yes hashed tag gkms Google KMS Yes hashed tag awskms Amazon KMS Yes hashed tag ref base64 No hashed tag However, not all backends are encrypted - this is not consistent! The ref type is not encrypted as its purpose is to allow getting started with the Kapitan Secrets workflow without the need of setting up the encryption backends tooling (gpg, gcloud, boto, etc...)","title":"Secret Backend Comparison"}]}